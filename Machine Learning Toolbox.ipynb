{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(repr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-sample RMSE for linear regression\n",
    "\n",
    "RMSE is commonly calculated in-sample on your training set. What's a potential drawback to calculating training set error?\n",
    "* You have no idea how well your model generalizes to new data (i.e. overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-sample RMSE for linear regression on diamonds\n",
    "\n",
    "As you saw in the video, included in the course is the diamonds dataset, which is a classic dataset from the ggplot2 package. The dataset contains physical attributes of diamonds as well as the price they sold for. One interesting modeling challenge is predicting diamond price based on their attributes using something like a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1129.84298657309"
      ],
      "text/latex": [
       "1129.84298657309"
      ],
      "text/markdown": [
       "1129.84298657309"
      ],
      "text/plain": [
       "[1] 1129.843"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit lm model: model\n",
    "model <- lm(price ~ ., data = diamonds)\n",
    "\n",
    "# Predict on full data: p\n",
    "p <- predict(model)\n",
    "\n",
    "# Compute errors: error\n",
    "error <- p - diamonds$price\n",
    "\n",
    "# Calculate RMSE\n",
    "sqrt(mean(error^2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-sample RMSE for linear regression\n",
    "\n",
    "What is the advantage of using a train/test split rather than just validating your model in-sample on the training set?\n",
    "* It gives you an estimate of how well your model performs on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly order the data frame\n",
    "\n",
    "One way you can take a train/test split of a dataset is to order the dataset randomly, then divide it into the two sets. This ensures that the training set and test set are both random samples and that any biases in the ordering of the dataset (e.g. if it had originally been ordered by price or size) are not retained in the samples we take for training and testing your models. You can think of this like shuffling a brand new deck of playing cards before dealing hands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>49345</li>\n",
       "\t<li>50545</li>\n",
       "\t<li>15434</li>\n",
       "\t<li>44792</li>\n",
       "\t<li>34614</li>\n",
       "\t<li>27998</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 49345\n",
       "\\item 50545\n",
       "\\item 15434\n",
       "\\item 44792\n",
       "\\item 34614\n",
       "\\item 27998\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 49345\n",
       "2. 50545\n",
       "3. 15434\n",
       "4. 44792\n",
       "5. 34614\n",
       "6. 27998\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 49345 50545 15434 44792 34614 27998"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>carat</th><th scope=col>cut</th><th scope=col>color</th><th scope=col>clarity</th><th scope=col>depth</th><th scope=col>table</th><th scope=col>price</th><th scope=col>x</th><th scope=col>y</th><th scope=col>z</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.71     </td><td>Very Good</td><td>H        </td><td>SI1      </td><td>62.5     </td><td>60       </td><td>2096     </td><td>5.68     </td><td>5.75     </td><td>3.57     </td></tr>\n",
       "\t<tr><td>0.79     </td><td>Premium  </td><td>H        </td><td>SI1      </td><td>61.8     </td><td>59       </td><td>2275     </td><td>5.97     </td><td>5.91     </td><td>3.67     </td></tr>\n",
       "\t<tr><td>1.03     </td><td>Ideal    </td><td>F        </td><td>SI1      </td><td>62.4     </td><td>57       </td><td>6178     </td><td>6.48     </td><td>6.44     </td><td>4.03     </td></tr>\n",
       "\t<tr><td>0.50     </td><td>Ideal    </td><td>E        </td><td>VS2      </td><td>62.2     </td><td>54       </td><td>1624     </td><td>5.08     </td><td>5.11     </td><td>3.17     </td></tr>\n",
       "\t<tr><td>0.27     </td><td>Ideal    </td><td>E        </td><td>VS1      </td><td>61.6     </td><td>56       </td><td> 470     </td><td>4.14     </td><td>4.17     </td><td>2.56     </td></tr>\n",
       "\t<tr><td>0.30     </td><td>Premium  </td><td>E        </td><td>VS2      </td><td>61.7     </td><td>58       </td><td> 658     </td><td>4.32     </td><td>4.34     </td><td>2.67     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllll}\n",
       " carat & cut & color & clarity & depth & table & price & x & y & z\\\\\n",
       "\\hline\n",
       "\t 0.71      & Very Good & H         & SI1       & 62.5      & 60        & 2096      & 5.68      & 5.75      & 3.57     \\\\\n",
       "\t 0.79      & Premium   & H         & SI1       & 61.8      & 59        & 2275      & 5.97      & 5.91      & 3.67     \\\\\n",
       "\t 1.03      & Ideal     & F         & SI1       & 62.4      & 57        & 6178      & 6.48      & 6.44      & 4.03     \\\\\n",
       "\t 0.50      & Ideal     & E         & VS2       & 62.2      & 54        & 1624      & 5.08      & 5.11      & 3.17     \\\\\n",
       "\t 0.27      & Ideal     & E         & VS1       & 61.6      & 56        &  470      & 4.14      & 4.17      & 2.56     \\\\\n",
       "\t 0.30      & Premium   & E         & VS2       & 61.7      & 58        &  658      & 4.32      & 4.34      & 2.67     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "carat | cut | color | clarity | depth | table | price | x | y | z | \n",
       "|---|---|---|---|---|---|\n",
       "| 0.71      | Very Good | H         | SI1       | 62.5      | 60        | 2096      | 5.68      | 5.75      | 3.57      | \n",
       "| 0.79      | Premium   | H         | SI1       | 61.8      | 59        | 2275      | 5.97      | 5.91      | 3.67      | \n",
       "| 1.03      | Ideal     | F         | SI1       | 62.4      | 57        | 6178      | 6.48      | 6.44      | 4.03      | \n",
       "| 0.50      | Ideal     | E         | VS2       | 62.2      | 54        | 1624      | 5.08      | 5.11      | 3.17      | \n",
       "| 0.27      | Ideal     | E         | VS1       | 61.6      | 56        |  470      | 4.14      | 4.17      | 2.56      | \n",
       "| 0.30      | Premium   | E         | VS2       | 61.7      | 58        |  658      | 4.32      | 4.34      | 2.67      | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  carat cut       color clarity depth table price x    y    z   \n",
       "1 0.71  Very Good H     SI1     62.5  60    2096  5.68 5.75 3.57\n",
       "2 0.79  Premium   H     SI1     61.8  59    2275  5.97 5.91 3.67\n",
       "3 1.03  Ideal     F     SI1     62.4  57    6178  6.48 6.44 4.03\n",
       "4 0.50  Ideal     E     VS2     62.2  54    1624  5.08 5.11 3.17\n",
       "5 0.27  Ideal     E     VS1     61.6  56     470  4.14 4.17 2.56\n",
       "6 0.30  Premium   E     VS2     61.7  58     658  4.32 4.34 2.67"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set seed\n",
    "set.seed(42)\n",
    "\n",
    "# Shuffle row indices: rows\n",
    "rows <- sample(nrow(diamonds))\n",
    "head(rows)\n",
    "\n",
    "# Randomly order data\n",
    "diamonds <- diamonds[rows, ]\n",
    "head(diamonds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try an 80/20 split\n",
    "\n",
    "Now that your dataset is randomly ordered, you can split the first 80% of it into a training set, and the last 20% into a test set. You can do this by choosing a split point approximately 80% of the way through your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "43152"
      ],
      "text/latex": [
       "43152"
      ],
      "text/markdown": [
       "43152"
      ],
      "text/plain": [
       "[1] 43152"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Determine row to split on: split\n",
    "split <- round(nrow(diamonds) * .80)\n",
    "split\n",
    "# Create train\n",
    "train <- diamonds[1:split, ]\n",
    "\n",
    "# Create test\n",
    "test <- diamonds[(split + 1): nrow(diamonds), ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test set\n",
    "\n",
    "Now that you have a randomly split training set and test set, you can use the lm() function as you did in the first exercise to fit a model to your training set, rather than the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit lm model on train: model\n",
    "model <- lm(price ~ ., data = train) \n",
    "\n",
    "# Predict on test: p\n",
    "p <- predict(model, newdata = test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate test set RMSE by hand\n",
    "\n",
    "Now that you have predictions on the test set, you can use these predictions to calculate an error metric (in this case RMSE) on the test set and see how the model performs out-of-sample, rather than in-sample as you did in the first exercise. You first do this by calculating the errors between the predicted diamond prices and the actual diamond prices by subtracting the predictions from the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1136.59565324766"
      ],
      "text/latex": [
       "1136.59565324766"
      ],
      "text/markdown": [
       "1136.59565324766"
      ],
      "text/plain": [
       "[1] 1136.596"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute errors: error\n",
    "error <- p - test$price\n",
    "\n",
    "# Calculate RMSE\n",
    "sqrt(mean(error^2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing out-of-sample RMSE to in-sample RMSE\n",
    "\n",
    "Why is the test set RMSE higher than the training set RMSE?\n",
    "* Because the test set has a smaller sample size the training set and thus the mean error is lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage of cross-validation\n",
    "\n",
    "What is the advantage of cross-validation over a single train/test split?\n",
    "* It gives you multiple estimates of out-of-sample error, rather than a single estimate.  If all of your estimates give similar outputs, you can be more certain of the model's accuracy. If your estimates give different outputs, that tells you the model does not perform consistently and suggests a problem with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-fold cross-validation\n",
    "\n",
    "As you saw in the video, a better approach to validating models is to use multiple systematic test sets, rather than a single random train/test split. Fortunately, the caret package makes this very easy to do:\n",
    "\n",
    "    model <- train(y ~ ., my_data)\n",
    "\n",
    "caret supports many types of cross-validation, and you can specify which type of cross-validation and the number of cross-validation folds with the `trainControl()` function, which you pass to the trControl argument in `train()`:\n",
    "\n",
    "    model <- train(\n",
    "      y ~ ., my_data,\n",
    "      method = \"lm\",\n",
    "      trControl = trainControl(\n",
    "        method = \"cv\", number = 10,\n",
    "        verboseIter = TRUE\n",
    "      )\n",
    "    )\n",
    "\n",
    "It's important to note that you pass the method for modeling to the main `train()` function and the method for cross-validation to the `trainControl()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: lattice\n",
      "\n",
      "Attaching package: 'caret'\n",
      "\n",
      "The following object is masked from 'package:purrr':\n",
      "\n",
      "    lift\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01: intercept=TRUE \n",
      "- Fold01: intercept=TRUE \n",
      "+ Fold02: intercept=TRUE \n",
      "- Fold02: intercept=TRUE \n",
      "+ Fold03: intercept=TRUE \n",
      "- Fold03: intercept=TRUE \n",
      "+ Fold04: intercept=TRUE \n",
      "- Fold04: intercept=TRUE \n",
      "+ Fold05: intercept=TRUE \n",
      "- Fold05: intercept=TRUE \n",
      "+ Fold06: intercept=TRUE \n",
      "- Fold06: intercept=TRUE \n",
      "+ Fold07: intercept=TRUE \n",
      "- Fold07: intercept=TRUE \n",
      "+ Fold08: intercept=TRUE \n",
      "- Fold08: intercept=TRUE \n",
      "+ Fold09: intercept=TRUE \n",
      "- Fold09: intercept=TRUE \n",
      "+ Fold10: intercept=TRUE \n",
      "- Fold10: intercept=TRUE \n",
      "Aggregating results\n",
      "Fitting final model on full training set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear Regression \n",
       "\n",
       "53940 samples\n",
       "    9 predictor\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 48547, 48546, 48546, 48545, 48545, 48545, ... \n",
       "Resampling results:\n",
       "\n",
       "  RMSE      Rsquared   MAE     \n",
       "  1130.658  0.9197492  740.4646\n",
       "\n",
       "Tuning parameter 'intercept' was held constant at a value of TRUE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(caret)\n",
    "\n",
    "# Fit lm model using 10-fold CV: model\n",
    "model <- train(\n",
    "  price ~ ., diamonds,\n",
    "  method = \"lm\",\n",
    "  trControl = trainControl(\n",
    "    method = \"cv\", number = 10,\n",
    "    verboseIter = TRUE\n",
    "  )\n",
    ")\n",
    "\n",
    "# Print model to console\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-fold cross-validation\n",
    "\n",
    "In this course, you will use a wide variety of datasets to explore the full flexibility of the caret package. Here, you will use the famous Boston housing dataset, where the goal is to predict median home values in various Boston suburbs.\n",
    "\n",
    "You can use exactly the same code as in the previous exercise, but change the dataset used by the model.\n",
    "Next, you can reduce the number of cross-validation folds from 10 to 5 using the number argument to the trainControl() argument:\n",
    "\n",
    "    trControl = trainControl(\n",
    "      method = \"cv\", number = 5,\n",
    "      verboseIter = TRUE\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: 'MASS'\n",
      "\n",
      "The following object is masked from 'package:dplyr':\n",
      "\n",
      "    select\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t506 obs. of  14 variables:\n",
      " $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n",
      " $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...\n",
      " $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...\n",
      " $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...\n",
      " $ rm     : num  6.58 6.42 7.18 7 7.15 ...\n",
      " $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...\n",
      " $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...\n",
      " $ rad    : int  1 2 2 3 3 3 5 5 5 5 ...\n",
      " $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...\n",
      " $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...\n",
      " $ black  : num  397 397 393 395 397 ...\n",
      " $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...\n",
      " $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...\n"
     ]
    }
   ],
   "source": [
    "library(MASS)\n",
    "data(\"Boston\")\n",
    "str(Boston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold1: intercept=TRUE \n",
      "- Fold1: intercept=TRUE \n",
      "+ Fold2: intercept=TRUE \n",
      "- Fold2: intercept=TRUE \n",
      "+ Fold3: intercept=TRUE \n",
      "- Fold3: intercept=TRUE \n",
      "+ Fold4: intercept=TRUE \n",
      "- Fold4: intercept=TRUE \n",
      "+ Fold5: intercept=TRUE \n",
      "- Fold5: intercept=TRUE \n",
      "Aggregating results\n",
      "Fitting final model on full training set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear Regression \n",
       "\n",
       "506 samples\n",
       " 13 predictor\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (5 fold) \n",
       "Summary of sample sizes: 405, 403, 405, 406, 405 \n",
       "Resampling results:\n",
       "\n",
       "  RMSE      Rsquared   MAE     \n",
       "  4.794707  0.7290369  3.372915\n",
       "\n",
       "Tuning parameter 'intercept' was held constant at a value of TRUE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit lm model using 5-fold CV: model\n",
    "model <- train(\n",
    "  medv ~ . , Boston,\n",
    "  method = \"lm\",\n",
    "  trControl = trainControl(\n",
    "    method = \"cv\", number = 5,\n",
    "    verboseIter = TRUE\n",
    "  )\n",
    ")\n",
    "\n",
    "# Print model to console\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 x 5-fold cross-validation\n",
    "\n",
    "You can do more than just one iteration of cross-validation. Repeated cross-validation gives you a better estimate of the test-set error. You can also repeat the entire cross-validation procedure. This takes longer, but gives you many more out-of-sample datasets to look at and much more precise assessments of how well the model performs.\n",
    "\n",
    "One of the awesome things about the `train()` function in caret is how easy it is to run very different models or methods of cross-validation just by tweaking a few simple arguments to the function call. For example, you could repeat your entire cross-validation procedure 5 times for greater confidence in your estimates of the model's out-of-sample accuracy, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"`repeats` has no meaning for this resampling method.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold1: intercept=TRUE \n",
      "- Fold1: intercept=TRUE \n",
      "+ Fold2: intercept=TRUE \n",
      "- Fold2: intercept=TRUE \n",
      "+ Fold3: intercept=TRUE \n",
      "- Fold3: intercept=TRUE \n",
      "+ Fold4: intercept=TRUE \n",
      "- Fold4: intercept=TRUE \n",
      "+ Fold5: intercept=TRUE \n",
      "- Fold5: intercept=TRUE \n",
      "Aggregating results\n",
      "Fitting final model on full training set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear Regression \n",
       "\n",
       "506 samples\n",
       " 13 predictor\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (5 fold) \n",
       "Summary of sample sizes: 405, 403, 405, 406, 405 \n",
       "Resampling results:\n",
       "\n",
       "  RMSE      Rsquared   MAE     \n",
       "  4.794707  0.7290369  3.372915\n",
       "\n",
       "Tuning parameter 'intercept' was held constant at a value of TRUE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit lm model using 5 x 5-fold CV: model\n",
    "model <- train(\n",
    "  medv ~ ., Boston,\n",
    "  method = \"lm\",\n",
    "  trControl = trainControl(\n",
    "    method = \"cv\", number = 5,\n",
    "    repeats = 5, verboseIter = TRUE\n",
    "  )\n",
    ")\n",
    "\n",
    "# Print model to console\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions on new data\n",
    "\n",
    "Finally, the model you fit with the `train()` function has the exact same `predict()` interface as the linear regression models you fit earlier in this chapter.\n",
    "\n",
    "After fitting a model with `train()`, you can simply call `predict()` with new data, e.g:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>1</dt>\n",
       "\t\t<dd>30.0038433770169</dd>\n",
       "\t<dt>2</dt>\n",
       "\t\t<dd>25.0255623790532</dd>\n",
       "\t<dt>3</dt>\n",
       "\t\t<dd>30.5675967186017</dd>\n",
       "\t<dt>4</dt>\n",
       "\t\t<dd>28.6070364887282</dd>\n",
       "\t<dt>5</dt>\n",
       "\t\t<dd>27.9435242328731</dd>\n",
       "\t<dt>6</dt>\n",
       "\t\t<dd>25.2562844615411</dd>\n",
       "\t<dt>7</dt>\n",
       "\t\t<dd>23.0018082684855</dd>\n",
       "\t<dt>8</dt>\n",
       "\t\t<dd>19.5359884287562</dd>\n",
       "\t<dt>9</dt>\n",
       "\t\t<dd>11.5236368531306</dd>\n",
       "\t<dt>10</dt>\n",
       "\t\t<dd>18.9202621070761</dd>\n",
       "\t<dt>11</dt>\n",
       "\t\t<dd>18.9994965111326</dd>\n",
       "\t<dt>12</dt>\n",
       "\t\t<dd>21.5867956813994</dd>\n",
       "\t<dt>13</dt>\n",
       "\t\t<dd>20.9065215278353</dd>\n",
       "\t<dt>14</dt>\n",
       "\t\t<dd>19.552902810581</dd>\n",
       "\t<dt>15</dt>\n",
       "\t\t<dd>19.2834820500903</dd>\n",
       "\t<dt>16</dt>\n",
       "\t\t<dd>19.2974832082279</dd>\n",
       "\t<dt>17</dt>\n",
       "\t\t<dd>20.5275097911623</dd>\n",
       "\t<dt>18</dt>\n",
       "\t\t<dd>16.9114013467979</dd>\n",
       "\t<dt>19</dt>\n",
       "\t\t<dd>16.1780110565719</dd>\n",
       "\t<dt>20</dt>\n",
       "\t\t<dd>18.4061360333325</dd>\n",
       "\t<dt>21</dt>\n",
       "\t\t<dd>12.5238575270953</dd>\n",
       "\t<dt>22</dt>\n",
       "\t\t<dd>17.6710366949151</dd>\n",
       "\t<dt>23</dt>\n",
       "\t\t<dd>15.8328812917867</dd>\n",
       "\t<dt>24</dt>\n",
       "\t\t<dd>13.8062853463548</dd>\n",
       "\t<dt>25</dt>\n",
       "\t\t<dd>15.6783383154631</dd>\n",
       "\t<dt>26</dt>\n",
       "\t\t<dd>13.3866856086074</dd>\n",
       "\t<dt>27</dt>\n",
       "\t\t<dd>15.4639765464399</dd>\n",
       "\t<dt>28</dt>\n",
       "\t\t<dd>14.7084742806359</dd>\n",
       "\t<dt>29</dt>\n",
       "\t\t<dd>19.5473728509617</dd>\n",
       "\t<dt>30</dt>\n",
       "\t\t<dd>20.8764282022614</dd>\n",
       "\t<dt>31</dt>\n",
       "\t\t<dd>11.4551175899561</dd>\n",
       "\t<dt>32</dt>\n",
       "\t\t<dd>18.0592329458028</dd>\n",
       "\t<dt>33</dt>\n",
       "\t\t<dd>8.81105736221471</dd>\n",
       "\t<dt>34</dt>\n",
       "\t\t<dd>14.2827581412295</dd>\n",
       "\t<dt>35</dt>\n",
       "\t\t<dd>13.706758912851</dd>\n",
       "\t<dt>36</dt>\n",
       "\t\t<dd>23.8146352648329</dd>\n",
       "\t<dt>37</dt>\n",
       "\t\t<dd>22.3419370756891</dd>\n",
       "\t<dt>38</dt>\n",
       "\t\t<dd>23.1089114245444</dd>\n",
       "\t<dt>39</dt>\n",
       "\t\t<dd>22.9150261161652</dd>\n",
       "\t<dt>40</dt>\n",
       "\t\t<dd>31.3576256881649</dd>\n",
       "\t<dt>41</dt>\n",
       "\t\t<dd>34.2151022537004</dd>\n",
       "\t<dt>42</dt>\n",
       "\t\t<dd>28.0205641386917</dd>\n",
       "\t<dt>43</dt>\n",
       "\t\t<dd>25.2038662804791</dd>\n",
       "\t<dt>44</dt>\n",
       "\t\t<dd>24.609792725452</dd>\n",
       "\t<dt>45</dt>\n",
       "\t\t<dd>22.9414917565553</dd>\n",
       "\t<dt>46</dt>\n",
       "\t\t<dd>22.0966981749791</dd>\n",
       "\t<dt>47</dt>\n",
       "\t\t<dd>20.4232003230147</dd>\n",
       "\t<dt>48</dt>\n",
       "\t\t<dd>18.0365508835475</dd>\n",
       "\t<dt>49</dt>\n",
       "\t\t<dd>9.10655377218262</dd>\n",
       "\t<dt>50</dt>\n",
       "\t\t<dd>17.2060775141252</dd>\n",
       "\t<dt>51</dt>\n",
       "\t\t<dd>21.2815253532114</dd>\n",
       "\t<dt>52</dt>\n",
       "\t\t<dd>23.9722228486896</dd>\n",
       "\t<dt>53</dt>\n",
       "\t\t<dd>27.6558508020574</dd>\n",
       "\t<dt>54</dt>\n",
       "\t\t<dd>24.0490180911578</dd>\n",
       "\t<dt>55</dt>\n",
       "\t\t<dd>15.3618477010034</dd>\n",
       "\t<dt>56</dt>\n",
       "\t\t<dd>31.1526494660351</dd>\n",
       "\t<dt>57</dt>\n",
       "\t\t<dd>24.8568697815556</dd>\n",
       "\t<dt>58</dt>\n",
       "\t\t<dd>33.1091980616468</dd>\n",
       "\t<dt>59</dt>\n",
       "\t\t<dd>21.7753798714888</dd>\n",
       "\t<dt>60</dt>\n",
       "\t\t<dd>21.0849355506311</dd>\n",
       "\t<dt>61</dt>\n",
       "\t\t<dd>17.872580396885</dd>\n",
       "\t<dt>62</dt>\n",
       "\t\t<dd>18.511102080221</dd>\n",
       "\t<dt>63</dt>\n",
       "\t\t<dd>23.9874285649094</dd>\n",
       "\t<dt>64</dt>\n",
       "\t\t<dd>22.5540886888834</dd>\n",
       "\t<dt>65</dt>\n",
       "\t\t<dd>23.3730864422663</dd>\n",
       "\t<dt>66</dt>\n",
       "\t\t<dd>30.3614835814626</dd>\n",
       "\t<dt>67</dt>\n",
       "\t\t<dd>25.5305651152043</dd>\n",
       "\t<dt>68</dt>\n",
       "\t\t<dd>21.1133856417968</dd>\n",
       "\t<dt>69</dt>\n",
       "\t\t<dd>17.4215378565483</dd>\n",
       "\t<dt>70</dt>\n",
       "\t\t<dd>20.7848363266959</dd>\n",
       "\t<dt>71</dt>\n",
       "\t\t<dd>25.2014885944357</dd>\n",
       "\t<dt>72</dt>\n",
       "\t\t<dd>21.7426577045306</dd>\n",
       "\t<dt>73</dt>\n",
       "\t\t<dd>24.5574495721502</dd>\n",
       "\t<dt>74</dt>\n",
       "\t\t<dd>24.0429571201258</dd>\n",
       "\t<dt>75</dt>\n",
       "\t\t<dd>25.5049971637137</dd>\n",
       "\t<dt>76</dt>\n",
       "\t\t<dd>23.9669302002643</dd>\n",
       "\t<dt>77</dt>\n",
       "\t\t<dd>22.9454540313291</dd>\n",
       "\t<dt>78</dt>\n",
       "\t\t<dd>23.3569981843484</dd>\n",
       "\t<dt>79</dt>\n",
       "\t\t<dd>21.2619826623063</dd>\n",
       "\t<dt>80</dt>\n",
       "\t\t<dd>22.4281737318016</dd>\n",
       "\t<dt>81</dt>\n",
       "\t\t<dd>28.4057696820841</dd>\n",
       "\t<dt>82</dt>\n",
       "\t\t<dd>26.9948608623198</dd>\n",
       "\t<dt>83</dt>\n",
       "\t\t<dd>26.035762968389</dd>\n",
       "\t<dt>84</dt>\n",
       "\t\t<dd>25.0587348192214</dd>\n",
       "\t<dt>85</dt>\n",
       "\t\t<dd>24.7845667379329</dd>\n",
       "\t<dt>86</dt>\n",
       "\t\t<dd>27.7904919512374</dd>\n",
       "\t<dt>87</dt>\n",
       "\t\t<dd>22.1685342314671</dd>\n",
       "\t<dt>88</dt>\n",
       "\t\t<dd>25.8927641531678</dd>\n",
       "\t<dt>89</dt>\n",
       "\t\t<dd>30.674618271762</dd>\n",
       "\t<dt>90</dt>\n",
       "\t\t<dd>30.8311062274245</dd>\n",
       "\t<dt>91</dt>\n",
       "\t\t<dd>27.1190193960488</dd>\n",
       "\t<dt>92</dt>\n",
       "\t\t<dd>27.4126673411188</dd>\n",
       "\t<dt>93</dt>\n",
       "\t\t<dd>28.9412276243972</dd>\n",
       "\t<dt>94</dt>\n",
       "\t\t<dd>29.0810554625794</dd>\n",
       "\t<dt>95</dt>\n",
       "\t\t<dd>27.0397736487632</dd>\n",
       "\t<dt>96</dt>\n",
       "\t\t<dd>28.6245994850462</dd>\n",
       "\t<dt>97</dt>\n",
       "\t\t<dd>24.7274497764369</dd>\n",
       "\t<dt>98</dt>\n",
       "\t\t<dd>35.7815951808593</dd>\n",
       "\t<dt>99</dt>\n",
       "\t\t<dd>35.1145458726225</dd>\n",
       "\t<dt>100</dt>\n",
       "\t\t<dd>32.2510280132342</dd>\n",
       "\t<dt>101</dt>\n",
       "\t\t<dd>24.5802201889238</dd>\n",
       "\t<dt>102</dt>\n",
       "\t\t<dd>25.5941347457338</dd>\n",
       "\t<dt>103</dt>\n",
       "\t\t<dd>19.7901368354644</dd>\n",
       "\t<dt>104</dt>\n",
       "\t\t<dd>20.3116712875621</dd>\n",
       "\t<dt>105</dt>\n",
       "\t\t<dd>21.4348259114541</dd>\n",
       "\t<dt>106</dt>\n",
       "\t\t<dd>18.5399400816041</dd>\n",
       "\t<dt>107</dt>\n",
       "\t\t<dd>17.187559917816</dd>\n",
       "\t<dt>108</dt>\n",
       "\t\t<dd>20.7504902593562</dd>\n",
       "\t<dt>109</dt>\n",
       "\t\t<dd>22.6482911459716</dd>\n",
       "\t<dt>110</dt>\n",
       "\t\t<dd>19.7720366953188</dd>\n",
       "\t<dt>111</dt>\n",
       "\t\t<dd>20.6496586426969</dd>\n",
       "\t<dt>112</dt>\n",
       "\t\t<dd>26.5258674379307</dd>\n",
       "\t<dt>113</dt>\n",
       "\t\t<dd>20.7732363768603</dd>\n",
       "\t<dt>114</dt>\n",
       "\t\t<dd>20.7154831497023</dd>\n",
       "\t<dt>115</dt>\n",
       "\t\t<dd>25.1720888126412</dd>\n",
       "\t<dt>116</dt>\n",
       "\t\t<dd>20.4302559094607</dd>\n",
       "\t<dt>117</dt>\n",
       "\t\t<dd>23.3772462569496</dd>\n",
       "\t<dt>118</dt>\n",
       "\t\t<dd>23.6904326062206</dd>\n",
       "\t<dt>119</dt>\n",
       "\t\t<dd>20.3357836425832</dd>\n",
       "\t<dt>120</dt>\n",
       "\t\t<dd>20.7918087320158</dd>\n",
       "\t<dt>121</dt>\n",
       "\t\t<dd>21.9163207105348</dd>\n",
       "\t<dt>122</dt>\n",
       "\t\t<dd>22.4710777674484</dd>\n",
       "\t<dt>123</dt>\n",
       "\t\t<dd>20.5573855584772</dd>\n",
       "\t<dt>124</dt>\n",
       "\t\t<dd>16.3666197742302</dd>\n",
       "\t<dt>125</dt>\n",
       "\t\t<dd>20.5609981919357</dd>\n",
       "\t<dt>126</dt>\n",
       "\t\t<dd>22.4817844617125</dd>\n",
       "\t<dt>127</dt>\n",
       "\t\t<dd>14.617066327799</dd>\n",
       "\t<dt>128</dt>\n",
       "\t\t<dd>15.178766839628</dd>\n",
       "\t<dt>129</dt>\n",
       "\t\t<dd>18.9386859238066</dd>\n",
       "\t<dt>130</dt>\n",
       "\t\t<dd>14.0557328510346</dd>\n",
       "\t<dt>131</dt>\n",
       "\t\t<dd>20.0352739892632</dd>\n",
       "\t<dt>132</dt>\n",
       "\t\t<dd>19.4101340209302</dd>\n",
       "\t<dt>133</dt>\n",
       "\t\t<dd>20.0619156618911</dd>\n",
       "\t<dt>134</dt>\n",
       "\t\t<dd>15.7580767312742</dd>\n",
       "\t<dt>135</dt>\n",
       "\t\t<dd>13.2564523816054</dd>\n",
       "\t<dt>136</dt>\n",
       "\t\t<dd>17.2627773498836</dd>\n",
       "\t<dt>137</dt>\n",
       "\t\t<dd>15.8784188341927</dd>\n",
       "\t<dt>138</dt>\n",
       "\t\t<dd>19.3616395402927</dd>\n",
       "\t<dt>139</dt>\n",
       "\t\t<dd>13.8148389669953</dd>\n",
       "\t<dt>140</dt>\n",
       "\t\t<dd>16.4488147477162</dd>\n",
       "\t<dt>141</dt>\n",
       "\t\t<dd>13.5714193186728</dd>\n",
       "\t<dt>142</dt>\n",
       "\t\t<dd>3.98885508299596</dd>\n",
       "\t<dt>143</dt>\n",
       "\t\t<dd>14.5949547799219</dd>\n",
       "\t<dt>144</dt>\n",
       "\t\t<dd>12.1488148030321</dd>\n",
       "\t<dt>145</dt>\n",
       "\t\t<dd>8.7282236234232</dd>\n",
       "\t<dt>146</dt>\n",
       "\t\t<dd>12.0358534274744</dd>\n",
       "\t<dt>147</dt>\n",
       "\t\t<dd>15.8208205796314</dd>\n",
       "\t<dt>148</dt>\n",
       "\t\t<dd>8.51499020343318</dd>\n",
       "\t<dt>149</dt>\n",
       "\t\t<dd>9.71844139274403</dd>\n",
       "\t<dt>150</dt>\n",
       "\t\t<dd>14.8045137374037</dd>\n",
       "\t<dt>151</dt>\n",
       "\t\t<dd>20.8385815286829</dd>\n",
       "\t<dt>152</dt>\n",
       "\t\t<dd>18.3010116935799</dd>\n",
       "\t<dt>153</dt>\n",
       "\t\t<dd>20.1228255785022</dd>\n",
       "\t<dt>154</dt>\n",
       "\t\t<dd>17.2860189361229</dd>\n",
       "\t<dt>155</dt>\n",
       "\t\t<dd>22.3660022807163</dd>\n",
       "\t<dt>156</dt>\n",
       "\t\t<dd>20.1037592273599</dd>\n",
       "\t<dt>157</dt>\n",
       "\t\t<dd>13.621258906724</dd>\n",
       "\t<dt>158</dt>\n",
       "\t\t<dd>33.2598269689017</dd>\n",
       "\t<dt>159</dt>\n",
       "\t\t<dd>29.0301726798453</dd>\n",
       "\t<dt>160</dt>\n",
       "\t\t<dd>25.5675276936532</dd>\n",
       "\t<dt>161</dt>\n",
       "\t\t<dd>32.708276658354</dd>\n",
       "\t<dt>162</dt>\n",
       "\t\t<dd>36.7746701457414</dd>\n",
       "\t<dt>163</dt>\n",
       "\t\t<dd>40.5576584445806</dd>\n",
       "\t<dt>164</dt>\n",
       "\t\t<dd>41.8472816770413</dd>\n",
       "\t<dt>165</dt>\n",
       "\t\t<dd>24.7886737909977</dd>\n",
       "\t<dt>166</dt>\n",
       "\t\t<dd>25.3788923776935</dd>\n",
       "\t<dt>167</dt>\n",
       "\t\t<dd>37.2034745459996</dd>\n",
       "\t<dt>168</dt>\n",
       "\t\t<dd>23.0874874733725</dd>\n",
       "\t<dt>169</dt>\n",
       "\t\t<dd>26.4027395527366</dd>\n",
       "\t<dt>170</dt>\n",
       "\t\t<dd>26.6538211372646</dd>\n",
       "\t<dt>171</dt>\n",
       "\t\t<dd>22.5551466048872</dd>\n",
       "\t<dt>172</dt>\n",
       "\t\t<dd>24.290828117749</dd>\n",
       "\t<dt>173</dt>\n",
       "\t\t<dd>22.9765721902133</dd>\n",
       "\t<dt>174</dt>\n",
       "\t\t<dd>29.0719430769009</dd>\n",
       "\t<dt>175</dt>\n",
       "\t\t<dd>26.5219433959125</dd>\n",
       "\t<dt>176</dt>\n",
       "\t\t<dd>30.7220905554837</dd>\n",
       "\t<dt>177</dt>\n",
       "\t\t<dd>25.6166930694015</dd>\n",
       "\t<dt>178</dt>\n",
       "\t\t<dd>29.1374097942842</dd>\n",
       "\t<dt>179</dt>\n",
       "\t\t<dd>31.4357196770866</dd>\n",
       "\t<dt>180</dt>\n",
       "\t\t<dd>32.9223156800249</dd>\n",
       "\t<dt>181</dt>\n",
       "\t\t<dd>34.7244046437735</dd>\n",
       "\t<dt>182</dt>\n",
       "\t\t<dd>27.7655211065964</dd>\n",
       "\t<dt>183</dt>\n",
       "\t\t<dd>33.8878732132414</dd>\n",
       "\t<dt>184</dt>\n",
       "\t\t<dd>30.9923803601486</dd>\n",
       "\t<dt>185</dt>\n",
       "\t\t<dd>22.7182000809268</dd>\n",
       "\t<dt>186</dt>\n",
       "\t\t<dd>24.7664780989573</dd>\n",
       "\t<dt>187</dt>\n",
       "\t\t<dd>35.8849722620772</dd>\n",
       "\t<dt>188</dt>\n",
       "\t\t<dd>33.4247672203157</dd>\n",
       "\t<dt>189</dt>\n",
       "\t\t<dd>32.4119914656022</dd>\n",
       "\t<dt>190</dt>\n",
       "\t\t<dd>34.5150994934214</dd>\n",
       "\t<dt>191</dt>\n",
       "\t\t<dd>30.7610948517462</dd>\n",
       "\t<dt>192</dt>\n",
       "\t\t<dd>30.2893414067509</dd>\n",
       "\t<dt>193</dt>\n",
       "\t\t<dd>32.9191871432063</dd>\n",
       "\t<dt>194</dt>\n",
       "\t\t<dd>32.1126077140193</dd>\n",
       "\t<dt>195</dt>\n",
       "\t\t<dd>31.5587100431723</dd>\n",
       "\t<dt>196</dt>\n",
       "\t\t<dd>40.8455572142263</dd>\n",
       "\t<dt>197</dt>\n",
       "\t\t<dd>36.1277007921085</dd>\n",
       "\t<dt>198</dt>\n",
       "\t\t<dd>32.6692080997737</dd>\n",
       "\t<dt>199</dt>\n",
       "\t\t<dd>34.7046911643692</dd>\n",
       "\t<dt>200</dt>\n",
       "\t\t<dd>30.0934516181175</dd>\n",
       "\t<dt>201</dt>\n",
       "\t\t<dd>30.6439390648543</dd>\n",
       "\t<dt>202</dt>\n",
       "\t\t<dd>29.2871950136557</dd>\n",
       "\t<dt>203</dt>\n",
       "\t\t<dd>37.0714839202035</dd>\n",
       "\t<dt>204</dt>\n",
       "\t\t<dd>42.031931238384</dd>\n",
       "\t<dt>205</dt>\n",
       "\t\t<dd>43.1894984369702</dd>\n",
       "\t<dt>206</dt>\n",
       "\t\t<dd>22.6903479637775</dd>\n",
       "\t<dt>207</dt>\n",
       "\t\t<dd>23.6828471154279</dd>\n",
       "\t<dt>208</dt>\n",
       "\t\t<dd>17.8544721365808</dd>\n",
       "\t<dt>209</dt>\n",
       "\t\t<dd>23.4942899201965</dd>\n",
       "\t<dt>210</dt>\n",
       "\t\t<dd>17.0058771788018</dd>\n",
       "\t<dt>211</dt>\n",
       "\t\t<dd>22.3925109635662</dd>\n",
       "\t<dt>212</dt>\n",
       "\t\t<dd>17.0604275380357</dd>\n",
       "\t<dt>213</dt>\n",
       "\t\t<dd>22.7389292148944</dd>\n",
       "\t<dt>214</dt>\n",
       "\t\t<dd>25.2194255392078</dd>\n",
       "\t<dt>215</dt>\n",
       "\t\t<dd>11.1191673715419</dd>\n",
       "\t<dt>216</dt>\n",
       "\t\t<dd>24.5104914792796</dd>\n",
       "\t<dt>217</dt>\n",
       "\t\t<dd>26.6033477476208</dd>\n",
       "\t<dt>218</dt>\n",
       "\t\t<dd>28.3551871270884</dd>\n",
       "\t<dt>219</dt>\n",
       "\t\t<dd>24.9152546360528</dd>\n",
       "\t<dt>220</dt>\n",
       "\t\t<dd>29.6865276781239</dd>\n",
       "\t<dt>221</dt>\n",
       "\t\t<dd>33.1841974563363</dd>\n",
       "\t<dt>222</dt>\n",
       "\t\t<dd>23.7745665596465</dd>\n",
       "\t<dt>223</dt>\n",
       "\t\t<dd>32.1405195785142</dd>\n",
       "\t<dt>224</dt>\n",
       "\t\t<dd>29.7458198957233</dd>\n",
       "\t<dt>225</dt>\n",
       "\t\t<dd>38.3710245269932</dd>\n",
       "\t<dt>226</dt>\n",
       "\t\t<dd>39.8146186702757</dd>\n",
       "\t<dt>227</dt>\n",
       "\t\t<dd>37.5860575496587</dd>\n",
       "\t<dt>228</dt>\n",
       "\t\t<dd>32.3995325026375</dd>\n",
       "\t<dt>229</dt>\n",
       "\t\t<dd>35.4566524198279</dd>\n",
       "\t<dt>230</dt>\n",
       "\t\t<dd>31.2341151169737</dd>\n",
       "\t<dt>231</dt>\n",
       "\t\t<dd>24.4844922690659</dd>\n",
       "\t<dt>232</dt>\n",
       "\t\t<dd>33.2883729160192</dd>\n",
       "\t<dt>233</dt>\n",
       "\t\t<dd>38.0481048035943</dd>\n",
       "\t<dt>234</dt>\n",
       "\t\t<dd>37.163286314685</dd>\n",
       "\t<dt>235</dt>\n",
       "\t\t<dd>31.713835225278</dd>\n",
       "\t<dt>236</dt>\n",
       "\t\t<dd>25.2670557133153</dd>\n",
       "\t<dt>237</dt>\n",
       "\t\t<dd>30.1001074490383</dd>\n",
       "\t<dt>238</dt>\n",
       "\t\t<dd>32.7198715580756</dd>\n",
       "\t<dt>239</dt>\n",
       "\t\t<dd>28.4271705659764</dd>\n",
       "\t<dt>240</dt>\n",
       "\t\t<dd>28.4294067805891</dd>\n",
       "\t<dt>241</dt>\n",
       "\t\t<dd>27.2937593840445</dd>\n",
       "\t<dt>242</dt>\n",
       "\t\t<dd>23.742624782955</dd>\n",
       "\t<dt>243</dt>\n",
       "\t\t<dd>24.1200789091646</dd>\n",
       "\t<dt>244</dt>\n",
       "\t\t<dd>27.4020841376108</dd>\n",
       "\t<dt>245</dt>\n",
       "\t\t<dd>16.3285756005362</dd>\n",
       "\t<dt>246</dt>\n",
       "\t\t<dd>13.398912612969</dd>\n",
       "\t<dt>247</dt>\n",
       "\t\t<dd>20.0163877523773</dd>\n",
       "\t<dt>248</dt>\n",
       "\t\t<dd>19.8618442782307</dd>\n",
       "\t<dt>249</dt>\n",
       "\t\t<dd>21.2883130984709</dd>\n",
       "\t<dt>250</dt>\n",
       "\t\t<dd>24.0798914957638</dd>\n",
       "\t<dt>251</dt>\n",
       "\t\t<dd>24.2063354700241</dd>\n",
       "\t<dt>252</dt>\n",
       "\t\t<dd>25.0421582129268</dd>\n",
       "\t<dt>253</dt>\n",
       "\t\t<dd>24.9196400703322</dd>\n",
       "\t<dt>254</dt>\n",
       "\t\t<dd>29.9456337448009</dd>\n",
       "\t<dt>255</dt>\n",
       "\t\t<dd>23.9722831649775</dd>\n",
       "\t<dt>256</dt>\n",
       "\t\t<dd>21.6958088655397</dd>\n",
       "\t<dt>257</dt>\n",
       "\t\t<dd>37.5110923902323</dd>\n",
       "\t<dt>258</dt>\n",
       "\t\t<dd>43.3023904269433</dd>\n",
       "\t<dt>259</dt>\n",
       "\t\t<dd>36.4836142055223</dd>\n",
       "\t<dt>260</dt>\n",
       "\t\t<dd>34.9898859358965</dd>\n",
       "\t<dt>261</dt>\n",
       "\t\t<dd>34.8121150806157</dd>\n",
       "\t<dt>262</dt>\n",
       "\t\t<dd>37.1663133083515</dd>\n",
       "\t<dt>263</dt>\n",
       "\t\t<dd>40.989285008878</dd>\n",
       "\t<dt>264</dt>\n",
       "\t\t<dd>34.4463408928534</dd>\n",
       "\t<dt>265</dt>\n",
       "\t\t<dd>35.833975468617</dd>\n",
       "\t<dt>266</dt>\n",
       "\t\t<dd>28.2457429993294</dd>\n",
       "\t<dt>267</dt>\n",
       "\t\t<dd>31.2267359259333</dd>\n",
       "\t<dt>268</dt>\n",
       "\t\t<dd>40.8395574962499</dd>\n",
       "\t<dt>269</dt>\n",
       "\t\t<dd>39.3179239307984</dd>\n",
       "\t<dt>270</dt>\n",
       "\t\t<dd>25.7081790522638</dd>\n",
       "\t<dt>271</dt>\n",
       "\t\t<dd>22.3029553295129</dd>\n",
       "\t<dt>272</dt>\n",
       "\t\t<dd>27.2034097188608</dd>\n",
       "\t<dt>273</dt>\n",
       "\t\t<dd>28.5116947200694</dd>\n",
       "\t<dt>274</dt>\n",
       "\t\t<dd>35.4767659823203</dd>\n",
       "\t<dt>275</dt>\n",
       "\t\t<dd>36.1063916433224</dd>\n",
       "\t<dt>276</dt>\n",
       "\t\t<dd>33.7966827352104</dd>\n",
       "\t<dt>277</dt>\n",
       "\t\t<dd>35.6108585794039</dd>\n",
       "\t<dt>278</dt>\n",
       "\t\t<dd>34.8399338194243</dd>\n",
       "\t<dt>279</dt>\n",
       "\t\t<dd>30.3519265579237</dd>\n",
       "\t<dt>280</dt>\n",
       "\t\t<dd>35.3098070094815</dd>\n",
       "\t<dt>281</dt>\n",
       "\t\t<dd>38.7975696598869</dd>\n",
       "\t<dt>282</dt>\n",
       "\t\t<dd>34.3312318595864</dd>\n",
       "\t<dt>283</dt>\n",
       "\t\t<dd>40.3396307477481</dd>\n",
       "\t<dt>284</dt>\n",
       "\t\t<dd>44.6730833873132</dd>\n",
       "\t<dt>285</dt>\n",
       "\t\t<dd>31.5968908550318</dd>\n",
       "\t<dt>286</dt>\n",
       "\t\t<dd>27.3565922979884</dd>\n",
       "\t<dt>287</dt>\n",
       "\t\t<dd>20.1017415410065</dd>\n",
       "\t<dt>288</dt>\n",
       "\t\t<dd>27.04206673569</dd>\n",
       "\t<dt>289</dt>\n",
       "\t\t<dd>27.2136458044923</dd>\n",
       "\t<dt>290</dt>\n",
       "\t\t<dd>26.9139583935991</dd>\n",
       "\t<dt>291</dt>\n",
       "\t\t<dd>33.4356331055855</dd>\n",
       "\t<dt>292</dt>\n",
       "\t\t<dd>34.4034963301773</dd>\n",
       "\t<dt>293</dt>\n",
       "\t\t<dd>31.8333982014346</dd>\n",
       "\t<dt>294</dt>\n",
       "\t\t<dd>25.8178323689479</dd>\n",
       "\t<dt>295</dt>\n",
       "\t\t<dd>24.4298234832478</dd>\n",
       "\t<dt>296</dt>\n",
       "\t\t<dd>28.4576433683768</dd>\n",
       "\t<dt>297</dt>\n",
       "\t\t<dd>27.3626699852368</dd>\n",
       "\t<dt>298</dt>\n",
       "\t\t<dd>19.5392875808004</dd>\n",
       "\t<dt>299</dt>\n",
       "\t\t<dd>29.1130984409263</dd>\n",
       "\t<dt>300</dt>\n",
       "\t\t<dd>31.9105461078277</dd>\n",
       "\t<dt>301</dt>\n",
       "\t\t<dd>30.7715944851236</dd>\n",
       "\t<dt>302</dt>\n",
       "\t\t<dd>28.9427587127511</dd>\n",
       "\t<dt>303</dt>\n",
       "\t\t<dd>28.8819102240024</dd>\n",
       "\t<dt>304</dt>\n",
       "\t\t<dd>32.7988723186604</dd>\n",
       "\t<dt>305</dt>\n",
       "\t\t<dd>33.2090545594115</dd>\n",
       "\t<dt>306</dt>\n",
       "\t\t<dd>30.7683179231372</dd>\n",
       "\t<dt>307</dt>\n",
       "\t\t<dd>35.5622685696683</dd>\n",
       "\t<dt>308</dt>\n",
       "\t\t<dd>32.7090512370312</dd>\n",
       "\t<dt>309</dt>\n",
       "\t\t<dd>28.6424423662638</dd>\n",
       "\t<dt>310</dt>\n",
       "\t\t<dd>23.5896582710816</dd>\n",
       "\t<dt>311</dt>\n",
       "\t\t<dd>18.5426689677346</dd>\n",
       "\t<dt>312</dt>\n",
       "\t\t<dd>26.8788984289197</dd>\n",
       "\t<dt>313</dt>\n",
       "\t\t<dd>23.2813397881904</dd>\n",
       "\t<dt>314</dt>\n",
       "\t\t<dd>25.5458024605245</dd>\n",
       "\t<dt>315</dt>\n",
       "\t\t<dd>25.4812005709242</dd>\n",
       "\t<dt>316</dt>\n",
       "\t\t<dd>20.5390990114911</dd>\n",
       "\t<dt>317</dt>\n",
       "\t\t<dd>17.6157257282673</dd>\n",
       "\t<dt>318</dt>\n",
       "\t\t<dd>18.3758168640772</dd>\n",
       "\t<dt>319</dt>\n",
       "\t\t<dd>24.290702769397</dd>\n",
       "\t<dt>320</dt>\n",
       "\t\t<dd>21.3252903916503</dd>\n",
       "\t<dt>321</dt>\n",
       "\t\t<dd>24.8868224406516</dd>\n",
       "\t<dt>322</dt>\n",
       "\t\t<dd>24.869372815388</dd>\n",
       "\t<dt>323</dt>\n",
       "\t\t<dd>22.8695244743481</dd>\n",
       "\t<dt>324</dt>\n",
       "\t\t<dd>19.4512379061529</dd>\n",
       "\t<dt>325</dt>\n",
       "\t\t<dd>25.1178340113773</dd>\n",
       "\t<dt>326</dt>\n",
       "\t\t<dd>24.6678691322565</dd>\n",
       "\t<dt>327</dt>\n",
       "\t\t<dd>23.6807617707272</dd>\n",
       "\t<dt>328</dt>\n",
       "\t\t<dd>19.3408961632679</dd>\n",
       "\t<dt>329</dt>\n",
       "\t\t<dd>21.1741810541345</dd>\n",
       "\t<dt>330</dt>\n",
       "\t\t<dd>24.2524907349313</dd>\n",
       "\t<dt>331</dt>\n",
       "\t\t<dd>21.5926089386147</dd>\n",
       "\t<dt>332</dt>\n",
       "\t\t<dd>19.98446605389</dd>\n",
       "\t<dt>333</dt>\n",
       "\t\t<dd>23.3388799978055</dd>\n",
       "\t<dt>334</dt>\n",
       "\t\t<dd>22.140606924958</dd>\n",
       "\t<dt>335</dt>\n",
       "\t\t<dd>21.555099290216</dd>\n",
       "\t<dt>336</dt>\n",
       "\t\t<dd>20.6187290679767</dd>\n",
       "\t<dt>337</dt>\n",
       "\t\t<dd>20.1609717632301</dd>\n",
       "\t<dt>338</dt>\n",
       "\t\t<dd>19.2849038729266</dd>\n",
       "\t<dt>339</dt>\n",
       "\t\t<dd>22.1667232042712</dd>\n",
       "\t<dt>340</dt>\n",
       "\t\t<dd>21.2496577442297</dd>\n",
       "\t<dt>341</dt>\n",
       "\t\t<dd>21.4293930542572</dd>\n",
       "\t<dt>342</dt>\n",
       "\t\t<dd>30.327887960245</dd>\n",
       "\t<dt>343</dt>\n",
       "\t\t<dd>22.0473497508577</dd>\n",
       "\t<dt>344</dt>\n",
       "\t\t<dd>27.7064791245651</dd>\n",
       "\t<dt>345</dt>\n",
       "\t\t<dd>28.5479411681596</dd>\n",
       "\t<dt>346</dt>\n",
       "\t\t<dd>16.5450112143803</dd>\n",
       "\t<dt>347</dt>\n",
       "\t\t<dd>14.7835964098647</dd>\n",
       "\t<dt>348</dt>\n",
       "\t\t<dd>25.2738008168571</dd>\n",
       "\t<dt>349</dt>\n",
       "\t\t<dd>27.542051173934</dd>\n",
       "\t<dt>350</dt>\n",
       "\t\t<dd>22.1483756244775</dd>\n",
       "\t<dt>351</dt>\n",
       "\t\t<dd>20.4594409474613</dd>\n",
       "\t<dt>352</dt>\n",
       "\t\t<dd>20.5460542274535</dd>\n",
       "\t<dt>353</dt>\n",
       "\t\t<dd>16.8806382656481</dd>\n",
       "\t<dt>354</dt>\n",
       "\t\t<dd>25.4025350580152</dd>\n",
       "\t<dt>355</dt>\n",
       "\t\t<dd>14.3248663190426</dd>\n",
       "\t<dt>356</dt>\n",
       "\t\t<dd>16.5948846173842</dd>\n",
       "\t<dt>357</dt>\n",
       "\t\t<dd>19.6370469133015</dd>\n",
       "\t<dt>358</dt>\n",
       "\t\t<dd>22.7180660747844</dd>\n",
       "\t<dt>359</dt>\n",
       "\t\t<dd>22.2021888675356</dd>\n",
       "\t<dt>360</dt>\n",
       "\t\t<dd>19.2054805739532</dd>\n",
       "\t<dt>361</dt>\n",
       "\t\t<dd>22.6661610513382</dd>\n",
       "\t<dt>362</dt>\n",
       "\t\t<dd>18.9319261816695</dd>\n",
       "\t<dt>363</dt>\n",
       "\t\t<dd>18.2284680430267</dd>\n",
       "\t<dt>364</dt>\n",
       "\t\t<dd>20.2315081138551</dd>\n",
       "\t<dt>365</dt>\n",
       "\t\t<dd>37.4944738996952</dd>\n",
       "\t<dt>366</dt>\n",
       "\t\t<dd>14.2819073412001</dd>\n",
       "\t<dt>367</dt>\n",
       "\t\t<dd>15.5428624827946</dd>\n",
       "\t<dt>368</dt>\n",
       "\t\t<dd>10.8316232410572</dd>\n",
       "\t<dt>369</dt>\n",
       "\t\t<dd>23.8007290220917</dd>\n",
       "\t<dt>370</dt>\n",
       "\t\t<dd>32.6440736012499</dd>\n",
       "\t<dt>371</dt>\n",
       "\t\t<dd>34.6068404244668</dd>\n",
       "\t<dt>372</dt>\n",
       "\t\t<dd>24.9433133251444</dd>\n",
       "\t<dt>373</dt>\n",
       "\t\t<dd>25.9998091007361</dd>\n",
       "\t<dt>374</dt>\n",
       "\t\t<dd>6.12632499643095</dd>\n",
       "\t<dt>375</dt>\n",
       "\t\t<dd>0.777798059570536</dd>\n",
       "\t<dt>376</dt>\n",
       "\t\t<dd>25.3071306417183</dd>\n",
       "\t<dt>377</dt>\n",
       "\t\t<dd>17.7406106453159</dd>\n",
       "\t<dt>378</dt>\n",
       "\t\t<dd>20.2327441401876</dd>\n",
       "\t<dt>379</dt>\n",
       "\t\t<dd>15.8333130075964</dd>\n",
       "\t<dt>380</dt>\n",
       "\t\t<dd>16.8351258716529</dd>\n",
       "\t<dt>381</dt>\n",
       "\t\t<dd>14.3699482548819</dd>\n",
       "\t<dt>382</dt>\n",
       "\t\t<dd>18.4768283307821</dd>\n",
       "\t<dt>383</dt>\n",
       "\t\t<dd>13.4276828046913</dd>\n",
       "\t<dt>384</dt>\n",
       "\t\t<dd>13.0617751178283</dd>\n",
       "\t<dt>385</dt>\n",
       "\t\t<dd>3.27918116096131</dd>\n",
       "\t<dt>386</dt>\n",
       "\t\t<dd>8.06022170824221</dd>\n",
       "\t<dt>387</dt>\n",
       "\t\t<dd>6.12842196430475</dd>\n",
       "\t<dt>388</dt>\n",
       "\t\t<dd>5.6186480998375</dd>\n",
       "\t<dt>389</dt>\n",
       "\t\t<dd>6.45198569812686</dd>\n",
       "\t<dt>390</dt>\n",
       "\t\t<dd>14.2076473519415</dd>\n",
       "\t<dt>391</dt>\n",
       "\t\t<dd>17.2122518314194</dd>\n",
       "\t<dt>392</dt>\n",
       "\t\t<dd>17.298872651552</dd>\n",
       "\t<dt>393</dt>\n",
       "\t\t<dd>9.89116643227555</dd>\n",
       "\t<dt>394</dt>\n",
       "\t\t<dd>20.2212419349943</dd>\n",
       "\t<dt>395</dt>\n",
       "\t\t<dd>17.9418117532261</dd>\n",
       "\t<dt>396</dt>\n",
       "\t\t<dd>20.3044578274683</dd>\n",
       "\t<dt>397</dt>\n",
       "\t\t<dd>19.2955907547463</dd>\n",
       "\t<dt>398</dt>\n",
       "\t\t<dd>16.3363277931955</dd>\n",
       "\t<dt>399</dt>\n",
       "\t\t<dd>6.55162319068339</dd>\n",
       "\t<dt>400</dt>\n",
       "\t\t<dd>10.8901677809486</dd>\n",
       "\t<dt>401</dt>\n",
       "\t\t<dd>11.881458707413</dd>\n",
       "\t<dt>402</dt>\n",
       "\t\t<dd>17.8117450679153</dd>\n",
       "\t<dt>403</dt>\n",
       "\t\t<dd>18.2612658716956</dd>\n",
       "\t<dt>404</dt>\n",
       "\t\t<dd>12.9794878073526</dd>\n",
       "\t<dt>405</dt>\n",
       "\t\t<dd>7.3781636071946</dd>\n",
       "\t<dt>406</dt>\n",
       "\t\t<dd>8.21115861388474</dd>\n",
       "\t<dt>407</dt>\n",
       "\t\t<dd>8.06626192987007</dd>\n",
       "\t<dt>408</dt>\n",
       "\t\t<dd>19.9829478598214</dd>\n",
       "\t<dt>409</dt>\n",
       "\t\t<dd>13.7075636921068</dd>\n",
       "\t<dt>410</dt>\n",
       "\t\t<dd>19.8526845447722</dd>\n",
       "\t<dt>411</dt>\n",
       "\t\t<dd>15.2230829768811</dd>\n",
       "\t<dt>412</dt>\n",
       "\t\t<dd>16.9607198099616</dd>\n",
       "\t<dt>413</dt>\n",
       "\t\t<dd>1.71851806781777</dd>\n",
       "\t<dt>414</dt>\n",
       "\t\t<dd>11.8057838715304</dd>\n",
       "\t<dt>415</dt>\n",
       "\t\t<dd>-4.28131070918481</dd>\n",
       "\t<dt>416</dt>\n",
       "\t\t<dd>9.58376736863224</dd>\n",
       "\t<dt>417</dt>\n",
       "\t\t<dd>13.3666081096374</dd>\n",
       "\t<dt>418</dt>\n",
       "\t\t<dd>6.89562362585981</dd>\n",
       "\t<dt>419</dt>\n",
       "\t\t<dd>6.1477985203214</dd>\n",
       "\t<dt>420</dt>\n",
       "\t\t<dd>14.6066179423843</dd>\n",
       "\t<dt>421</dt>\n",
       "\t\t<dd>19.6000267019875</dd>\n",
       "\t<dt>422</dt>\n",
       "\t\t<dd>18.1242747588778</dd>\n",
       "\t<dt>423</dt>\n",
       "\t\t<dd>18.5217713215163</dd>\n",
       "\t<dt>424</dt>\n",
       "\t\t<dd>13.1752861045673</dd>\n",
       "\t<dt>425</dt>\n",
       "\t\t<dd>14.6261762418156</dd>\n",
       "\t<dt>426</dt>\n",
       "\t\t<dd>9.92374975966266</dd>\n",
       "\t<dt>427</dt>\n",
       "\t\t<dd>16.3459064665755</dd>\n",
       "\t<dt>428</dt>\n",
       "\t\t<dd>14.0751942552311</dd>\n",
       "\t<dt>429</dt>\n",
       "\t\t<dd>14.2575624260639</dd>\n",
       "\t<dt>430</dt>\n",
       "\t\t<dd>13.0423478746219</dd>\n",
       "\t<dt>431</dt>\n",
       "\t\t<dd>18.1595569337206</dd>\n",
       "\t<dt>432</dt>\n",
       "\t\t<dd>18.6955435411763</dd>\n",
       "\t<dt>433</dt>\n",
       "\t\t<dd>21.5272830021116</dd>\n",
       "\t<dt>434</dt>\n",
       "\t\t<dd>17.0314186086468</dd>\n",
       "\t<dt>435</dt>\n",
       "\t\t<dd>15.9609043533334</dd>\n",
       "\t<dt>436</dt>\n",
       "\t\t<dd>13.3614161071054</dd>\n",
       "\t<dt>437</dt>\n",
       "\t\t<dd>14.5207938390617</dd>\n",
       "\t<dt>438</dt>\n",
       "\t\t<dd>8.81976005426038</dd>\n",
       "\t<dt>439</dt>\n",
       "\t\t<dd>4.86751102276619</dd>\n",
       "\t<dt>440</dt>\n",
       "\t\t<dd>13.0659131295214</dd>\n",
       "\t<dt>441</dt>\n",
       "\t\t<dd>12.7060969931816</dd>\n",
       "\t<dt>442</dt>\n",
       "\t\t<dd>17.2955805920873</dd>\n",
       "\t<dt>443</dt>\n",
       "\t\t<dd>18.740485001511</dd>\n",
       "\t<dt>444</dt>\n",
       "\t\t<dd>18.0590102945304</dd>\n",
       "\t<dt>445</dt>\n",
       "\t\t<dd>11.5147468339056</dd>\n",
       "\t<dt>446</dt>\n",
       "\t\t<dd>11.974003586659</dd>\n",
       "\t<dt>447</dt>\n",
       "\t\t<dd>17.6834461847101</dd>\n",
       "\t<dt>448</dt>\n",
       "\t\t<dd>18.1269523902661</dd>\n",
       "\t<dt>449</dt>\n",
       "\t\t<dd>17.5183465039488</dd>\n",
       "\t<dt>450</dt>\n",
       "\t\t<dd>17.2274250730164</dd>\n",
       "\t<dt>451</dt>\n",
       "\t\t<dd>16.5227163133693</dd>\n",
       "\t<dt>452</dt>\n",
       "\t\t<dd>19.412910950902</dd>\n",
       "\t<dt>453</dt>\n",
       "\t\t<dd>18.5821523618059</dd>\n",
       "\t<dt>454</dt>\n",
       "\t\t<dd>22.4894479081572</dd>\n",
       "\t<dt>455</dt>\n",
       "\t\t<dd>15.2800013303762</dd>\n",
       "\t<dt>456</dt>\n",
       "\t\t<dd>15.8208933517198</dd>\n",
       "\t<dt>457</dt>\n",
       "\t\t<dd>12.6872558131638</dd>\n",
       "\t<dt>458</dt>\n",
       "\t\t<dd>12.8763379019582</dd>\n",
       "\t<dt>459</dt>\n",
       "\t\t<dd>17.1866853085876</dd>\n",
       "\t<dt>460</dt>\n",
       "\t\t<dd>18.5124760929217</dd>\n",
       "\t<dt>461</dt>\n",
       "\t\t<dd>19.0486053324833</dd>\n",
       "\t<dt>462</dt>\n",
       "\t\t<dd>20.1720892732935</dd>\n",
       "\t<dt>463</dt>\n",
       "\t\t<dd>19.7740731969026</dd>\n",
       "\t<dt>464</dt>\n",
       "\t\t<dd>22.4294076788369</dd>\n",
       "\t<dt>465</dt>\n",
       "\t\t<dd>20.3191185432229</dd>\n",
       "\t<dt>466</dt>\n",
       "\t\t<dd>17.8861625321675</dd>\n",
       "\t<dt>467</dt>\n",
       "\t\t<dd>14.374785228368</dd>\n",
       "\t<dt>468</dt>\n",
       "\t\t<dd>16.9477685071966</dd>\n",
       "\t<dt>469</dt>\n",
       "\t\t<dd>16.9840576215105</dd>\n",
       "\t<dt>470</dt>\n",
       "\t\t<dd>18.5883839676952</dd>\n",
       "\t<dt>471</dt>\n",
       "\t\t<dd>20.1671944106316</dd>\n",
       "\t<dt>472</dt>\n",
       "\t\t<dd>22.9771803171487</dd>\n",
       "\t<dt>473</dt>\n",
       "\t\t<dd>22.4558072635363</dd>\n",
       "\t<dt>474</dt>\n",
       "\t\t<dd>25.5782462654826</dd>\n",
       "\t<dt>475</dt>\n",
       "\t\t<dd>16.3914763163556</dd>\n",
       "\t<dt>476</dt>\n",
       "\t\t<dd>16.1114628043018</dd>\n",
       "\t<dt>477</dt>\n",
       "\t\t<dd>20.5348159958905</dd>\n",
       "\t<dt>478</dt>\n",
       "\t\t<dd>11.5427273815651</dd>\n",
       "\t<dt>479</dt>\n",
       "\t\t<dd>19.2049630449181</dd>\n",
       "\t<dt>480</dt>\n",
       "\t\t<dd>21.8627639063056</dd>\n",
       "\t<dt>481</dt>\n",
       "\t\t<dd>23.4687886630714</dd>\n",
       "\t<dt>482</dt>\n",
       "\t\t<dd>27.0988731548861</dd>\n",
       "\t<dt>483</dt>\n",
       "\t\t<dd>28.5699430166971</dd>\n",
       "\t<dt>484</dt>\n",
       "\t\t<dd>21.0839878308985</dd>\n",
       "\t<dt>485</dt>\n",
       "\t\t<dd>19.4551619556368</dd>\n",
       "\t<dt>486</dt>\n",
       "\t\t<dd>22.2222591407908</dd>\n",
       "\t<dt>487</dt>\n",
       "\t\t<dd>19.655919607581</dd>\n",
       "\t<dt>488</dt>\n",
       "\t\t<dd>21.3253610439986</dd>\n",
       "\t<dt>489</dt>\n",
       "\t\t<dd>11.8558371664913</dd>\n",
       "\t<dt>490</dt>\n",
       "\t\t<dd>8.22386686792797</dd>\n",
       "\t<dt>491</dt>\n",
       "\t\t<dd>3.66399672048603</dd>\n",
       "\t<dt>492</dt>\n",
       "\t\t<dd>13.7590853820706</dd>\n",
       "\t<dt>493</dt>\n",
       "\t\t<dd>15.9311854513375</dd>\n",
       "\t<dt>494</dt>\n",
       "\t\t<dd>20.6266205443521</dd>\n",
       "\t<dt>495</dt>\n",
       "\t\t<dd>20.612494139848</dd>\n",
       "\t<dt>496</dt>\n",
       "\t\t<dd>16.8854196394897</dd>\n",
       "\t<dt>497</dt>\n",
       "\t\t<dd>14.0132078709419</dd>\n",
       "\t<dt>498</dt>\n",
       "\t\t<dd>19.108541438765</dd>\n",
       "\t<dt>499</dt>\n",
       "\t\t<dd>21.2980517444353</dd>\n",
       "\t<dt>500</dt>\n",
       "\t\t<dd>18.454988408699</dd>\n",
       "\t<dt>501</dt>\n",
       "\t\t<dd>20.4687084696061</dd>\n",
       "\t<dt>502</dt>\n",
       "\t\t<dd>23.5333405466671</dd>\n",
       "\t<dt>503</dt>\n",
       "\t\t<dd>22.3757189202883</dd>\n",
       "\t<dt>504</dt>\n",
       "\t\t<dd>27.6274260950359</dd>\n",
       "\t<dt>505</dt>\n",
       "\t\t<dd>26.1279668065958</dd>\n",
       "\t<dt>506</dt>\n",
       "\t\t<dd>22.3442122929037</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[1] 30.0038433770169\n",
       "\\item[2] 25.0255623790532\n",
       "\\item[3] 30.5675967186017\n",
       "\\item[4] 28.6070364887282\n",
       "\\item[5] 27.9435242328731\n",
       "\\item[6] 25.2562844615411\n",
       "\\item[7] 23.0018082684855\n",
       "\\item[8] 19.5359884287562\n",
       "\\item[9] 11.5236368531306\n",
       "\\item[10] 18.9202621070761\n",
       "\\item[11] 18.9994965111326\n",
       "\\item[12] 21.5867956813994\n",
       "\\item[13] 20.9065215278353\n",
       "\\item[14] 19.552902810581\n",
       "\\item[15] 19.2834820500903\n",
       "\\item[16] 19.2974832082279\n",
       "\\item[17] 20.5275097911623\n",
       "\\item[18] 16.9114013467979\n",
       "\\item[19] 16.1780110565719\n",
       "\\item[20] 18.4061360333325\n",
       "\\item[21] 12.5238575270953\n",
       "\\item[22] 17.6710366949151\n",
       "\\item[23] 15.8328812917867\n",
       "\\item[24] 13.8062853463548\n",
       "\\item[25] 15.6783383154631\n",
       "\\item[26] 13.3866856086074\n",
       "\\item[27] 15.4639765464399\n",
       "\\item[28] 14.7084742806359\n",
       "\\item[29] 19.5473728509617\n",
       "\\item[30] 20.8764282022614\n",
       "\\item[31] 11.4551175899561\n",
       "\\item[32] 18.0592329458028\n",
       "\\item[33] 8.81105736221471\n",
       "\\item[34] 14.2827581412295\n",
       "\\item[35] 13.706758912851\n",
       "\\item[36] 23.8146352648329\n",
       "\\item[37] 22.3419370756891\n",
       "\\item[38] 23.1089114245444\n",
       "\\item[39] 22.9150261161652\n",
       "\\item[40] 31.3576256881649\n",
       "\\item[41] 34.2151022537004\n",
       "\\item[42] 28.0205641386917\n",
       "\\item[43] 25.2038662804791\n",
       "\\item[44] 24.609792725452\n",
       "\\item[45] 22.9414917565553\n",
       "\\item[46] 22.0966981749791\n",
       "\\item[47] 20.4232003230147\n",
       "\\item[48] 18.0365508835475\n",
       "\\item[49] 9.10655377218262\n",
       "\\item[50] 17.2060775141252\n",
       "\\item[51] 21.2815253532114\n",
       "\\item[52] 23.9722228486896\n",
       "\\item[53] 27.6558508020574\n",
       "\\item[54] 24.0490180911578\n",
       "\\item[55] 15.3618477010034\n",
       "\\item[56] 31.1526494660351\n",
       "\\item[57] 24.8568697815556\n",
       "\\item[58] 33.1091980616468\n",
       "\\item[59] 21.7753798714888\n",
       "\\item[60] 21.0849355506311\n",
       "\\item[61] 17.872580396885\n",
       "\\item[62] 18.511102080221\n",
       "\\item[63] 23.9874285649094\n",
       "\\item[64] 22.5540886888834\n",
       "\\item[65] 23.3730864422663\n",
       "\\item[66] 30.3614835814626\n",
       "\\item[67] 25.5305651152043\n",
       "\\item[68] 21.1133856417968\n",
       "\\item[69] 17.4215378565483\n",
       "\\item[70] 20.7848363266959\n",
       "\\item[71] 25.2014885944357\n",
       "\\item[72] 21.7426577045306\n",
       "\\item[73] 24.5574495721502\n",
       "\\item[74] 24.0429571201258\n",
       "\\item[75] 25.5049971637137\n",
       "\\item[76] 23.9669302002643\n",
       "\\item[77] 22.9454540313291\n",
       "\\item[78] 23.3569981843484\n",
       "\\item[79] 21.2619826623063\n",
       "\\item[80] 22.4281737318016\n",
       "\\item[81] 28.4057696820841\n",
       "\\item[82] 26.9948608623198\n",
       "\\item[83] 26.035762968389\n",
       "\\item[84] 25.0587348192214\n",
       "\\item[85] 24.7845667379329\n",
       "\\item[86] 27.7904919512374\n",
       "\\item[87] 22.1685342314671\n",
       "\\item[88] 25.8927641531678\n",
       "\\item[89] 30.674618271762\n",
       "\\item[90] 30.8311062274245\n",
       "\\item[91] 27.1190193960488\n",
       "\\item[92] 27.4126673411188\n",
       "\\item[93] 28.9412276243972\n",
       "\\item[94] 29.0810554625794\n",
       "\\item[95] 27.0397736487632\n",
       "\\item[96] 28.6245994850462\n",
       "\\item[97] 24.7274497764369\n",
       "\\item[98] 35.7815951808593\n",
       "\\item[99] 35.1145458726225\n",
       "\\item[100] 32.2510280132342\n",
       "\\item[101] 24.5802201889238\n",
       "\\item[102] 25.5941347457338\n",
       "\\item[103] 19.7901368354644\n",
       "\\item[104] 20.3116712875621\n",
       "\\item[105] 21.4348259114541\n",
       "\\item[106] 18.5399400816041\n",
       "\\item[107] 17.187559917816\n",
       "\\item[108] 20.7504902593562\n",
       "\\item[109] 22.6482911459716\n",
       "\\item[110] 19.7720366953188\n",
       "\\item[111] 20.6496586426969\n",
       "\\item[112] 26.5258674379307\n",
       "\\item[113] 20.7732363768603\n",
       "\\item[114] 20.7154831497023\n",
       "\\item[115] 25.1720888126412\n",
       "\\item[116] 20.4302559094607\n",
       "\\item[117] 23.3772462569496\n",
       "\\item[118] 23.6904326062206\n",
       "\\item[119] 20.3357836425832\n",
       "\\item[120] 20.7918087320158\n",
       "\\item[121] 21.9163207105348\n",
       "\\item[122] 22.4710777674484\n",
       "\\item[123] 20.5573855584772\n",
       "\\item[124] 16.3666197742302\n",
       "\\item[125] 20.5609981919357\n",
       "\\item[126] 22.4817844617125\n",
       "\\item[127] 14.617066327799\n",
       "\\item[128] 15.178766839628\n",
       "\\item[129] 18.9386859238066\n",
       "\\item[130] 14.0557328510346\n",
       "\\item[131] 20.0352739892632\n",
       "\\item[132] 19.4101340209302\n",
       "\\item[133] 20.0619156618911\n",
       "\\item[134] 15.7580767312742\n",
       "\\item[135] 13.2564523816054\n",
       "\\item[136] 17.2627773498836\n",
       "\\item[137] 15.8784188341927\n",
       "\\item[138] 19.3616395402927\n",
       "\\item[139] 13.8148389669953\n",
       "\\item[140] 16.4488147477162\n",
       "\\item[141] 13.5714193186728\n",
       "\\item[142] 3.98885508299596\n",
       "\\item[143] 14.5949547799219\n",
       "\\item[144] 12.1488148030321\n",
       "\\item[145] 8.7282236234232\n",
       "\\item[146] 12.0358534274744\n",
       "\\item[147] 15.8208205796314\n",
       "\\item[148] 8.51499020343318\n",
       "\\item[149] 9.71844139274403\n",
       "\\item[150] 14.8045137374037\n",
       "\\item[151] 20.8385815286829\n",
       "\\item[152] 18.3010116935799\n",
       "\\item[153] 20.1228255785022\n",
       "\\item[154] 17.2860189361229\n",
       "\\item[155] 22.3660022807163\n",
       "\\item[156] 20.1037592273599\n",
       "\\item[157] 13.621258906724\n",
       "\\item[158] 33.2598269689017\n",
       "\\item[159] 29.0301726798453\n",
       "\\item[160] 25.5675276936532\n",
       "\\item[161] 32.708276658354\n",
       "\\item[162] 36.7746701457414\n",
       "\\item[163] 40.5576584445806\n",
       "\\item[164] 41.8472816770413\n",
       "\\item[165] 24.7886737909977\n",
       "\\item[166] 25.3788923776935\n",
       "\\item[167] 37.2034745459996\n",
       "\\item[168] 23.0874874733725\n",
       "\\item[169] 26.4027395527366\n",
       "\\item[170] 26.6538211372646\n",
       "\\item[171] 22.5551466048872\n",
       "\\item[172] 24.290828117749\n",
       "\\item[173] 22.9765721902133\n",
       "\\item[174] 29.0719430769009\n",
       "\\item[175] 26.5219433959125\n",
       "\\item[176] 30.7220905554837\n",
       "\\item[177] 25.6166930694015\n",
       "\\item[178] 29.1374097942842\n",
       "\\item[179] 31.4357196770866\n",
       "\\item[180] 32.9223156800249\n",
       "\\item[181] 34.7244046437735\n",
       "\\item[182] 27.7655211065964\n",
       "\\item[183] 33.8878732132414\n",
       "\\item[184] 30.9923803601486\n",
       "\\item[185] 22.7182000809268\n",
       "\\item[186] 24.7664780989573\n",
       "\\item[187] 35.8849722620772\n",
       "\\item[188] 33.4247672203157\n",
       "\\item[189] 32.4119914656022\n",
       "\\item[190] 34.5150994934214\n",
       "\\item[191] 30.7610948517462\n",
       "\\item[192] 30.2893414067509\n",
       "\\item[193] 32.9191871432063\n",
       "\\item[194] 32.1126077140193\n",
       "\\item[195] 31.5587100431723\n",
       "\\item[196] 40.8455572142263\n",
       "\\item[197] 36.1277007921085\n",
       "\\item[198] 32.6692080997737\n",
       "\\item[199] 34.7046911643692\n",
       "\\item[200] 30.0934516181175\n",
       "\\item[201] 30.6439390648543\n",
       "\\item[202] 29.2871950136557\n",
       "\\item[203] 37.0714839202035\n",
       "\\item[204] 42.031931238384\n",
       "\\item[205] 43.1894984369702\n",
       "\\item[206] 22.6903479637775\n",
       "\\item[207] 23.6828471154279\n",
       "\\item[208] 17.8544721365808\n",
       "\\item[209] 23.4942899201965\n",
       "\\item[210] 17.0058771788018\n",
       "\\item[211] 22.3925109635662\n",
       "\\item[212] 17.0604275380357\n",
       "\\item[213] 22.7389292148944\n",
       "\\item[214] 25.2194255392078\n",
       "\\item[215] 11.1191673715419\n",
       "\\item[216] 24.5104914792796\n",
       "\\item[217] 26.6033477476208\n",
       "\\item[218] 28.3551871270884\n",
       "\\item[219] 24.9152546360528\n",
       "\\item[220] 29.6865276781239\n",
       "\\item[221] 33.1841974563363\n",
       "\\item[222] 23.7745665596465\n",
       "\\item[223] 32.1405195785142\n",
       "\\item[224] 29.7458198957233\n",
       "\\item[225] 38.3710245269932\n",
       "\\item[226] 39.8146186702757\n",
       "\\item[227] 37.5860575496587\n",
       "\\item[228] 32.3995325026375\n",
       "\\item[229] 35.4566524198279\n",
       "\\item[230] 31.2341151169737\n",
       "\\item[231] 24.4844922690659\n",
       "\\item[232] 33.2883729160192\n",
       "\\item[233] 38.0481048035943\n",
       "\\item[234] 37.163286314685\n",
       "\\item[235] 31.713835225278\n",
       "\\item[236] 25.2670557133153\n",
       "\\item[237] 30.1001074490383\n",
       "\\item[238] 32.7198715580756\n",
       "\\item[239] 28.4271705659764\n",
       "\\item[240] 28.4294067805891\n",
       "\\item[241] 27.2937593840445\n",
       "\\item[242] 23.742624782955\n",
       "\\item[243] 24.1200789091646\n",
       "\\item[244] 27.4020841376108\n",
       "\\item[245] 16.3285756005362\n",
       "\\item[246] 13.398912612969\n",
       "\\item[247] 20.0163877523773\n",
       "\\item[248] 19.8618442782307\n",
       "\\item[249] 21.2883130984709\n",
       "\\item[250] 24.0798914957638\n",
       "\\item[251] 24.2063354700241\n",
       "\\item[252] 25.0421582129268\n",
       "\\item[253] 24.9196400703322\n",
       "\\item[254] 29.9456337448009\n",
       "\\item[255] 23.9722831649775\n",
       "\\item[256] 21.6958088655397\n",
       "\\item[257] 37.5110923902323\n",
       "\\item[258] 43.3023904269433\n",
       "\\item[259] 36.4836142055223\n",
       "\\item[260] 34.9898859358965\n",
       "\\item[261] 34.8121150806157\n",
       "\\item[262] 37.1663133083515\n",
       "\\item[263] 40.989285008878\n",
       "\\item[264] 34.4463408928534\n",
       "\\item[265] 35.833975468617\n",
       "\\item[266] 28.2457429993294\n",
       "\\item[267] 31.2267359259333\n",
       "\\item[268] 40.8395574962499\n",
       "\\item[269] 39.3179239307984\n",
       "\\item[270] 25.7081790522638\n",
       "\\item[271] 22.3029553295129\n",
       "\\item[272] 27.2034097188608\n",
       "\\item[273] 28.5116947200694\n",
       "\\item[274] 35.4767659823203\n",
       "\\item[275] 36.1063916433224\n",
       "\\item[276] 33.7966827352104\n",
       "\\item[277] 35.6108585794039\n",
       "\\item[278] 34.8399338194243\n",
       "\\item[279] 30.3519265579237\n",
       "\\item[280] 35.3098070094815\n",
       "\\item[281] 38.7975696598869\n",
       "\\item[282] 34.3312318595864\n",
       "\\item[283] 40.3396307477481\n",
       "\\item[284] 44.6730833873132\n",
       "\\item[285] 31.5968908550318\n",
       "\\item[286] 27.3565922979884\n",
       "\\item[287] 20.1017415410065\n",
       "\\item[288] 27.04206673569\n",
       "\\item[289] 27.2136458044923\n",
       "\\item[290] 26.9139583935991\n",
       "\\item[291] 33.4356331055855\n",
       "\\item[292] 34.4034963301773\n",
       "\\item[293] 31.8333982014346\n",
       "\\item[294] 25.8178323689479\n",
       "\\item[295] 24.4298234832478\n",
       "\\item[296] 28.4576433683768\n",
       "\\item[297] 27.3626699852368\n",
       "\\item[298] 19.5392875808004\n",
       "\\item[299] 29.1130984409263\n",
       "\\item[300] 31.9105461078277\n",
       "\\item[301] 30.7715944851236\n",
       "\\item[302] 28.9427587127511\n",
       "\\item[303] 28.8819102240024\n",
       "\\item[304] 32.7988723186604\n",
       "\\item[305] 33.2090545594115\n",
       "\\item[306] 30.7683179231372\n",
       "\\item[307] 35.5622685696683\n",
       "\\item[308] 32.7090512370312\n",
       "\\item[309] 28.6424423662638\n",
       "\\item[310] 23.5896582710816\n",
       "\\item[311] 18.5426689677346\n",
       "\\item[312] 26.8788984289197\n",
       "\\item[313] 23.2813397881904\n",
       "\\item[314] 25.5458024605245\n",
       "\\item[315] 25.4812005709242\n",
       "\\item[316] 20.5390990114911\n",
       "\\item[317] 17.6157257282673\n",
       "\\item[318] 18.3758168640772\n",
       "\\item[319] 24.290702769397\n",
       "\\item[320] 21.3252903916503\n",
       "\\item[321] 24.8868224406516\n",
       "\\item[322] 24.869372815388\n",
       "\\item[323] 22.8695244743481\n",
       "\\item[324] 19.4512379061529\n",
       "\\item[325] 25.1178340113773\n",
       "\\item[326] 24.6678691322565\n",
       "\\item[327] 23.6807617707272\n",
       "\\item[328] 19.3408961632679\n",
       "\\item[329] 21.1741810541345\n",
       "\\item[330] 24.2524907349313\n",
       "\\item[331] 21.5926089386147\n",
       "\\item[332] 19.98446605389\n",
       "\\item[333] 23.3388799978055\n",
       "\\item[334] 22.140606924958\n",
       "\\item[335] 21.555099290216\n",
       "\\item[336] 20.6187290679767\n",
       "\\item[337] 20.1609717632301\n",
       "\\item[338] 19.2849038729266\n",
       "\\item[339] 22.1667232042712\n",
       "\\item[340] 21.2496577442297\n",
       "\\item[341] 21.4293930542572\n",
       "\\item[342] 30.327887960245\n",
       "\\item[343] 22.0473497508577\n",
       "\\item[344] 27.7064791245651\n",
       "\\item[345] 28.5479411681596\n",
       "\\item[346] 16.5450112143803\n",
       "\\item[347] 14.7835964098647\n",
       "\\item[348] 25.2738008168571\n",
       "\\item[349] 27.542051173934\n",
       "\\item[350] 22.1483756244775\n",
       "\\item[351] 20.4594409474613\n",
       "\\item[352] 20.5460542274535\n",
       "\\item[353] 16.8806382656481\n",
       "\\item[354] 25.4025350580152\n",
       "\\item[355] 14.3248663190426\n",
       "\\item[356] 16.5948846173842\n",
       "\\item[357] 19.6370469133015\n",
       "\\item[358] 22.7180660747844\n",
       "\\item[359] 22.2021888675356\n",
       "\\item[360] 19.2054805739532\n",
       "\\item[361] 22.6661610513382\n",
       "\\item[362] 18.9319261816695\n",
       "\\item[363] 18.2284680430267\n",
       "\\item[364] 20.2315081138551\n",
       "\\item[365] 37.4944738996952\n",
       "\\item[366] 14.2819073412001\n",
       "\\item[367] 15.5428624827946\n",
       "\\item[368] 10.8316232410572\n",
       "\\item[369] 23.8007290220917\n",
       "\\item[370] 32.6440736012499\n",
       "\\item[371] 34.6068404244668\n",
       "\\item[372] 24.9433133251444\n",
       "\\item[373] 25.9998091007361\n",
       "\\item[374] 6.12632499643095\n",
       "\\item[375] 0.777798059570536\n",
       "\\item[376] 25.3071306417183\n",
       "\\item[377] 17.7406106453159\n",
       "\\item[378] 20.2327441401876\n",
       "\\item[379] 15.8333130075964\n",
       "\\item[380] 16.8351258716529\n",
       "\\item[381] 14.3699482548819\n",
       "\\item[382] 18.4768283307821\n",
       "\\item[383] 13.4276828046913\n",
       "\\item[384] 13.0617751178283\n",
       "\\item[385] 3.27918116096131\n",
       "\\item[386] 8.06022170824221\n",
       "\\item[387] 6.12842196430475\n",
       "\\item[388] 5.6186480998375\n",
       "\\item[389] 6.45198569812686\n",
       "\\item[390] 14.2076473519415\n",
       "\\item[391] 17.2122518314194\n",
       "\\item[392] 17.298872651552\n",
       "\\item[393] 9.89116643227555\n",
       "\\item[394] 20.2212419349943\n",
       "\\item[395] 17.9418117532261\n",
       "\\item[396] 20.3044578274683\n",
       "\\item[397] 19.2955907547463\n",
       "\\item[398] 16.3363277931955\n",
       "\\item[399] 6.55162319068339\n",
       "\\item[400] 10.8901677809486\n",
       "\\item[401] 11.881458707413\n",
       "\\item[402] 17.8117450679153\n",
       "\\item[403] 18.2612658716956\n",
       "\\item[404] 12.9794878073526\n",
       "\\item[405] 7.3781636071946\n",
       "\\item[406] 8.21115861388474\n",
       "\\item[407] 8.06626192987007\n",
       "\\item[408] 19.9829478598214\n",
       "\\item[409] 13.7075636921068\n",
       "\\item[410] 19.8526845447722\n",
       "\\item[411] 15.2230829768811\n",
       "\\item[412] 16.9607198099616\n",
       "\\item[413] 1.71851806781777\n",
       "\\item[414] 11.8057838715304\n",
       "\\item[415] -4.28131070918481\n",
       "\\item[416] 9.58376736863224\n",
       "\\item[417] 13.3666081096374\n",
       "\\item[418] 6.89562362585981\n",
       "\\item[419] 6.1477985203214\n",
       "\\item[420] 14.6066179423843\n",
       "\\item[421] 19.6000267019875\n",
       "\\item[422] 18.1242747588778\n",
       "\\item[423] 18.5217713215163\n",
       "\\item[424] 13.1752861045673\n",
       "\\item[425] 14.6261762418156\n",
       "\\item[426] 9.92374975966266\n",
       "\\item[427] 16.3459064665755\n",
       "\\item[428] 14.0751942552311\n",
       "\\item[429] 14.2575624260639\n",
       "\\item[430] 13.0423478746219\n",
       "\\item[431] 18.1595569337206\n",
       "\\item[432] 18.6955435411763\n",
       "\\item[433] 21.5272830021116\n",
       "\\item[434] 17.0314186086468\n",
       "\\item[435] 15.9609043533334\n",
       "\\item[436] 13.3614161071054\n",
       "\\item[437] 14.5207938390617\n",
       "\\item[438] 8.81976005426038\n",
       "\\item[439] 4.86751102276619\n",
       "\\item[440] 13.0659131295214\n",
       "\\item[441] 12.7060969931816\n",
       "\\item[442] 17.2955805920873\n",
       "\\item[443] 18.740485001511\n",
       "\\item[444] 18.0590102945304\n",
       "\\item[445] 11.5147468339056\n",
       "\\item[446] 11.974003586659\n",
       "\\item[447] 17.6834461847101\n",
       "\\item[448] 18.1269523902661\n",
       "\\item[449] 17.5183465039488\n",
       "\\item[450] 17.2274250730164\n",
       "\\item[451] 16.5227163133693\n",
       "\\item[452] 19.412910950902\n",
       "\\item[453] 18.5821523618059\n",
       "\\item[454] 22.4894479081572\n",
       "\\item[455] 15.2800013303762\n",
       "\\item[456] 15.8208933517198\n",
       "\\item[457] 12.6872558131638\n",
       "\\item[458] 12.8763379019582\n",
       "\\item[459] 17.1866853085876\n",
       "\\item[460] 18.5124760929217\n",
       "\\item[461] 19.0486053324833\n",
       "\\item[462] 20.1720892732935\n",
       "\\item[463] 19.7740731969026\n",
       "\\item[464] 22.4294076788369\n",
       "\\item[465] 20.3191185432229\n",
       "\\item[466] 17.8861625321675\n",
       "\\item[467] 14.374785228368\n",
       "\\item[468] 16.9477685071966\n",
       "\\item[469] 16.9840576215105\n",
       "\\item[470] 18.5883839676952\n",
       "\\item[471] 20.1671944106316\n",
       "\\item[472] 22.9771803171487\n",
       "\\item[473] 22.4558072635363\n",
       "\\item[474] 25.5782462654826\n",
       "\\item[475] 16.3914763163556\n",
       "\\item[476] 16.1114628043018\n",
       "\\item[477] 20.5348159958905\n",
       "\\item[478] 11.5427273815651\n",
       "\\item[479] 19.2049630449181\n",
       "\\item[480] 21.8627639063056\n",
       "\\item[481] 23.4687886630714\n",
       "\\item[482] 27.0988731548861\n",
       "\\item[483] 28.5699430166971\n",
       "\\item[484] 21.0839878308985\n",
       "\\item[485] 19.4551619556368\n",
       "\\item[486] 22.2222591407908\n",
       "\\item[487] 19.655919607581\n",
       "\\item[488] 21.3253610439986\n",
       "\\item[489] 11.8558371664913\n",
       "\\item[490] 8.22386686792797\n",
       "\\item[491] 3.66399672048603\n",
       "\\item[492] 13.7590853820706\n",
       "\\item[493] 15.9311854513375\n",
       "\\item[494] 20.6266205443521\n",
       "\\item[495] 20.612494139848\n",
       "\\item[496] 16.8854196394897\n",
       "\\item[497] 14.0132078709419\n",
       "\\item[498] 19.108541438765\n",
       "\\item[499] 21.2980517444353\n",
       "\\item[500] 18.454988408699\n",
       "\\item[501] 20.4687084696061\n",
       "\\item[502] 23.5333405466671\n",
       "\\item[503] 22.3757189202883\n",
       "\\item[504] 27.6274260950359\n",
       "\\item[505] 26.1279668065958\n",
       "\\item[506] 22.3442122929037\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "1\n",
       ":   30.00384337701692\n",
       ":   25.02556237905323\n",
       ":   30.56759671860174\n",
       ":   28.60703648872825\n",
       ":   27.94352423287316\n",
       ":   25.25628446154117\n",
       ":   23.00180826848558\n",
       ":   19.53598842875629\n",
       ":   11.523636853130610\n",
       ":   18.920262107076111\n",
       ":   18.999496511132612\n",
       ":   21.586795681399413\n",
       ":   20.906521527835314\n",
       ":   19.55290281058115\n",
       ":   19.283482050090316\n",
       ":   19.297483208227917\n",
       ":   20.527509791162318\n",
       ":   16.911401346797919\n",
       ":   16.178011056571920\n",
       ":   18.406136033332521\n",
       ":   12.523857527095322\n",
       ":   17.671036694915123\n",
       ":   15.832881291786724\n",
       ":   13.806285346354825\n",
       ":   15.678338315463126\n",
       ":   13.386685608607427\n",
       ":   15.463976546439928\n",
       ":   14.708474280635929\n",
       ":   19.547372850961730\n",
       ":   20.876428202261431\n",
       ":   11.455117589956132\n",
       ":   18.059232945802833\n",
       ":   8.8110573622147134\n",
       ":   14.282758141229535\n",
       ":   13.70675891285136\n",
       ":   23.814635264832937\n",
       ":   22.341937075689138\n",
       ":   23.108911424544439\n",
       ":   22.915026116165240\n",
       ":   31.357625688164941\n",
       ":   34.215102253700442\n",
       ":   28.020564138691743\n",
       ":   25.203866280479144\n",
       ":   24.60979272545245\n",
       ":   22.941491756555346\n",
       ":   22.096698174979147\n",
       ":   20.423200323014748\n",
       ":   18.036550883547549\n",
       ":   9.1065537721826250\n",
       ":   17.206077514125251\n",
       ":   21.281525353211452\n",
       ":   23.972222848689653\n",
       ":   27.655850802057454\n",
       ":   24.049018091157855\n",
       ":   15.361847701003456\n",
       ":   31.152649466035157\n",
       ":   24.856869781555658\n",
       ":   33.109198061646859\n",
       ":   21.775379871488860\n",
       ":   21.084935550631161\n",
       ":   17.87258039688562\n",
       ":   18.51110208022163\n",
       ":   23.987428564909464\n",
       ":   22.554088688883465\n",
       ":   23.373086442266366\n",
       ":   30.361483581462667\n",
       ":   25.530565115204368\n",
       ":   21.113385641796869\n",
       ":   17.421537856548370\n",
       ":   20.784836326695971\n",
       ":   25.201488594435772\n",
       ":   21.742657704530673\n",
       ":   24.557449572150274\n",
       ":   24.042957120125875\n",
       ":   25.504997163713776\n",
       ":   23.966930200264377\n",
       ":   22.945454031329178\n",
       ":   23.356998184348479\n",
       ":   21.261982662306380\n",
       ":   22.428173731801681\n",
       ":   28.405769682084182\n",
       ":   26.994860862319883\n",
       ":   26.03576296838984\n",
       ":   25.058734819221485\n",
       ":   24.784566737932986\n",
       ":   27.790491951237487\n",
       ":   22.168534231467188\n",
       ":   25.892764153167889\n",
       ":   30.67461827176290\n",
       ":   30.831106227424591\n",
       ":   27.119019396048892\n",
       ":   27.412667341118893\n",
       ":   28.941227624397294\n",
       ":   29.081055462579495\n",
       ":   27.039773648763296\n",
       ":   28.624599485046297\n",
       ":   24.727449776436998\n",
       ":   35.781595180859399\n",
       ":   35.1145458726225100\n",
       ":   32.2510280132342101\n",
       ":   24.5802201889238102\n",
       ":   25.5941347457338103\n",
       ":   19.7901368354644104\n",
       ":   20.3116712875621105\n",
       ":   21.4348259114541106\n",
       ":   18.5399400816041107\n",
       ":   17.187559917816108\n",
       ":   20.7504902593562109\n",
       ":   22.6482911459716110\n",
       ":   19.7720366953188111\n",
       ":   20.6496586426969112\n",
       ":   26.5258674379307113\n",
       ":   20.7732363768603114\n",
       ":   20.7154831497023115\n",
       ":   25.1720888126412116\n",
       ":   20.4302559094607117\n",
       ":   23.3772462569496118\n",
       ":   23.6904326062206119\n",
       ":   20.3357836425832120\n",
       ":   20.7918087320158121\n",
       ":   21.9163207105348122\n",
       ":   22.4710777674484123\n",
       ":   20.5573855584772124\n",
       ":   16.3666197742302125\n",
       ":   20.5609981919357126\n",
       ":   22.4817844617125127\n",
       ":   14.617066327799128\n",
       ":   15.178766839628129\n",
       ":   18.9386859238066130\n",
       ":   14.0557328510346131\n",
       ":   20.0352739892632132\n",
       ":   19.4101340209302133\n",
       ":   20.0619156618911134\n",
       ":   15.7580767312742135\n",
       ":   13.2564523816054136\n",
       ":   17.2627773498836137\n",
       ":   15.8784188341927138\n",
       ":   19.3616395402927139\n",
       ":   13.8148389669953140\n",
       ":   16.4488147477162141\n",
       ":   13.5714193186728142\n",
       ":   3.98885508299596143\n",
       ":   14.5949547799219144\n",
       ":   12.1488148030321145\n",
       ":   8.7282236234232146\n",
       ":   12.0358534274744147\n",
       ":   15.8208205796314148\n",
       ":   8.51499020343318149\n",
       ":   9.71844139274403150\n",
       ":   14.8045137374037151\n",
       ":   20.8385815286829152\n",
       ":   18.3010116935799153\n",
       ":   20.1228255785022154\n",
       ":   17.2860189361229155\n",
       ":   22.3660022807163156\n",
       ":   20.1037592273599157\n",
       ":   13.621258906724158\n",
       ":   33.2598269689017159\n",
       ":   29.0301726798453160\n",
       ":   25.5675276936532161\n",
       ":   32.708276658354162\n",
       ":   36.7746701457414163\n",
       ":   40.5576584445806164\n",
       ":   41.8472816770413165\n",
       ":   24.7886737909977166\n",
       ":   25.3788923776935167\n",
       ":   37.2034745459996168\n",
       ":   23.0874874733725169\n",
       ":   26.4027395527366170\n",
       ":   26.6538211372646171\n",
       ":   22.5551466048872172\n",
       ":   24.290828117749173\n",
       ":   22.9765721902133174\n",
       ":   29.0719430769009175\n",
       ":   26.5219433959125176\n",
       ":   30.7220905554837177\n",
       ":   25.6166930694015178\n",
       ":   29.1374097942842179\n",
       ":   31.4357196770866180\n",
       ":   32.9223156800249181\n",
       ":   34.7244046437735182\n",
       ":   27.7655211065964183\n",
       ":   33.8878732132414184\n",
       ":   30.9923803601486185\n",
       ":   22.7182000809268186\n",
       ":   24.7664780989573187\n",
       ":   35.8849722620772188\n",
       ":   33.4247672203157189\n",
       ":   32.4119914656022190\n",
       ":   34.5150994934214191\n",
       ":   30.7610948517462192\n",
       ":   30.2893414067509193\n",
       ":   32.9191871432063194\n",
       ":   32.1126077140193195\n",
       ":   31.5587100431723196\n",
       ":   40.8455572142263197\n",
       ":   36.1277007921085198\n",
       ":   32.6692080997737199\n",
       ":   34.7046911643692200\n",
       ":   30.0934516181175201\n",
       ":   30.6439390648543202\n",
       ":   29.2871950136557203\n",
       ":   37.0714839202035204\n",
       ":   42.031931238384205\n",
       ":   43.1894984369702206\n",
       ":   22.6903479637775207\n",
       ":   23.6828471154279208\n",
       ":   17.8544721365808209\n",
       ":   23.4942899201965210\n",
       ":   17.0058771788018211\n",
       ":   22.3925109635662212\n",
       ":   17.0604275380357213\n",
       ":   22.7389292148944214\n",
       ":   25.2194255392078215\n",
       ":   11.1191673715419216\n",
       ":   24.5104914792796217\n",
       ":   26.6033477476208218\n",
       ":   28.3551871270884219\n",
       ":   24.9152546360528220\n",
       ":   29.6865276781239221\n",
       ":   33.1841974563363222\n",
       ":   23.7745665596465223\n",
       ":   32.1405195785142224\n",
       ":   29.7458198957233225\n",
       ":   38.3710245269932226\n",
       ":   39.8146186702757227\n",
       ":   37.5860575496587228\n",
       ":   32.3995325026375229\n",
       ":   35.4566524198279230\n",
       ":   31.2341151169737231\n",
       ":   24.4844922690659232\n",
       ":   33.2883729160192233\n",
       ":   38.0481048035943234\n",
       ":   37.163286314685235\n",
       ":   31.713835225278236\n",
       ":   25.2670557133153237\n",
       ":   30.1001074490383238\n",
       ":   32.7198715580756239\n",
       ":   28.4271705659764240\n",
       ":   28.4294067805891241\n",
       ":   27.2937593840445242\n",
       ":   23.742624782955243\n",
       ":   24.1200789091646244\n",
       ":   27.4020841376108245\n",
       ":   16.3285756005362246\n",
       ":   13.398912612969247\n",
       ":   20.0163877523773248\n",
       ":   19.8618442782307249\n",
       ":   21.2883130984709250\n",
       ":   24.0798914957638251\n",
       ":   24.2063354700241252\n",
       ":   25.0421582129268253\n",
       ":   24.9196400703322254\n",
       ":   29.9456337448009255\n",
       ":   23.9722831649775256\n",
       ":   21.6958088655397257\n",
       ":   37.5110923902323258\n",
       ":   43.3023904269433259\n",
       ":   36.4836142055223260\n",
       ":   34.9898859358965261\n",
       ":   34.8121150806157262\n",
       ":   37.1663133083515263\n",
       ":   40.989285008878264\n",
       ":   34.4463408928534265\n",
       ":   35.833975468617266\n",
       ":   28.2457429993294267\n",
       ":   31.2267359259333268\n",
       ":   40.8395574962499269\n",
       ":   39.3179239307984270\n",
       ":   25.7081790522638271\n",
       ":   22.3029553295129272\n",
       ":   27.2034097188608273\n",
       ":   28.5116947200694274\n",
       ":   35.4767659823203275\n",
       ":   36.1063916433224276\n",
       ":   33.7966827352104277\n",
       ":   35.6108585794039278\n",
       ":   34.8399338194243279\n",
       ":   30.3519265579237280\n",
       ":   35.3098070094815281\n",
       ":   38.7975696598869282\n",
       ":   34.3312318595864283\n",
       ":   40.3396307477481284\n",
       ":   44.6730833873132285\n",
       ":   31.5968908550318286\n",
       ":   27.3565922979884287\n",
       ":   20.1017415410065288\n",
       ":   27.04206673569289\n",
       ":   27.2136458044923290\n",
       ":   26.9139583935991291\n",
       ":   33.4356331055855292\n",
       ":   34.4034963301773293\n",
       ":   31.8333982014346294\n",
       ":   25.8178323689479295\n",
       ":   24.4298234832478296\n",
       ":   28.4576433683768297\n",
       ":   27.3626699852368298\n",
       ":   19.5392875808004299\n",
       ":   29.1130984409263300\n",
       ":   31.9105461078277301\n",
       ":   30.7715944851236302\n",
       ":   28.9427587127511303\n",
       ":   28.8819102240024304\n",
       ":   32.7988723186604305\n",
       ":   33.2090545594115306\n",
       ":   30.7683179231372307\n",
       ":   35.5622685696683308\n",
       ":   32.7090512370312309\n",
       ":   28.6424423662638310\n",
       ":   23.5896582710816311\n",
       ":   18.5426689677346312\n",
       ":   26.8788984289197313\n",
       ":   23.2813397881904314\n",
       ":   25.5458024605245315\n",
       ":   25.4812005709242316\n",
       ":   20.5390990114911317\n",
       ":   17.6157257282673318\n",
       ":   18.3758168640772319\n",
       ":   24.290702769397320\n",
       ":   21.3252903916503321\n",
       ":   24.8868224406516322\n",
       ":   24.869372815388323\n",
       ":   22.8695244743481324\n",
       ":   19.4512379061529325\n",
       ":   25.1178340113773326\n",
       ":   24.6678691322565327\n",
       ":   23.6807617707272328\n",
       ":   19.3408961632679329\n",
       ":   21.1741810541345330\n",
       ":   24.2524907349313331\n",
       ":   21.5926089386147332\n",
       ":   19.98446605389333\n",
       ":   23.3388799978055334\n",
       ":   22.140606924958335\n",
       ":   21.555099290216336\n",
       ":   20.6187290679767337\n",
       ":   20.1609717632301338\n",
       ":   19.2849038729266339\n",
       ":   22.1667232042712340\n",
       ":   21.2496577442297341\n",
       ":   21.4293930542572342\n",
       ":   30.327887960245343\n",
       ":   22.0473497508577344\n",
       ":   27.7064791245651345\n",
       ":   28.5479411681596346\n",
       ":   16.5450112143803347\n",
       ":   14.7835964098647348\n",
       ":   25.2738008168571349\n",
       ":   27.542051173934350\n",
       ":   22.1483756244775351\n",
       ":   20.4594409474613352\n",
       ":   20.5460542274535353\n",
       ":   16.8806382656481354\n",
       ":   25.4025350580152355\n",
       ":   14.3248663190426356\n",
       ":   16.5948846173842357\n",
       ":   19.6370469133015358\n",
       ":   22.7180660747844359\n",
       ":   22.2021888675356360\n",
       ":   19.2054805739532361\n",
       ":   22.6661610513382362\n",
       ":   18.9319261816695363\n",
       ":   18.2284680430267364\n",
       ":   20.2315081138551365\n",
       ":   37.4944738996952366\n",
       ":   14.2819073412001367\n",
       ":   15.5428624827946368\n",
       ":   10.8316232410572369\n",
       ":   23.8007290220917370\n",
       ":   32.6440736012499371\n",
       ":   34.6068404244668372\n",
       ":   24.9433133251444373\n",
       ":   25.9998091007361374\n",
       ":   6.12632499643095375\n",
       ":   0.777798059570536376\n",
       ":   25.3071306417183377\n",
       ":   17.7406106453159378\n",
       ":   20.2327441401876379\n",
       ":   15.8333130075964380\n",
       ":   16.8351258716529381\n",
       ":   14.3699482548819382\n",
       ":   18.4768283307821383\n",
       ":   13.4276828046913384\n",
       ":   13.0617751178283385\n",
       ":   3.27918116096131386\n",
       ":   8.06022170824221387\n",
       ":   6.12842196430475388\n",
       ":   5.6186480998375389\n",
       ":   6.45198569812686390\n",
       ":   14.2076473519415391\n",
       ":   17.2122518314194392\n",
       ":   17.298872651552393\n",
       ":   9.89116643227555394\n",
       ":   20.2212419349943395\n",
       ":   17.9418117532261396\n",
       ":   20.3044578274683397\n",
       ":   19.2955907547463398\n",
       ":   16.3363277931955399\n",
       ":   6.55162319068339400\n",
       ":   10.8901677809486401\n",
       ":   11.881458707413402\n",
       ":   17.8117450679153403\n",
       ":   18.2612658716956404\n",
       ":   12.9794878073526405\n",
       ":   7.3781636071946406\n",
       ":   8.21115861388474407\n",
       ":   8.06626192987007408\n",
       ":   19.9829478598214409\n",
       ":   13.7075636921068410\n",
       ":   19.8526845447722411\n",
       ":   15.2230829768811412\n",
       ":   16.9607198099616413\n",
       ":   1.71851806781777414\n",
       ":   11.8057838715304415\n",
       ":   -4.28131070918481416\n",
       ":   9.58376736863224417\n",
       ":   13.3666081096374418\n",
       ":   6.89562362585981419\n",
       ":   6.1477985203214420\n",
       ":   14.6066179423843421\n",
       ":   19.6000267019875422\n",
       ":   18.1242747588778423\n",
       ":   18.5217713215163424\n",
       ":   13.1752861045673425\n",
       ":   14.6261762418156426\n",
       ":   9.92374975966266427\n",
       ":   16.3459064665755428\n",
       ":   14.0751942552311429\n",
       ":   14.2575624260639430\n",
       ":   13.0423478746219431\n",
       ":   18.1595569337206432\n",
       ":   18.6955435411763433\n",
       ":   21.5272830021116434\n",
       ":   17.0314186086468435\n",
       ":   15.9609043533334436\n",
       ":   13.3614161071054437\n",
       ":   14.5207938390617438\n",
       ":   8.81976005426038439\n",
       ":   4.86751102276619440\n",
       ":   13.0659131295214441\n",
       ":   12.7060969931816442\n",
       ":   17.2955805920873443\n",
       ":   18.740485001511444\n",
       ":   18.0590102945304445\n",
       ":   11.5147468339056446\n",
       ":   11.974003586659447\n",
       ":   17.6834461847101448\n",
       ":   18.1269523902661449\n",
       ":   17.5183465039488450\n",
       ":   17.2274250730164451\n",
       ":   16.5227163133693452\n",
       ":   19.412910950902453\n",
       ":   18.5821523618059454\n",
       ":   22.4894479081572455\n",
       ":   15.2800013303762456\n",
       ":   15.8208933517198457\n",
       ":   12.6872558131638458\n",
       ":   12.8763379019582459\n",
       ":   17.1866853085876460\n",
       ":   18.5124760929217461\n",
       ":   19.0486053324833462\n",
       ":   20.1720892732935463\n",
       ":   19.7740731969026464\n",
       ":   22.4294076788369465\n",
       ":   20.3191185432229466\n",
       ":   17.8861625321675467\n",
       ":   14.374785228368468\n",
       ":   16.9477685071966469\n",
       ":   16.9840576215105470\n",
       ":   18.5883839676952471\n",
       ":   20.1671944106316472\n",
       ":   22.9771803171487473\n",
       ":   22.4558072635363474\n",
       ":   25.5782462654826475\n",
       ":   16.3914763163556476\n",
       ":   16.1114628043018477\n",
       ":   20.5348159958905478\n",
       ":   11.5427273815651479\n",
       ":   19.2049630449181480\n",
       ":   21.8627639063056481\n",
       ":   23.4687886630714482\n",
       ":   27.0988731548861483\n",
       ":   28.5699430166971484\n",
       ":   21.0839878308985485\n",
       ":   19.4551619556368486\n",
       ":   22.2222591407908487\n",
       ":   19.655919607581488\n",
       ":   21.3253610439986489\n",
       ":   11.8558371664913490\n",
       ":   8.22386686792797491\n",
       ":   3.66399672048603492\n",
       ":   13.7590853820706493\n",
       ":   15.9311854513375494\n",
       ":   20.6266205443521495\n",
       ":   20.612494139848496\n",
       ":   16.8854196394897497\n",
       ":   14.0132078709419498\n",
       ":   19.108541438765499\n",
       ":   21.2980517444353500\n",
       ":   18.454988408699501\n",
       ":   20.4687084696061502\n",
       ":   23.5333405466671503\n",
       ":   22.3757189202883504\n",
       ":   27.6274260950359505\n",
       ":   26.1279668065958506\n",
       ":   22.3442122929037\n",
       "\n"
      ],
      "text/plain": [
       "         1          2          3          4          5          6          7 \n",
       "30.0038434 25.0255624 30.5675967 28.6070365 27.9435242 25.2562845 23.0018083 \n",
       "         8          9         10         11         12         13         14 \n",
       "19.5359884 11.5236369 18.9202621 18.9994965 21.5867957 20.9065215 19.5529028 \n",
       "        15         16         17         18         19         20         21 \n",
       "19.2834821 19.2974832 20.5275098 16.9114013 16.1780111 18.4061360 12.5238575 \n",
       "        22         23         24         25         26         27         28 \n",
       "17.6710367 15.8328813 13.8062853 15.6783383 13.3866856 15.4639765 14.7084743 \n",
       "        29         30         31         32         33         34         35 \n",
       "19.5473729 20.8764282 11.4551176 18.0592329  8.8110574 14.2827581 13.7067589 \n",
       "        36         37         38         39         40         41         42 \n",
       "23.8146353 22.3419371 23.1089114 22.9150261 31.3576257 34.2151023 28.0205641 \n",
       "        43         44         45         46         47         48         49 \n",
       "25.2038663 24.6097927 22.9414918 22.0966982 20.4232003 18.0365509  9.1065538 \n",
       "        50         51         52         53         54         55         56 \n",
       "17.2060775 21.2815254 23.9722228 27.6558508 24.0490181 15.3618477 31.1526495 \n",
       "        57         58         59         60         61         62         63 \n",
       "24.8568698 33.1091981 21.7753799 21.0849356 17.8725804 18.5111021 23.9874286 \n",
       "        64         65         66         67         68         69         70 \n",
       "22.5540887 23.3730864 30.3614836 25.5305651 21.1133856 17.4215379 20.7848363 \n",
       "        71         72         73         74         75         76         77 \n",
       "25.2014886 21.7426577 24.5574496 24.0429571 25.5049972 23.9669302 22.9454540 \n",
       "        78         79         80         81         82         83         84 \n",
       "23.3569982 21.2619827 22.4281737 28.4057697 26.9948609 26.0357630 25.0587348 \n",
       "        85         86         87         88         89         90         91 \n",
       "24.7845667 27.7904920 22.1685342 25.8927642 30.6746183 30.8311062 27.1190194 \n",
       "        92         93         94         95         96         97         98 \n",
       "27.4126673 28.9412276 29.0810555 27.0397736 28.6245995 24.7274498 35.7815952 \n",
       "        99        100        101        102        103        104        105 \n",
       "35.1145459 32.2510280 24.5802202 25.5941347 19.7901368 20.3116713 21.4348259 \n",
       "       106        107        108        109        110        111        112 \n",
       "18.5399401 17.1875599 20.7504903 22.6482911 19.7720367 20.6496586 26.5258674 \n",
       "       113        114        115        116        117        118        119 \n",
       "20.7732364 20.7154831 25.1720888 20.4302559 23.3772463 23.6904326 20.3357836 \n",
       "       120        121        122        123        124        125        126 \n",
       "20.7918087 21.9163207 22.4710778 20.5573856 16.3666198 20.5609982 22.4817845 \n",
       "       127        128        129        130        131        132        133 \n",
       "14.6170663 15.1787668 18.9386859 14.0557329 20.0352740 19.4101340 20.0619157 \n",
       "       134        135        136        137        138        139        140 \n",
       "15.7580767 13.2564524 17.2627773 15.8784188 19.3616395 13.8148390 16.4488147 \n",
       "       141        142        143        144        145        146        147 \n",
       "13.5714193  3.9888551 14.5949548 12.1488148  8.7282236 12.0358534 15.8208206 \n",
       "       148        149        150        151        152        153        154 \n",
       " 8.5149902  9.7184414 14.8045137 20.8385815 18.3010117 20.1228256 17.2860189 \n",
       "       155        156        157        158        159        160        161 \n",
       "22.3660023 20.1037592 13.6212589 33.2598270 29.0301727 25.5675277 32.7082767 \n",
       "       162        163        164        165        166        167        168 \n",
       "36.7746701 40.5576584 41.8472817 24.7886738 25.3788924 37.2034745 23.0874875 \n",
       "       169        170        171        172        173        174        175 \n",
       "26.4027396 26.6538211 22.5551466 24.2908281 22.9765722 29.0719431 26.5219434 \n",
       "       176        177        178        179        180        181        182 \n",
       "30.7220906 25.6166931 29.1374098 31.4357197 32.9223157 34.7244046 27.7655211 \n",
       "       183        184        185        186        187        188        189 \n",
       "33.8878732 30.9923804 22.7182001 24.7664781 35.8849723 33.4247672 32.4119915 \n",
       "       190        191        192        193        194        195        196 \n",
       "34.5150995 30.7610949 30.2893414 32.9191871 32.1126077 31.5587100 40.8455572 \n",
       "       197        198        199        200        201        202        203 \n",
       "36.1277008 32.6692081 34.7046912 30.0934516 30.6439391 29.2871950 37.0714839 \n",
       "       204        205        206        207        208        209        210 \n",
       "42.0319312 43.1894984 22.6903480 23.6828471 17.8544721 23.4942899 17.0058772 \n",
       "       211        212        213        214        215        216        217 \n",
       "22.3925110 17.0604275 22.7389292 25.2194255 11.1191674 24.5104915 26.6033477 \n",
       "       218        219        220        221        222        223        224 \n",
       "28.3551871 24.9152546 29.6865277 33.1841975 23.7745666 32.1405196 29.7458199 \n",
       "       225        226        227        228        229        230        231 \n",
       "38.3710245 39.8146187 37.5860575 32.3995325 35.4566524 31.2341151 24.4844923 \n",
       "       232        233        234        235        236        237        238 \n",
       "33.2883729 38.0481048 37.1632863 31.7138352 25.2670557 30.1001074 32.7198716 \n",
       "       239        240        241        242        243        244        245 \n",
       "28.4271706 28.4294068 27.2937594 23.7426248 24.1200789 27.4020841 16.3285756 \n",
       "       246        247        248        249        250        251        252 \n",
       "13.3989126 20.0163878 19.8618443 21.2883131 24.0798915 24.2063355 25.0421582 \n",
       "       253        254        255        256        257        258        259 \n",
       "24.9196401 29.9456337 23.9722832 21.6958089 37.5110924 43.3023904 36.4836142 \n",
       "       260        261        262        263        264        265        266 \n",
       "34.9898859 34.8121151 37.1663133 40.9892850 34.4463409 35.8339755 28.2457430 \n",
       "       267        268        269        270        271        272        273 \n",
       "31.2267359 40.8395575 39.3179239 25.7081791 22.3029553 27.2034097 28.5116947 \n",
       "       274        275        276        277        278        279        280 \n",
       "35.4767660 36.1063916 33.7966827 35.6108586 34.8399338 30.3519266 35.3098070 \n",
       "       281        282        283        284        285        286        287 \n",
       "38.7975697 34.3312319 40.3396307 44.6730834 31.5968909 27.3565923 20.1017415 \n",
       "       288        289        290        291        292        293        294 \n",
       "27.0420667 27.2136458 26.9139584 33.4356331 34.4034963 31.8333982 25.8178324 \n",
       "       295        296        297        298        299        300        301 \n",
       "24.4298235 28.4576434 27.3626700 19.5392876 29.1130984 31.9105461 30.7715945 \n",
       "       302        303        304        305        306        307        308 \n",
       "28.9427587 28.8819102 32.7988723 33.2090546 30.7683179 35.5622686 32.7090512 \n",
       "       309        310        311        312        313        314        315 \n",
       "28.6424424 23.5896583 18.5426690 26.8788984 23.2813398 25.5458025 25.4812006 \n",
       "       316        317        318        319        320        321        322 \n",
       "20.5390990 17.6157257 18.3758169 24.2907028 21.3252904 24.8868224 24.8693728 \n",
       "       323        324        325        326        327        328        329 \n",
       "22.8695245 19.4512379 25.1178340 24.6678691 23.6807618 19.3408962 21.1741811 \n",
       "       330        331        332        333        334        335        336 \n",
       "24.2524907 21.5926089 19.9844661 23.3388800 22.1406069 21.5550993 20.6187291 \n",
       "       337        338        339        340        341        342        343 \n",
       "20.1609718 19.2849039 22.1667232 21.2496577 21.4293931 30.3278880 22.0473498 \n",
       "       344        345        346        347        348        349        350 \n",
       "27.7064791 28.5479412 16.5450112 14.7835964 25.2738008 27.5420512 22.1483756 \n",
       "       351        352        353        354        355        356        357 \n",
       "20.4594409 20.5460542 16.8806383 25.4025351 14.3248663 16.5948846 19.6370469 \n",
       "       358        359        360        361        362        363        364 \n",
       "22.7180661 22.2021889 19.2054806 22.6661611 18.9319262 18.2284680 20.2315081 \n",
       "       365        366        367        368        369        370        371 \n",
       "37.4944739 14.2819073 15.5428625 10.8316232 23.8007290 32.6440736 34.6068404 \n",
       "       372        373        374        375        376        377        378 \n",
       "24.9433133 25.9998091  6.1263250  0.7777981 25.3071306 17.7406106 20.2327441 \n",
       "       379        380        381        382        383        384        385 \n",
       "15.8333130 16.8351259 14.3699483 18.4768283 13.4276828 13.0617751  3.2791812 \n",
       "       386        387        388        389        390        391        392 \n",
       " 8.0602217  6.1284220  5.6186481  6.4519857 14.2076474 17.2122518 17.2988727 \n",
       "       393        394        395        396        397        398        399 \n",
       " 9.8911664 20.2212419 17.9418118 20.3044578 19.2955908 16.3363278  6.5516232 \n",
       "       400        401        402        403        404        405        406 \n",
       "10.8901678 11.8814587 17.8117451 18.2612659 12.9794878  7.3781636  8.2111586 \n",
       "       407        408        409        410        411        412        413 \n",
       " 8.0662619 19.9829479 13.7075637 19.8526845 15.2230830 16.9607198  1.7185181 \n",
       "       414        415        416        417        418        419        420 \n",
       "11.8057839 -4.2813107  9.5837674 13.3666081  6.8956236  6.1477985 14.6066179 \n",
       "       421        422        423        424        425        426        427 \n",
       "19.6000267 18.1242748 18.5217713 13.1752861 14.6261762  9.9237498 16.3459065 \n",
       "       428        429        430        431        432        433        434 \n",
       "14.0751943 14.2575624 13.0423479 18.1595569 18.6955435 21.5272830 17.0314186 \n",
       "       435        436        437        438        439        440        441 \n",
       "15.9609044 13.3614161 14.5207938  8.8197601  4.8675110 13.0659131 12.7060970 \n",
       "       442        443        444        445        446        447        448 \n",
       "17.2955806 18.7404850 18.0590103 11.5147468 11.9740036 17.6834462 18.1269524 \n",
       "       449        450        451        452        453        454        455 \n",
       "17.5183465 17.2274251 16.5227163 19.4129110 18.5821524 22.4894479 15.2800013 \n",
       "       456        457        458        459        460        461        462 \n",
       "15.8208934 12.6872558 12.8763379 17.1866853 18.5124761 19.0486053 20.1720893 \n",
       "       463        464        465        466        467        468        469 \n",
       "19.7740732 22.4294077 20.3191185 17.8861625 14.3747852 16.9477685 16.9840576 \n",
       "       470        471        472        473        474        475        476 \n",
       "18.5883840 20.1671944 22.9771803 22.4558073 25.5782463 16.3914763 16.1114628 \n",
       "       477        478        479        480        481        482        483 \n",
       "20.5348160 11.5427274 19.2049630 21.8627639 23.4687887 27.0988732 28.5699430 \n",
       "       484        485        486        487        488        489        490 \n",
       "21.0839878 19.4551620 22.2222591 19.6559196 21.3253610 11.8558372  8.2238669 \n",
       "       491        492        493        494        495        496        497 \n",
       " 3.6639967 13.7590854 15.9311855 20.6266205 20.6124941 16.8854196 14.0132079 \n",
       "       498        499        500        501        502        503        504 \n",
       "19.1085414 21.2980517 18.4549884 20.4687085 23.5333405 22.3757189 27.6274261 \n",
       "       505        506 \n",
       "26.1279668 22.3442123 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on full Boston dataset\n",
    "predict(model, Boston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why a train/test split?\n",
    "\n",
    "What is the point of making a train/test split for binary classification problems?\n",
    "* To evaluate your models out-of-sample, on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a 60/40 split\n",
    "\n",
    "As you saw in the video, you'll be working with the Sonar dataset in this chapter, using a 60% training set and a 40% test set. We'll practice making a train/test split one more time, just to be sure you have the hang of it. Recall that you can use the `sample()` function to get a random permutation of the row indices in a dataset, to use when making train/test splits, e.g.:\n",
    "\n",
    "    rows <- sample(nrow(my_data))\n",
    "\n",
    "And then use those row indices to randomly reorder the dataset, e.g.:\n",
    "\n",
    "    my_data <- my_data[rows, ]\n",
    "\n",
    "Once your dataset is randomly ordered, you can split off the first 60% as a training set and the last 40% as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>V1</th><th scope=col>V2</th><th scope=col>V3</th><th scope=col>V4</th><th scope=col>V5</th><th scope=col>V6</th><th scope=col>V7</th><th scope=col>V8</th><th scope=col>V9</th><th scope=col>V10</th><th scope=col>...</th><th scope=col>V52</th><th scope=col>V53</th><th scope=col>V54</th><th scope=col>V55</th><th scope=col>V56</th><th scope=col>V57</th><th scope=col>V58</th><th scope=col>V59</th><th scope=col>V60</th><th scope=col>Class</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>191</th><td>0.0156</td><td>0.0210</td><td>0.0282</td><td>0.0596</td><td>0.0462</td><td>0.0779</td><td>0.1365</td><td>0.0780</td><td>0.1038</td><td>0.1567</td><td>...   </td><td>0.0150</td><td>0.0060</td><td>0.0082</td><td>0.0091</td><td>0.0038</td><td>0.0056</td><td>0.0056</td><td>0.0048</td><td>0.0024</td><td>M     </td></tr>\n",
       "\t<tr><th scope=row>194</th><td>0.0203</td><td>0.0121</td><td>0.0380</td><td>0.0128</td><td>0.0537</td><td>0.0874</td><td>0.1021</td><td>0.0852</td><td>0.1136</td><td>0.1747</td><td>...   </td><td>0.0134</td><td>0.0094</td><td>0.0047</td><td>0.0045</td><td>0.0042</td><td>0.0028</td><td>0.0036</td><td>0.0013</td><td>0.0016</td><td>M     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll}\n",
       "  & V1 & V2 & V3 & V4 & V5 & V6 & V7 & V8 & V9 & V10 & ... & V52 & V53 & V54 & V55 & V56 & V57 & V58 & V59 & V60 & Class\\\\\n",
       "\\hline\n",
       "\t191 & 0.0156 & 0.0210 & 0.0282 & 0.0596 & 0.0462 & 0.0779 & 0.1365 & 0.0780 & 0.1038 & 0.1567 & ...    & 0.0150 & 0.0060 & 0.0082 & 0.0091 & 0.0038 & 0.0056 & 0.0056 & 0.0048 & 0.0024 & M     \\\\\n",
       "\t194 & 0.0203 & 0.0121 & 0.0380 & 0.0128 & 0.0537 & 0.0874 & 0.1021 & 0.0852 & 0.1136 & 0.1747 & ...    & 0.0134 & 0.0094 & 0.0047 & 0.0045 & 0.0042 & 0.0028 & 0.0036 & 0.0013 & 0.0016 & M     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | V1 | V2 | V3 | V4 | V5 | V6 | V7 | V8 | V9 | V10 | ... | V52 | V53 | V54 | V55 | V56 | V57 | V58 | V59 | V60 | Class | \n",
       "|---|---|\n",
       "| 191 | 0.0156 | 0.0210 | 0.0282 | 0.0596 | 0.0462 | 0.0779 | 0.1365 | 0.0780 | 0.1038 | 0.1567 | ...    | 0.0150 | 0.0060 | 0.0082 | 0.0091 | 0.0038 | 0.0056 | 0.0056 | 0.0048 | 0.0024 | M      | \n",
       "| 194 | 0.0203 | 0.0121 | 0.0380 | 0.0128 | 0.0537 | 0.0874 | 0.1021 | 0.0852 | 0.1136 | 0.1747 | ...    | 0.0134 | 0.0094 | 0.0047 | 0.0045 | 0.0042 | 0.0028 | 0.0036 | 0.0013 | 0.0016 | M      | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "    V1     V2     V3     V4     V5     V6     V7     V8     V9     V10    ...\n",
       "191 0.0156 0.0210 0.0282 0.0596 0.0462 0.0779 0.1365 0.0780 0.1038 0.1567 ...\n",
       "194 0.0203 0.0121 0.0380 0.0128 0.0537 0.0874 0.1021 0.0852 0.1136 0.1747 ...\n",
       "    V52    V53    V54    V55    V56    V57    V58    V59    V60    Class\n",
       "191 0.0150 0.0060 0.0082 0.0091 0.0038 0.0056 0.0056 0.0048 0.0024 M    \n",
       "194 0.0134 0.0094 0.0047 0.0045 0.0042 0.0028 0.0036 0.0013 0.0016 M    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>M</dt>\n",
       "\t\t<dd>63</dd>\n",
       "\t<dt>R</dt>\n",
       "\t\t<dd>62</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[M] 63\n",
       "\\item[R] 62\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "M\n",
       ":   63R\n",
       ":   62\n",
       "\n"
      ],
      "text/plain": [
       " M  R \n",
       "63 62 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>V1</th><th scope=col>V2</th><th scope=col>V3</th><th scope=col>V4</th><th scope=col>V5</th><th scope=col>V6</th><th scope=col>V7</th><th scope=col>V8</th><th scope=col>V9</th><th scope=col>V10</th><th scope=col>...</th><th scope=col>V52</th><th scope=col>V53</th><th scope=col>V54</th><th scope=col>V55</th><th scope=col>V56</th><th scope=col>V57</th><th scope=col>V58</th><th scope=col>V59</th><th scope=col>V60</th><th scope=col>Class</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>190</th><td>0.0158</td><td>0.0239</td><td>0.0150</td><td>0.0494</td><td>0.0988</td><td>0.1425</td><td>0.1463</td><td>0.1219</td><td>0.1697</td><td>0.1923</td><td>...   </td><td>0.0121</td><td>0.0108</td><td>0.0057</td><td>0.0028</td><td>0.0079</td><td>0.0034</td><td>0.0046</td><td>0.0022</td><td>0.0021</td><td>M     </td></tr>\n",
       "\t<tr><th scope=row>102</th><td>0.0335</td><td>0.0134</td><td>0.0696</td><td>0.1180</td><td>0.0348</td><td>0.1180</td><td>0.1948</td><td>0.1607</td><td>0.3036</td><td>0.4372</td><td>...   </td><td>0.0244</td><td>0.0232</td><td>0.0093</td><td>0.0159</td><td>0.0193</td><td>0.0032</td><td>0.0377</td><td>0.0126</td><td>0.0156</td><td>M     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll}\n",
       "  & V1 & V2 & V3 & V4 & V5 & V6 & V7 & V8 & V9 & V10 & ... & V52 & V53 & V54 & V55 & V56 & V57 & V58 & V59 & V60 & Class\\\\\n",
       "\\hline\n",
       "\t190 & 0.0158 & 0.0239 & 0.0150 & 0.0494 & 0.0988 & 0.1425 & 0.1463 & 0.1219 & 0.1697 & 0.1923 & ...    & 0.0121 & 0.0108 & 0.0057 & 0.0028 & 0.0079 & 0.0034 & 0.0046 & 0.0022 & 0.0021 & M     \\\\\n",
       "\t102 & 0.0335 & 0.0134 & 0.0696 & 0.1180 & 0.0348 & 0.1180 & 0.1948 & 0.1607 & 0.3036 & 0.4372 & ...    & 0.0244 & 0.0232 & 0.0093 & 0.0159 & 0.0193 & 0.0032 & 0.0377 & 0.0126 & 0.0156 & M     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | V1 | V2 | V3 | V4 | V5 | V6 | V7 | V8 | V9 | V10 | ... | V52 | V53 | V54 | V55 | V56 | V57 | V58 | V59 | V60 | Class | \n",
       "|---|---|\n",
       "| 190 | 0.0158 | 0.0239 | 0.0150 | 0.0494 | 0.0988 | 0.1425 | 0.1463 | 0.1219 | 0.1697 | 0.1923 | ...    | 0.0121 | 0.0108 | 0.0057 | 0.0028 | 0.0079 | 0.0034 | 0.0046 | 0.0022 | 0.0021 | M      | \n",
       "| 102 | 0.0335 | 0.0134 | 0.0696 | 0.1180 | 0.0348 | 0.1180 | 0.1948 | 0.1607 | 0.3036 | 0.4372 | ...    | 0.0244 | 0.0232 | 0.0093 | 0.0159 | 0.0193 | 0.0032 | 0.0377 | 0.0126 | 0.0156 | M      | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "    V1     V2     V3     V4     V5     V6     V7     V8     V9     V10    ...\n",
       "190 0.0158 0.0239 0.0150 0.0494 0.0988 0.1425 0.1463 0.1219 0.1697 0.1923 ...\n",
       "102 0.0335 0.0134 0.0696 0.1180 0.0348 0.1180 0.1948 0.1607 0.3036 0.4372 ...\n",
       "    V52    V53    V54    V55    V56    V57    V58    V59    V60    Class\n",
       "190 0.0121 0.0108 0.0057 0.0028 0.0079 0.0034 0.0046 0.0022 0.0021 M    \n",
       "102 0.0244 0.0232 0.0093 0.0159 0.0193 0.0032 0.0377 0.0126 0.0156 M    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>M</dt>\n",
       "\t\t<dd>48</dd>\n",
       "\t<dt>R</dt>\n",
       "\t\t<dd>35</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[M] 48\n",
       "\\item[R] 35\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "M\n",
       ":   48R\n",
       ":   35\n",
       "\n"
      ],
      "text/plain": [
       " M  R \n",
       "48 35 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Sonar <- load(\"Sonar.rdata\")\n",
    "\n",
    "head(sonar_train, 2)\n",
    "summary(sonar_train$Class)\n",
    "head(sonar_test, 2)\n",
    "summary(sonar_test$Class)\n",
    "\n",
    "Sonar <- rbind(sonar_train, sonar_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a logistic regression model\n",
    "\n",
    "Once you have your random training and test sets you can fit a logistic regression model to your training set using the `glm()` function. `glm()` is a more advanced version of `lm()` that allows for more varied types of regression models, aside from plain vanilla ordinary least squares regression.\n",
    "\n",
    "Be sure to pass the argument family = \"binomial\" to `glm()` to specify that you want to do logistic (rather than linear) regression. For example:\n",
    "\n",
    "    glm(Target ~ ., family = \"binomial\", dataset)\n",
    "\n",
    "Don't worry about warnings like glm.fit: algorithm did not converge or glm.fit: fitted probabilities numerically 0 or 1 occurred. These are common on smaller datasets and usually don't cause any issues. They typically mean your dataset is perfectly separable, which can cause problems for the math behind the model, but R's glm() function is almost always robust enough to handle this case with no problems.\n",
    "\n",
    "Once you have a glm() model fit to your dataset, you can predict the outcome (e.g. rock or mine) on the test set using the predict() function with the argument type = \"response\":\n",
    "\n",
    "    predict(my_model, test, type = \"response\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t125 obs. of  61 variables:\n",
      " $ V1   : num  0.0156 0.0203 0.0225 0.0134 0.0443 0.0116 0.0712 0.0177 0.115 0.0412 ...\n",
      " $ V2   : num  0.021 0.0121 0.0019 0.0172 0.0446 ...\n",
      " $ V3   : num  0.0282 0.038 0.0075 0.0178 0.0235 ...\n",
      " $ V4   : num  0.0596 0.0128 0.0097 0.0363 0.1008 ...\n",
      " $ V5   : num  0.0462 0.0537 0.0445 0.0444 0.2252 ...\n",
      " $ V6   : num  0.0779 0.0874 0.0906 0.0744 0.2611 ...\n",
      " $ V7   : num  0.1365 0.1021 0.0889 0.08 0.2061 ...\n",
      " $ V8   : num  0.078 0.0852 0.0655 0.0456 0.1668 ...\n",
      " $ V9   : num  0.1038 0.1136 0.1624 0.0368 0.1801 ...\n",
      " $ V10  : num  0.157 0.175 0.145 0.125 0.308 ...\n",
      " $ V11  : num  0.248 0.22 0.144 0.24 0.379 ...\n",
      " $ V12  : num  0.2783 0.2721 0.0948 0.2325 0.5364 ...\n",
      " $ V13  : num  0.2896 0.2105 0.0618 0.2523 0.6173 ...\n",
      " $ V14  : num  0.296 0.173 0.164 0.147 0.784 ...\n",
      " $ V15  : num  0.3189 0.204 0.0708 0.0669 0.8392 ...\n",
      " $ V16  : num  0.1892 0.1786 0.0844 0.11 0.9016 ...\n",
      " $ V17  : num  0.173 0.132 0.259 0.235 1 ...\n",
      " $ V18  : num  0.223 0.226 0.268 0.328 0.891 ...\n",
      " $ V19  : num  0.243 0.236 0.309 0.442 0.875 ...\n",
      " $ V20  : num  0.315 0.311 0.468 0.517 0.789 ...\n",
      " $ V21  : num  0.41 0.391 0.596 0.651 0.716 ...\n",
      " $ V22  : num  0.381 0.363 0.725 0.779 0.758 ...\n",
      " $ V23  : num  0.49 0.481 0.877 0.798 0.637 ...\n",
      " $ V24  : num  0.629 0.653 0.921 0.779 0.321 ...\n",
      " $ V25  : num  0.752 0.781 0.928 0.859 0.208 ...\n",
      " $ V26  : num  0.798 0.84 0.994 0.932 0.228 ...\n",
      " $ V27  : num  0.883 0.918 1 0.945 0.331 ...\n",
      " $ V28  : num  0.992 0.977 0.907 0.865 0.285 ...\n",
      " $ V29  : num  0.922 0.894 0.855 0.722 0.195 ...\n",
      " $ V30  : num  0.698 0.702 0.729 0.485 0.167 ...\n",
      " $ V31  : num  0.617 0.65 0.65 0.136 0.102 ...\n",
      " $ V32  : num  0.507 0.507 0.607 0.295 0.136 ...\n",
      " $ V33  : num  0.392 0.39 0.559 0.471 0.221 ...\n",
      " $ V34  : num  0.352 0.301 0.597 0.604 0.112 ...\n",
      " $ V35  : num  0.218 0.156 0.627 0.808 0.168 ...\n",
      " $ V36  : num  0.1245 0.0985 0.5459 0.987 0.1039 ...\n",
      " $ V37  : num  0.159 0.22 0.479 0.88 0.256 ...\n",
      " $ V38  : num  0.163 0.224 0.397 0.641 0.262 ...\n",
      " $ V39  : num  0.236 0.274 0.209 0.428 0.224 ...\n",
      " $ V40  : num  0.248 0.215 0.165 0.27 0.118 ...\n",
      " $ V41  : num  0.244 0.244 0.184 0.264 0.11 ...\n",
      " $ V42  : num  0.2715 0.3154 0.0652 0.3342 0.2831 ...\n",
      " $ V43  : num  0.1184 0.2112 0.0758 0.4335 0.2385 ...\n",
      " $ V44  : num  0.1157 0.0991 0.0486 0.4542 0.0255 ...\n",
      " $ V45  : num  0.1449 0.0594 0.0353 0.396 0.1967 ...\n",
      " $ V46  : num  0.1883 0.194 0.0297 0.2525 0.1483 ...\n",
      " $ V47  : num  0.1954 0.1937 0.0241 0.1084 0.0434 ...\n",
      " $ V48  : num  0.1492 0.1082 0.0379 0.0372 0.0627 ...\n",
      " $ V49  : num  0.0511 0.0336 0.0119 0.0286 0.0513 ...\n",
      " $ V50  : num  0.0155 0.0177 0.0073 0.0099 0.0473 0.0057 0.0286 0.027 0.0507 0.0772 ...\n",
      " $ V51  : num  0.0189 0.0209 0.0051 0.0046 0.0248 0.0031 0.0154 0.0168 0.0228 0.0798 ...\n",
      " $ V52  : num  0.015 0.0134 0.0034 0.0094 0.0274 0.0163 0.0154 0.0102 0.0099 0.0376 ...\n",
      " $ V53  : num  0.006 0.0094 0.0129 0.0048 0.0205 0.0099 0.0156 0.0122 0.0065 0.0143 ...\n",
      " $ V54  : num  0.0082 0.0047 0.01 0.0047 0.0141 0.0084 0.0054 0.0044 0.0085 0.0272 ...\n",
      " $ V55  : num  0.0091 0.0045 0.0044 0.0016 0.0185 0.027 0.003 0.0075 0.0166 0.0127 ...\n",
      " $ V56  : num  0.0038 0.0042 0.0057 0.0008 0.0055 0.0277 0.0048 0.0124 0.011 0.0166 ...\n",
      " $ V57  : num  0.0056 0.0028 0.003 0.0042 0.0045 0.0097 0.0087 0.0099 0.019 0.0095 ...\n",
      " $ V58  : num  0.0056 0.0036 0.0035 0.0024 0.0115 0.0054 0.0101 0.0057 0.0141 0.0225 ...\n",
      " $ V59  : num  0.0048 0.0013 0.0021 0.0027 0.0152 0.0148 0.0095 0.0032 0.0068 0.0098 ...\n",
      " $ V60  : num  0.0024 0.0016 0.0027 0.0041 0.01 0.0092 0.0068 0.0019 0.0086 0.0085 ...\n",
      " $ Class: num  1 1 0 1 1 1 1 0 1 1 ...\n"
     ]
    }
   ],
   "source": [
    "sonar_train$Class <- ifelse(sonar_train$Class == \"M\", 1, 0)\n",
    "str(sonar_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    }
   ],
   "source": [
    "# Fit glm model: model\n",
    "model <- glm(Class ~ . ,family = \"binomial\", data = sonar_train)\n",
    "\n",
    "# Predict on test: p\n",
    "p <- predict(model, newdata = sonar_test, type = \"response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix takeaways\n",
    "\n",
    "What information does a confusion matrix provide?\n",
    "* True positive rates\n",
    "* True negative rates\n",
    "* False positive rates\n",
    "* False negative rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate a confusion matrix\n",
    "\n",
    "As you saw in the video, a confusion matrix is a very useful tool for calibrating the output of a model and examining all possible outcomes of your predictions (true positive, true negative, false positive, false negative).\n",
    "\n",
    "Before you make your confusion matrix, you need to \"cut\" your predicted probabilities at a given threshold to turn probabilities into a factor of class predictions. Combine `ifelse()` with `factor()` as follows:\n",
    "\n",
    "    pos_or_neg <- ifelse(probability_prediction > threshold, positive_class, negative_class)\n",
    "    p_class <- factor(pos_or_neg, levels = levels(test_values))\n",
    "\n",
    "`confusionMatrix()` in caret improves on `table()` from base R by adding lots of useful ancillary statistics in addition to the base rates in the table. You can calculate the confusion matrix (and the associated statistics) using the predicted outcomes as well as the actual outcomes, e.g.:\n",
    "\n",
    "    confusionMatrix(p_class, test_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction  M  R\n",
       "         M 40 17\n",
       "         R  8 18\n",
       "                                          \n",
       "               Accuracy : 0.6988          \n",
       "                 95% CI : (0.5882, 0.7947)\n",
       "    No Information Rate : 0.5783          \n",
       "    P-Value [Acc > NIR] : 0.01616         \n",
       "                                          \n",
       "                  Kappa : 0.3602          \n",
       " Mcnemar's Test P-Value : 0.10960         \n",
       "                                          \n",
       "            Sensitivity : 0.8333          \n",
       "            Specificity : 0.5143          \n",
       "         Pos Pred Value : 0.7018          \n",
       "         Neg Pred Value : 0.6923          \n",
       "             Prevalence : 0.5783          \n",
       "         Detection Rate : 0.4819          \n",
       "   Detection Prevalence : 0.6867          \n",
       "      Balanced Accuracy : 0.6738          \n",
       "                                          \n",
       "       'Positive' Class : M               \n",
       "                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If p exceeds threshold of 0.5, M else R: m_or_r\n",
    "m_or_r <- ifelse(p > 0.5, \"M\", \"R\")\n",
    "\n",
    "# Convert to factor: p_class\n",
    "p_class <- factor(m_or_r, levels = levels(sonar_test[[\"Class\"]]))\n",
    "\n",
    "# Create confusion matrix\n",
    "confusionMatrix(p_class, sonar_test$Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the test set true positive rate (or sensitivity) of this model (rounded to the nearest percent)?\n",
    "* 17%\n",
    "\n",
    "What is the test set true negative rate (or specificity) of this model (rounded to the nearest percent)?\n",
    "* 49%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilities and classes\n",
    "\n",
    "What's the relationship between the predicted probabilities and the predicted classes?\n",
    "* Predicted classes are based off of predicted probabilities plus a classification threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try another threshold\n",
    "\n",
    "In the previous exercises, you used a threshold of 0.50 to cut your predicted probabilities to make class predictions (rock vs mine). However, this classification threshold does not always align with the goals for a given modeling problem.\n",
    "\n",
    "For example, pretend you want to identify the objects you are really certain are mines. In this case, you might want to use a probability threshold of 0.90 to get fewer predicted mines, but with greater confidence in each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction  M  R\n",
       "         M 40 15\n",
       "         R  8 20\n",
       "                                          \n",
       "               Accuracy : 0.7229          \n",
       "                 95% CI : (0.6138, 0.8155)\n",
       "    No Information Rate : 0.5783          \n",
       "    P-Value [Acc > NIR] : 0.004583        \n",
       "                                          \n",
       "                  Kappa : 0.416           \n",
       " Mcnemar's Test P-Value : 0.210903        \n",
       "                                          \n",
       "            Sensitivity : 0.8333          \n",
       "            Specificity : 0.5714          \n",
       "         Pos Pred Value : 0.7273          \n",
       "         Neg Pred Value : 0.7143          \n",
       "             Prevalence : 0.5783          \n",
       "         Detection Rate : 0.4819          \n",
       "   Detection Prevalence : 0.6627          \n",
       "      Balanced Accuracy : 0.7024          \n",
       "                                          \n",
       "       'Positive' Class : M               \n",
       "                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If p exceeds threshold of 0.9, M else R: m_or_r\n",
    "m_or_r <- ifelse(p > 0.9, \"M\", \"R\")\n",
    "\n",
    "# Convert to factor: p_class\n",
    "p_class <- factor(m_or_r, levels = levels(sonar_test[[\"Class\"]]))\n",
    "\n",
    "# Create confusion matrix\n",
    "confusionMatrix(p_class, sonar_test$Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From probabilites to confusion matrix\n",
    "\n",
    "Conversely, say you want to be really certain that your model correctly identifies all the mines as mines. In this case, you might use a prediction threshold of 0.10, instead of 0.90."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction  M  R\n",
       "         M 40 18\n",
       "         R  8 17\n",
       "                                          \n",
       "               Accuracy : 0.6867          \n",
       "                 95% CI : (0.5756, 0.7841)\n",
       "    No Information Rate : 0.5783          \n",
       "    P-Value [Acc > NIR] : 0.02806         \n",
       "                                          \n",
       "                  Kappa : 0.3319          \n",
       " Mcnemar's Test P-Value : 0.07756         \n",
       "                                          \n",
       "            Sensitivity : 0.8333          \n",
       "            Specificity : 0.4857          \n",
       "         Pos Pred Value : 0.6897          \n",
       "         Neg Pred Value : 0.6800          \n",
       "             Prevalence : 0.5783          \n",
       "         Detection Rate : 0.4819          \n",
       "   Detection Prevalence : 0.6988          \n",
       "      Balanced Accuracy : 0.6595          \n",
       "                                          \n",
       "       'Positive' Class : M               \n",
       "                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If p exceeds threshold of 0.1, M else R: m_or_r\n",
    "m_or_r <- ifelse(p > 0.1, \"M\", \"R\")\n",
    "\n",
    "# Convert to factor: p_class\n",
    "p_class <- factor(m_or_r, levels = levels(sonar_test$Class))\n",
    "\n",
    "# Create confusion matrix\n",
    "confusionMatrix(p_class, sonar_test$Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are (slightly) more predicted mines with this lower threshold: 58 (40 + 18) as compared to 47 for the 0.50 threshold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the value of a ROC curve?\n",
    "\n",
    "What is the primary value of an ROC curve?\n",
    "* It evaluates all possible thresholds for splitting predicted probabilities into predicted classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot an ROC curve\n",
    "\n",
    "As you saw in the video, an ROC curve is a really useful shortcut for summarizing the performance of a classifier over all possible thresholds. This saves you a lot of tedious work computing class predictions for many different thresholds and examining the confusion matrix for each.\n",
    "\n",
    "My favorite package for computing ROC curves is caTools, which contains a function called `colAUC()* . This function is very user-friendly and can actually calculate ROC curves for multiple predictors at once. In this case, you only need to calculate the ROC curve for one predictor, e.g.:\n",
    "\n",
    "    colAUC(predicted_probabilities, actual, plotROC = TRUE)\n",
    "\n",
    "The function will return a score called AUC (more on that later) and the `plotROC = TRUE` argument will return the plot of the ROC curve for visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>M vs. R</th><td>0.7452381</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       "\tM vs. R & 0.7452381\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| M vs. R | 0.7452381 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "        [,1]     \n",
       "M vs. R 0.7452381"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAAHgCAMAAAC7G6qeAAAAM1BMVEUAAABNTU1oaGh8fHyM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\nCUBDUek/Rnwn+odxm+IAAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title \"ROC Curves\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(caTools)\n",
    "# Predict on test: p\n",
    "p <- predict(model, sonar_test, type = \"response\")\n",
    "\n",
    "# Make ROC curve\n",
    "colAUC(p, sonar_test$Class, plotROC = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model, ROC, and AUC\n",
    "\n",
    "What is the AUC of a perfect model?\n",
    "* 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing trainControl\n",
    "\n",
    "As you saw in the video, area under the ROC curve is a very useful, single-number summary of a model's ability to discriminate the positive from the negative class (e.g. mines from rocks). An AUC of 0.5 is no better than random guessing, an AUC of 1.0 is a perfectly predictive model, and an AUC of 0.0 is perfectly anti-predictive (which rarely happens).\n",
    "\n",
    "This is often a much more useful metric than simply ranking models by their accuracy at a set threshold, as different models might require different calibration steps (looking at a confusion matrix at each step) to find the optimal classification threshold for that model.\n",
    "\n",
    "You can use the `trainControl()` function in caret to use AUC (instead of acccuracy), to tune the parameters of your models. The `twoClassSummary()` convenience function allows you to do this easily.\n",
    "\n",
    "When using `twoClassSummary()`, be sure to always include the argument `classProbs = TRUE` or your model will throw an error! (You cannot calculate AUC with just class predictions. You need to have class probabilities as well.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainControl object: myControl\n",
    "myControl <- trainControl(\n",
    "  method = \"cv\",\n",
    "  number = 10,\n",
    "  summaryFunction = twoClassSummary,\n",
    "  classProbs = TRUE, # IMPORTANT!\n",
    "  verboseIter = TRUE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using custom trainControl\n",
    "\n",
    "Now that you have a custom trainControl object, it's easy to fit caret models that use AUC rather than accuracy to tune and evaluate the model. You can just pass your custom trainControl object to the `train()` function via the trControl argument, e.g.:\n",
    "\n",
    "    train(<standard arguments here>, trControl = myControl)\n",
    "    \n",
    "This syntax gives you a convenient way to store a lot of custom modeling parameters and then use them across multiple different calls to `train()`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in train.default(x, y, weights = w, ...):\n",
      "\"The metric \"Accuracy\" was not in the result set. ROC will be used instead.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01: parameter=none \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold01: parameter=none \n",
      "+ Fold02: parameter=none \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold02: parameter=none \n",
      "+ Fold03: parameter=none \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold03: parameter=none \n",
      "+ Fold04: parameter=none \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold04: parameter=none \n",
      "+ Fold05: parameter=none \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold05: parameter=none \n",
      "+ Fold06: parameter=none \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold06: parameter=none \n",
      "+ Fold07: parameter=none \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold07: parameter=none \n",
      "+ Fold08: parameter=none \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold08: parameter=none \n",
      "+ Fold09: parameter=none \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold09: parameter=none \n",
      "+ Fold10: parameter=none \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold10: parameter=none \n",
      "Aggregating results\n",
      "Fitting final model on full training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generalized Linear Model \n",
       "\n",
       "208 samples\n",
       " 60 predictor\n",
       "  2 classes: 'M', 'R' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 188, 187, 187, 186, 187, 188, ... \n",
       "Resampling results:\n",
       "\n",
       "  ROC        Sens       Spec     \n",
       "  0.7130934  0.6924242  0.6877778\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train glm with custom trainControl: model\n",
    "model <- train(Class ~ . , Sonar, method = \"glm\", trControl = myControl)\n",
    "\n",
    "\n",
    "# Print model to console\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forests vs. linear models\n",
    "\n",
    "What's the primary advantage of random forests over linear models?\n",
    "* A random forest is a more flexible model than a linear model, but just as easy to fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a random forest\n",
    "\n",
    "As you saw in the video, random forest models are much more flexible than linear models, and can model complicated nonlinear effects as well as automatically capture interactions between variables. They tend to give very good results on real world data, so let's try one out on the wine quality dataset, where the goal is to predict the human-evaluated quality of a batch of wine, given some of the machine-measured chemical and physical properties of that batch.\n",
    "\n",
    "Fitting a random forest model is exactly the same as fitting a generalized linear regression model, as you did in the previous chapter. You simply change the method argument in the train function to be \"ranger\". The ranger package is a rewrite of R's classic randomForest package and fits models much faster, but gives almost exactly the same results. We suggest that all beginners use the ranger package for random forest modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine <- read_rds(\"wine_100.rds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t100 obs. of  13 variables:\n",
      " $ fixed.acidity       : num  6.7 6.7 5.8 6.3 6.6 7.8 5.5 9.1 6.2 8 ...\n",
      " $ volatile.acidity    : num  0.27 0.48 0.36 0.32 0.24 0.39 0.12 0.21 0.21 0.14 ...\n",
      " $ citric.acid         : num  0.69 0.49 0.38 0.26 0.28 0.26 0.33 0.37 0.34 0.33 ...\n",
      " $ residual.sugar      : num  1.2 2.9 0.9 12 1.8 9.9 1 1.6 6.6 1.2 ...\n",
      " $ chlorides           : num  0.176 0.03 0.037 0.049 0.028 0.059 0.038 0.067 0.03 0.045 ...\n",
      " $ free.sulfur.dioxide : num  36 28 3 63 39 33 23 6 36 71 ...\n",
      " $ total.sulfur.dioxide: num  106 122 75 170 132 181 131 10 91 162 ...\n",
      " $ density             : num  0.993 0.989 0.99 0.996 0.992 ...\n",
      " $ pH                  : num  2.96 3.13 3.28 3.14 3.34 3.04 3.25 3.23 3.32 3.07 ...\n",
      " $ sulphates           : num  0.43 0.4 0.34 0.55 0.46 0.42 0.45 0.58 0.45 0.47 ...\n",
      " $ alcohol             : num  9.2 13 11.4 9.9 11.4 10.9 9.8 11.1 12.5 11 ...\n",
      " $ quality             : int  6 6 4 6 5 6 5 7 7 6 ...\n",
      " $ color               : Factor w/ 2 levels \"red\",\"white\": 2 2 2 2 2 2 2 1 2 2 ...\n"
     ]
    }
   ],
   "source": [
    "str(wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold1: mtry=3, min.node.size=5, splitrule=variance \n",
      "- Fold1: mtry=3, min.node.size=5, splitrule=variance \n",
      "+ Fold1: mtry=3, min.node.size=5, splitrule=extratrees \n",
      "- Fold1: mtry=3, min.node.size=5, splitrule=extratrees \n",
      "+ Fold2: mtry=3, min.node.size=5, splitrule=variance \n",
      "- Fold2: mtry=3, min.node.size=5, splitrule=variance \n",
      "+ Fold2: mtry=3, min.node.size=5, splitrule=extratrees \n",
      "- Fold2: mtry=3, min.node.size=5, splitrule=extratrees \n",
      "+ Fold3: mtry=3, min.node.size=5, splitrule=variance \n",
      "- Fold3: mtry=3, min.node.size=5, splitrule=variance \n",
      "+ Fold3: mtry=3, min.node.size=5, splitrule=extratrees \n",
      "- Fold3: mtry=3, min.node.size=5, splitrule=extratrees \n",
      "+ Fold4: mtry=3, min.node.size=5, splitrule=variance \n",
      "- Fold4: mtry=3, min.node.size=5, splitrule=variance \n",
      "+ Fold4: mtry=3, min.node.size=5, splitrule=extratrees \n",
      "- Fold4: mtry=3, min.node.size=5, splitrule=extratrees \n",
      "+ Fold5: mtry=3, min.node.size=5, splitrule=variance \n",
      "- Fold5: mtry=3, min.node.size=5, splitrule=variance \n",
      "+ Fold5: mtry=3, min.node.size=5, splitrule=extratrees \n",
      "- Fold5: mtry=3, min.node.size=5, splitrule=extratrees \n",
      "Aggregating results\n",
      "Selecting tuning parameters\n",
      "Fitting mtry = 3, splitrule = variance, min.node.size = 5 on full training set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "100 samples\n",
       " 12 predictor\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (5 fold) \n",
       "Summary of sample sizes: 79, 81, 80, 80, 80 \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  splitrule   RMSE       Rsquared   MAE      \n",
       "  variance    0.6470478  0.3004669  0.5110721\n",
       "  extratrees  0.6837132  0.2311434  0.5229462\n",
       "\n",
       "Tuning parameter 'mtry' was held constant at a value of 3\n",
       "Tuning\n",
       " parameter 'min.node.size' was held constant at a value of 5\n",
       "RMSE was used to select the optimal model using the smallest value.\n",
       "The final values used for the model were mtry = 3, splitrule = variance\n",
       " and min.node.size = 5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit random forest: model\n",
    "model <- train(\n",
    "  quality ~ .,\n",
    "  tuneLength = 1,\n",
    "  data = wine,  method = \"ranger\",\n",
    "  trControl = trainControl(method = \"cv\", number = 5, verboseIter = TRUE)\n",
    ")\n",
    "\n",
    "# Print model to console\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage of a longer tune length\n",
    "\n",
    "What's the advantage of a longer tuneLength?\n",
    "* You explore more potential models and can potentially find a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a longer tune length\n",
    "\n",
    "Recall from the video that random forest models have a primary tuning parameter of mtry, which controls how many variables are exposed to the splitting search routine at each split. For example, suppose that a tree has a total of 10 splits and mtry = 2. This means that there are 10 samples of 2 predictors each time a split is evaluated.\n",
    "\n",
    "Use a larger tuning grid this time, but stick to the defaults provided by the `train()` function. Try a tuneLength of 3, rather than 1, to explore some more potential models, and plot the resulting model using the plot function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold1: mtry= 2, min.node.size=5, splitrule=variance \n",
      "- Fold1: mtry= 2, min.node.size=5, splitrule=variance \n",
      "+ Fold1: mtry= 7, min.node.size=5, splitrule=variance \n",
      "- Fold1: mtry= 7, min.node.size=5, splitrule=variance \n",
      "+ Fold1: mtry=12, min.node.size=5, splitrule=variance \n",
      "- Fold1: mtry=12, min.node.size=5, splitrule=variance \n",
      "+ Fold1: mtry= 2, min.node.size=5, splitrule=extratrees \n",
      "- Fold1: mtry= 2, min.node.size=5, splitrule=extratrees \n",
      "+ Fold1: mtry= 7, min.node.size=5, splitrule=extratrees \n",
      "- Fold1: mtry= 7, min.node.size=5, splitrule=extratrees \n",
      "+ Fold1: mtry=12, min.node.size=5, splitrule=extratrees \n",
      "- Fold1: mtry=12, min.node.size=5, splitrule=extratrees \n",
      "+ Fold2: mtry= 2, min.node.size=5, splitrule=variance \n",
      "- Fold2: mtry= 2, min.node.size=5, splitrule=variance \n",
      "+ Fold2: mtry= 7, min.node.size=5, splitrule=variance \n",
      "- Fold2: mtry= 7, min.node.size=5, splitrule=variance \n",
      "+ Fold2: mtry=12, min.node.size=5, splitrule=variance \n",
      "- Fold2: mtry=12, min.node.size=5, splitrule=variance \n",
      "+ Fold2: mtry= 2, min.node.size=5, splitrule=extratrees \n",
      "- Fold2: mtry= 2, min.node.size=5, splitrule=extratrees \n",
      "+ Fold2: mtry= 7, min.node.size=5, splitrule=extratrees \n",
      "- Fold2: mtry= 7, min.node.size=5, splitrule=extratrees \n",
      "+ Fold2: mtry=12, min.node.size=5, splitrule=extratrees \n",
      "- Fold2: mtry=12, min.node.size=5, splitrule=extratrees \n",
      "+ Fold3: mtry= 2, min.node.size=5, splitrule=variance \n",
      "- Fold3: mtry= 2, min.node.size=5, splitrule=variance \n",
      "+ Fold3: mtry= 7, min.node.size=5, splitrule=variance \n",
      "- Fold3: mtry= 7, min.node.size=5, splitrule=variance \n",
      "+ Fold3: mtry=12, min.node.size=5, splitrule=variance \n",
      "- Fold3: mtry=12, min.node.size=5, splitrule=variance \n",
      "+ Fold3: mtry= 2, min.node.size=5, splitrule=extratrees \n",
      "- Fold3: mtry= 2, min.node.size=5, splitrule=extratrees \n",
      "+ Fold3: mtry= 7, min.node.size=5, splitrule=extratrees \n",
      "- Fold3: mtry= 7, min.node.size=5, splitrule=extratrees \n",
      "+ Fold3: mtry=12, min.node.size=5, splitrule=extratrees \n",
      "- Fold3: mtry=12, min.node.size=5, splitrule=extratrees \n",
      "+ Fold4: mtry= 2, min.node.size=5, splitrule=variance \n",
      "- Fold4: mtry= 2, min.node.size=5, splitrule=variance \n",
      "+ Fold4: mtry= 7, min.node.size=5, splitrule=variance \n",
      "- Fold4: mtry= 7, min.node.size=5, splitrule=variance \n",
      "+ Fold4: mtry=12, min.node.size=5, splitrule=variance \n",
      "- Fold4: mtry=12, min.node.size=5, splitrule=variance \n",
      "+ Fold4: mtry= 2, min.node.size=5, splitrule=extratrees \n",
      "- Fold4: mtry= 2, min.node.size=5, splitrule=extratrees \n",
      "+ Fold4: mtry= 7, min.node.size=5, splitrule=extratrees \n",
      "- Fold4: mtry= 7, min.node.size=5, splitrule=extratrees \n",
      "+ Fold4: mtry=12, min.node.size=5, splitrule=extratrees \n",
      "- Fold4: mtry=12, min.node.size=5, splitrule=extratrees \n",
      "+ Fold5: mtry= 2, min.node.size=5, splitrule=variance \n",
      "- Fold5: mtry= 2, min.node.size=5, splitrule=variance \n",
      "+ Fold5: mtry= 7, min.node.size=5, splitrule=variance \n",
      "- Fold5: mtry= 7, min.node.size=5, splitrule=variance \n",
      "+ Fold5: mtry=12, min.node.size=5, splitrule=variance \n",
      "- Fold5: mtry=12, min.node.size=5, splitrule=variance \n",
      "+ Fold5: mtry= 2, min.node.size=5, splitrule=extratrees \n",
      "- Fold5: mtry= 2, min.node.size=5, splitrule=extratrees \n",
      "+ Fold5: mtry= 7, min.node.size=5, splitrule=extratrees \n",
      "- Fold5: mtry= 7, min.node.size=5, splitrule=extratrees \n",
      "+ Fold5: mtry=12, min.node.size=5, splitrule=extratrees \n",
      "- Fold5: mtry=12, min.node.size=5, splitrule=extratrees \n",
      "Aggregating results\n",
      "Selecting tuning parameters\n",
      "Fitting mtry = 2, splitrule = variance, min.node.size = 5 on full training set\n"
     ]
    }
   ],
   "source": [
    "# Fit random forest: model\n",
    "model <- train(\n",
    "  quality ~ . ,\n",
    "  tuneLength = 3,\n",
    "  data = wine, method = \"ranger\",\n",
    "  trControl = trainControl(method = \"cv\", number = 5, verboseIter = TRUE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "100 samples\n",
       " 12 predictor\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (5 fold) \n",
       "Summary of sample sizes: 81, 79, 80, 80, 80 \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  splitrule   RMSE       Rsquared   MAE      \n",
       "   2    variance    0.6690661  0.2599893  0.4968742\n",
       "   2    extratrees  0.6919334  0.2070825  0.5139082\n",
       "   7    variance    0.6736951  0.2347155  0.5044623\n",
       "   7    extratrees  0.6833274  0.2054887  0.5118286\n",
       "  12    variance    0.6770627  0.2304739  0.5096112\n",
       "  12    extratrees  0.6863720  0.1945324  0.5176281\n",
       "\n",
       "Tuning parameter 'min.node.size' was held constant at a value of 5\n",
       "RMSE was used to select the optimal model using the smallest value.\n",
       "The final values used for the model were mtry = 2, splitrule = variance\n",
       " and min.node.size = 5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAAHgCAMAAAC7G6qeAAAAOVBMVEUAAAAAgP9NTU1oaGh8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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print model to console\n",
    "model\n",
    "\n",
    "# Plot model\n",
    "plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of a custom tuning grid\n",
    "\n",
    "Why use a custom tuneGrid?\n",
    "* It gives you more fine-grained control over the tuning parameters that are explored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a random forest with custom tuning\n",
    "\n",
    "Now that you've explored the default tuning grids provided by the train() function, let's customize your models a bit more.\n",
    "\n",
    "You can provide any number of values for mtry, from 2 up to the number of columns in the dataset. In practice, there are diminishing returns for much larger values of mtry, so you will use a custom tuning grid that explores 2 simple models (mtry = 2 and mtry = 3) as well as one more complicated model (mtry = 7).\n",
    "Set the number of variables to possibly split at each node, .mtry, to a vector of 2, 3, and 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold1: mtry=2, splitrule=variance, min.node.size=5 \n",
      "- Fold1: mtry=2, splitrule=variance, min.node.size=5 \n",
      "+ Fold1: mtry=3, splitrule=variance, min.node.size=5 \n",
      "- Fold1: mtry=3, splitrule=variance, min.node.size=5 \n",
      "+ Fold1: mtry=7, splitrule=variance, min.node.size=5 \n",
      "- Fold1: mtry=7, splitrule=variance, min.node.size=5 \n",
      "+ Fold2: mtry=2, splitrule=variance, min.node.size=5 \n",
      "- Fold2: mtry=2, splitrule=variance, min.node.size=5 \n",
      "+ Fold2: mtry=3, splitrule=variance, min.node.size=5 \n",
      "- Fold2: mtry=3, splitrule=variance, min.node.size=5 \n",
      "+ Fold2: mtry=7, splitrule=variance, min.node.size=5 \n",
      "- Fold2: mtry=7, splitrule=variance, min.node.size=5 \n",
      "+ Fold3: mtry=2, splitrule=variance, min.node.size=5 \n",
      "- Fold3: mtry=2, splitrule=variance, min.node.size=5 \n",
      "+ Fold3: mtry=3, splitrule=variance, min.node.size=5 \n",
      "- Fold3: mtry=3, splitrule=variance, min.node.size=5 \n",
      "+ Fold3: mtry=7, splitrule=variance, min.node.size=5 \n",
      "- Fold3: mtry=7, splitrule=variance, min.node.size=5 \n",
      "+ Fold4: mtry=2, splitrule=variance, min.node.size=5 \n",
      "- Fold4: mtry=2, splitrule=variance, min.node.size=5 \n",
      "+ Fold4: mtry=3, splitrule=variance, min.node.size=5 \n",
      "- Fold4: mtry=3, splitrule=variance, min.node.size=5 \n",
      "+ Fold4: mtry=7, splitrule=variance, min.node.size=5 \n",
      "- Fold4: mtry=7, splitrule=variance, min.node.size=5 \n",
      "+ Fold5: mtry=2, splitrule=variance, min.node.size=5 \n",
      "- Fold5: mtry=2, splitrule=variance, min.node.size=5 \n",
      "+ Fold5: mtry=3, splitrule=variance, min.node.size=5 \n",
      "- Fold5: mtry=3, splitrule=variance, min.node.size=5 \n",
      "+ Fold5: mtry=7, splitrule=variance, min.node.size=5 \n",
      "- Fold5: mtry=7, splitrule=variance, min.node.size=5 \n",
      "Aggregating results\n",
      "Selecting tuning parameters\n",
      "Fitting mtry = 7, splitrule = variance, min.node.size = 5 on full training set\n"
     ]
    }
   ],
   "source": [
    "# Define the tuning grid: tuneGrid\n",
    "tuneGrid <- data.frame(\n",
    "  .mtry = c(2,3, 7),\n",
    "  .splitrule = \"variance\",\n",
    "  .min.node.size = 5\n",
    ")\n",
    "\n",
    "# Fit random forest: model\n",
    "model <- train(\n",
    "  quality ~ .,\n",
    "  tuneGrid = tuneGrid,\n",
    "  data = wine, \n",
    "  method = \"ranger\",\n",
    "  trControl = trainControl(method = \"cv\", number = 5, verboseIter = TRUE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "100 samples\n",
       " 12 predictor\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (5 fold) \n",
       "Summary of sample sizes: 80, 80, 80, 80, 80 \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  RMSE       Rsquared   MAE      \n",
       "  2     0.6395336  0.3693696  0.4763893\n",
       "  3     0.6329337  0.3735120  0.4716450\n",
       "  7     0.6238573  0.3785370  0.4698560\n",
       "\n",
       "Tuning parameter 'splitrule' was held constant at a value of variance\n",
       "\n",
       "Tuning parameter 'min.node.size' was held constant at a value of 5\n",
       "RMSE was used to select the optimal model using the smallest value.\n",
       "The final values used for the model were mtry = 7, splitrule = variance\n",
       " and min.node.size = 5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAAHgCAMAAAC7G6qeAAAANlBMVEUAAAAAgP9NTU1oaGh8\nfHyMjIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHm5ubp6enw8PD////lZQhBAAAACXBIWXMA\nABJ0AAASdAHeZh94AAAakElEQVR4nO2diZaiQAxFSxCXVkf5/58dFhVkUWpLJZX3zukZbbWu\nwTtMKBBMjSAZxaR+AwgSMhAaySoQGskqEBrJKhAaySoQGskqEBrJKhAaySoQGskqEBrJKhAa\nySoQGskqEBrJKhAaySoQGskqEBrJKhAaySoQGskqEBrJKhAaySoQGskqEBrJKhAaySoQGskq\nEBrJKhAaySoQGskqEBrJKhAaySoQGskqEBrJKhAaySoQGskqEBrJKhAaySoQGskqEBrJKhAa\nySoQGskqEBrJKhAaySoQGskqEBrJKhAaySoQGskqEBrJKhAaySqchCZ4LxTlooyUCAgtkYEy\nSMd0jdBFmICBMkjHdI3QRZiAgTJIx3SN0EWYgIEySMd0jdBFmICBMkjHdI3QRZiAgTJIx3SN\n0EWYgIEySMd0jdBFmICBMkjHdI3QRZiAgTJIx1wGIYhDrD2LIa8j6F/8d0GAQBkhERA6OQJl\nhERA6OQIlBESAaGTI1BGSASETo5AGSEREDo5AmWERMQX+mMmZXzHzB+2BPFYhBIYesqILrQZ\nv2Z8pxf542FbEI9FKIGhp4zYQpvxi8zHn2b6sDWIxyKUwNBTRgKhXzchNB1DTxnEQr87ZghN\nytBTBq3Qb4dNvSL0aKf8PwSxjvVRHUF66Mn2INbQGSCYlJFko3D4JYSmYegpI43Q7/8TIDQN\nQ08ZyabtAqyhdzu79+ISPSZIYDAQen3HyrSRtgU1Ov+Lr7QeEyQwOAj9nqkz4zvDSM67vndt\nfRCaDYJJGWIPTmpV/ldHN1qPCRIYuQu9g9B8EEzKECt013Kgh+aDYFKGYKGxUcgKwaQMuUJ3\nSkNoNggmZUgWGhuFnBBMypAudGyj9ZgggaFB6MhG6zFBAkOF0HGN1mOCBAaE9o4eEyQwdAgd\n1Wg9JkhgKBE6ptF6TJDA0CJ0RKP1mCCBoUboeEbrMUECA0J7R48JEhh6hI5mtB4TJDAUCR3L\naD0mSGBoEjqS0XpMkMBQJXQco/WYIIEBob2jxwQJDF1CRzFajwkSGMqEjmG0HhMkMLQJHcFo\nPSZIYKgTOrzRekyQwIDQ3tFjggSGPqGDG63HBAkMhUKHNlqPCRIYGoUObLQeEyQwVAod1mg9\nJkhgQGjv6DFBAkOn0EGN1mOCBIZSoUMarccECQytQgc0Wo8JEhhqhQ5ntB4TJDAgtHf0mCCB\noVfoYEbrMUECQ7HQoYzWY4IEhmahAxmtxwQJDBKhr8e9MWZ/vNqOZAua53t9QYzWY4IEBoHQ\nf+XrOsemvNiOZQNaCoRmhGBShp/Q973Zn2+P5tbjempu321H2wpazo/6QhitxwQJjNhCX8zx\nMbp7P5qAK2l/oUMYrccECYzYQlePyYOPg+1w20Ar+Vmfv9F6TJDAUD3L0cXbaD0mSGBAaAhN\nhmBSRuZCexutxwQJDBKhT++JO9uhLEHzbFmEnkbrMUECg0Lo03semqfQnkbrMUECg0Lowpxt\nh3ADLWTbIvQyWo8JEhgUQodeMa+CFgKhGSGYlOEtdGWmk9GBEkxoL6P1mCCBQSH0vdiHPixp\nGbSQrYvQw2g9Jkhg0LQczDcK27gbrccECQwI/Yqz0XpMkMDAjpVXIHQWDAj9jqvRekyQwKAR\n+q/9xkr1ZzuQPWgWm0XoaLQeEyQwSITePzvove1ItqB5rBahm9F6TJDAoBD6bIr2qP5L8D2G\noYV2M1qPCRIYFEKX5tb9fTOl7VB2oIVAaEYIJmWE2/XNetqui4vRekyQwKBdQxe2Q9mBFmK7\nCB2M1mOCBAZ66EnsjdZjggQGZjkmgdCyGUTz0JWEeeg+1kbrMUECA3sKZ7E1Wo8JEhgQeh5L\no/WYIIERW+h2pk7G0Xaj2BmtxwQJDAi9EAgtl4GWYylWRusxQQIDQi/Gxmg9JkhgkO76Lvjv\nKXzFwmg9JkhgUAp9l9JDt9lutB4TJDBiC30x43A/2m4UCC2TEX0NXY59Dnw2g5hCbzdajwkS\nGKQ9dOhEFXqz0XpMkMDALMd6NhqtxwQJDFKhr5XtUI6gIV6LcJvRekyQwCAR+ihsT+ErEFoe\ng0LoweewlymMLvQ2o/WYIIFBIXRh/uq9ud/3RtIsR5ctRusxQQKDapbj1Kydb6G/shJf6C1G\n6zFBAoNK6Ev7fcK1HvqjuX7fed1Y774JhN5gtB4TJDAohK6aluNuyvq6IrQZv+Z95+OG8zuD\n0IwQTMrwFvrSitx9UXbxKrJm/KL3ncHltEL/NlqPCRIYJNN2p/Y3B2OOX57+IfT4oS94EqF/\nGq3HBAkMBnsKJ0KP+uXu1qyDHh0c8o8kOxoMQhTrb1B5CT2slJ+8L20HzRr61zpaz6pNAiP+\ndwo/sv70aQ89GSnZLEcbCC2HwVvohRuW7yzMIvxqtB4TJDBIeuiqO7fdtVic5JAg9Fej9Zgg\ngUFzLMfr7KOL0xxfpu0+bji9s1CL8IvRekyQwCA9wN9mx8qwdbg+d0co9Bej9ZgggUFzcNKP\n80O/d3KP7yzccHhnEJoRgkkZAVqOoj3M7lKYk+1QdqCFhFuEq0brMUECg2Sj8HV+6MBfWKEV\netVoPSZIYNDsKezPDx348H5qodeM1mOCBAaDXd/uIRZ6xWg9JkhgQGiLQGj+jPh7Cmt5p9Nd\nzaLRekyQwIDQVlkyWo8JEhhoOeyyYLQeEyQwILRl5kbrMUECg8HRdu6B0LIQTMqA0B+ZGa3H\nBAkMtBzWmRqtxwQJDAhtn4nRekyQwCAVWtrZR9fyabQeEyQwSISWevbRtUBovgyaw0elnn10\nLR9G6zFBAoPmAH+xZx9dy9hoPSZIYFB9BUvq2UfXMjJajwkSGFRCfz37qGsSCj0yWo8JEhgU\nQv86+6hzILQsBJMyvIX+cfZR96QUejBajwkSGCTTdt/PPuqepEK/jdZjggRGbKGDf49wDbSS\nmItwFx/xCg8TJDCiH5xUHO+2r3cCrSTqItzFRzzDwwQJjNhCt5f63sdaTUNoWQgmZfj10Pdj\n0Th9vNkOYg1aTtxFuIuP6MPDBAkMio3Ca7M9aMrzw3Yca9BCIi/CXXxEFx4mSGAQHW33187a\nHUK3HumFbozebbzGvVd4mCCBQXb46OPUtNMrJ2t0DQehd/U/AqV5mCCBQXk89CWnPYXPtEJv\nvMi9T3iYIIGBNbRXGpV3/zZe5N4nPEyQwEAP7ZXW5GYlDaHZMEiO5ch3lqNV+R/BdiEPEyQw\nogt9beehi0znod8bhbGV5mGCBAb2FPrm6fIuqtM8TJDAiH8sxyl8q7EEWgnlIoyoNA8TJDBi\nCx34W4TroJXQLsJoSvMwQQKDbNou9Bz0Kugz1IswUufBwwQJDAjtnSkihtI8TJDAgNDemSPC\nK83DBAkMCO2dJUTozoOHCRIYENo7K4igSvMwQQIDQntnFRFQaR4mSGBQHm0XPMyFDth58DBB\nAoNE6HNZ1/fSlKEnpdkLXYdaTScvQwyD5OCktt1ov1mYz8kabRAhlGZQhhAGhdB781ffTFn/\nZXSyRiuEf+fBogwRDAqh2xX0rT1tUobfWNmK8FSaSxn8GVRCV+3JzhUL7ak0nzK4M2hajtul\n/faV2pbjGY/Og1MZvBlEG4XGnNoVdC6XpHBGuCrNrAzGDJppu6I78Wj5ZzuSLWgeHotwFDel\n2ZXBloEdK96xRbh0HgzLYMqA0N5xQFgrzbMMjgzsKfSOE8JyNc21DH4M7Cn0jivCRmnGZTBj\nYE+hd9wR25VmXQYrBvYUescHsbXzYF4GIwb2FHrHE7FJaf5lcGFgT6F3vBEblJZQBg8G9hR6\nJwDiZ+chowwODOwp9E4YxHelxZSRnIEdK94JhfimtKAyEjMgtHfCIdY7D1FlJGXQCN2d77wK\n3XFkJ3S9upqWVkY6BonQ3YXr2/Pq2o5kC5qHxyK0yaLS8spIxaAQ+myKdnrjUpiz7VB2oIXw\nWIR2Weg8JJaRhkEhdGn68/e3u7+DJlOh6/lqWmgZCRhUewo/bwRKvkJPlRZbBjmDdg2d3WXd\nYiLGnYfgMogZ6KG9ExHxVlp2GZQMzHJ4JyriqbT0MugYRPPQFeahXdN1HvLLoGJgT6F34iN2\nuyzK4PJpeAtdHW1HcAQthMci9E78a9VC6O0viHGu80XQQngswgCMuBf2rCG0xQtKE+nam5qE\nrmOvpiH05hc8qn2c628qEzqu0hB68wvMENuh7EAL4bEIwzHidR4QevMLIHRQRiSlIXS4F7hG\np9CRlIbQAV8wXnW/78xvOIB4LMLgjAidB4Te+IL7oTuC41GuHshhxq9535nfcHlnPBZhDEZo\npSH0thfcC1O1f1+MKe5fnm4mdyYuL74J1UKHVhpCb3tBaQ79LPR1v3J8/4LQ44cg9DojZOcB\noTe94NKeYeaZyiwenjQRetQxGwj9kxFMaQi96QWH0V7C+/Lxo59Cjxw2Zvbw8/ev/EP+/dvt\nUr8DYbGeRh4/zazeWRH6YwSsoTcxgnQe6cugQnitoQsvod9bhxD6R/yVZlEGCcKz5RhO0Hjp\n5zuWnw6hPRm+SjMpgwDhJfRtmKy7F5s2Cp9/zm84vTMei5CG4dd5sCkjOsJv2u5oilP7pe/b\nqVj7TuHijpVh6xA7VrYzPJTmVEZchOeewtN7k/Kw+gIzWg1j17cXw1lpXmXERPgey3E/dqdq\nPC3vJ/QJhF6KY+fBrYx4CBxtlxxhy3BRmmEZkRAQOjnCnmG/mmZZRhSEl9DV9OuEj9VO2j4Q\n+lssleZaRniE57Ecx7HS92PICwdB6O+xUppvGaERnoeP7s3+fGulflxPze2Qm4YQ+lcsOg/O\nZYRF+PbQf+V74q4Me103CL0hW5VmXkZAhP9G4bWbuNsfQ5/MAEJvyjal2ZcRDIFZjuQIX8aW\nzkNAGYEQEDo5IgDjp9IyygiBgNDJEUEYP5SWUoY/AkInRwRifO085JThi4DQyRHhGOtKiyrD\nCwGhkyNCMtaUFlaGBwJCJ0eEZSx3HuLKcEZA6OSI4IwFpSWW4YbwEnrDt77dA6HdM1NaZhku\niABCv76RAqEZMSadh9Qy7BEQOjkiFmOstOAyLBEQOjkiHmNQWnQZVggInRwRk/HqPISXYYGA\n0MkRkRmd0vLL2IqA0MkR0RmN0jmUsQ0BoZMjCBjRL+zZhsen4Sn0R2yH2g5aCY9FKIHxj+D6\nyzw+DQidHEFVRgaXX8aub+9kJHTszoPHpwGhkyNIy4ioNI9PA0InRxCXIfnyy9GFfhy7u9fS\nFKsXKnQNhI6FiNR58Pg0/IQuui3BS7dJuHJ+aOdA6IiIGErz+DS8hD6bfXvWpKK41Y/98hn8\n3QOhoyJEXn45ttB7057769pdrfAaehUNoSMjQncePD6NAHsKj+Y63AkXCB0fEVRpHp9GAKFL\n7Ppmz1hFyLr8cmyhy7bluPeXV3mYwnao7aCV8FiEEhhfEME6Dx6fhpfQx3aj8HmxwvP6ZYPc\nAqHJEGGUTl5GFy+hH8V7vu5szM12qO2glfBYhBIYvxBCLr8cf8fKwZhj99vn3wEDoUkR/p0H\nizJC7fo2VejTQ0NocoSn0jzKwLEcyRGMyuB++WUI7R1dQnt1HjzKgNDJEdzKcFWaRxleQhf4\nxooQhh2C7eWXYwtdQWghDFuES+fBowzPo+3K41/4y9bPQSvhsQglMBwQHC+/HFvo+6FtOopD\nFKkhdGqE5WqaRxm+G4W3c9d3RJAaQjNA2CjNo4wQsxztVZFbqW2HsgZNw2MRSmC4I1hdfplu\n2u5xxEYhZ4YPYmvnwaMMrKGTIwSUsUlpHmWgh06OEFEGj8sv08xyRJq6g9C8ED87Dx5leM9D\nXx62IziAVsJjEUpghEF8V5pHGdhTmBwhqYzEl1/GsRzegdCTrHcePMrA0XbJEeLKWFGaRxkQ\nOjlCYBmpLr8Mob0DoZez0HnwKCOc0LfKdihH0BAei1ACIwZiqjSPMvyEvu6N2XenL7hV2Chk\nzIiDIL/8cmyhr/3sxq2+V+HPYwChBSDGnQePMjzPPnrsTp/UniC6Cr2DBULLQLyV5lFGgJM1\nGlOYKvBpk2oILQdBd/llKqHL4GeZqSG0JETXefAoI4jQtmNYg1bCYxFKYMRH7HY8yoDQyRG5\nlEFw+WUI7R0IbcGIfknx+ELj4CQZDLIy4ioNob0DoW0ZMZXGsRzegdD2jHidB4T2DoR2YkRS\nGkJ7B0I7MqIoDaG9A6GdGRE6DwjtHQjtwwitNIT2DoT2Y4RVGkJ7B0L7MkJ2HhDaOxA6ACOY\n0hDaOxA6CCOQ0hDaO6lNkIP4wQjSeUBo76Q3QQriN8NfaQjtHRYmiEBsYfgqzULoj6OW3nde\nBzOtH9YEoWUhtjH8Og8OQpvxa953Pm44gzSZwB6xmeGhNAOhzfhF7zvDbyG0wjKcleYp9Pix\nL6NBaFkIK4Zj58FP6M+G+tlCfzx/yD8k5+x2UYa1/sKJl9DrDbUTSOGqjS/CnmG/mua3hh6P\nYCZPsgdpNYElwolhqTRroc30SfYgxSbwQzgyrJTmLLSZPckepNoEbghnhkXnwVjoxSbEFqTc\nBF4IH8ZWpRkI/WXHyvxhWxBMYITwY2xTmoPQ75k6M7ozmldZnV6B0LIQvowtnQcLoV0DoWUh\nAjB+Kg2hvSPDBA6IIIwfSkNo70gxIT0iEONr5wGhvSPHhNSIcIx1pSG0d0SZkBQRkrGmNIT2\njjATEiLCMpY7DwjtHXEmJEMEZywoDaG9I9GENIgIjJnSENo7Mk1IgYjCmHQeENo7Uk2gR8Ri\njJWG0N4RbAIxIh7jrfSWAz4gdHIEyviZrvNofv79VhpCJ0egjC15XqwWQntGvglUiNiM1ugG\n8ctoCJ0cgTI2pW86ILRnMjCBCBGdgZYjRHIwgQYRX2hsFAZIDibQIAgYmLbzTx4mKCoDQidH\noIyQCAidHIEyQiIgdHIEygiJgNDJESgjJAJCJ0egjJAICJ0cgTJCIiB0cgTKCImA0MkRKCMk\nAkInR6CMkAgInRyBMkIiIHRyBMoIiYDQyREoIyQCQidHoIyQCAidHIEyQiIgdHIEygiJYCw0\ngjjE2rMY8jqG4L1QlIsyUiIgtEQGyiAd0zVCF2ECBsogHdM1QhdhAgbKIB3TNUIXYQIGyiAd\n0zVCF2ECBsogHdM1QhdhAgbKIB3TNUIXYQIGyiAd0zVCF2ECBsogHdM1QhdhAgbKIB0TQZIF\nQiNZBUIjWQVCI1kFQiNZBUIjWQVCI1kFQiNZBUIjWQVCI1mFj9Au34i0R0Qm9JjIwzt9edQe\nEh0QpQw2Qps6vgnxER2GoIzYoVpUIoZ0ihn9GRMRv15D8e8ycgg+jVgELkL3EbkIZ4C4jEyO\ntIuFUSZ0/CY6vtAUHTTN5kb2Qovf0qFoPik6J0PSQ0cBqBI6PoKoT6dpa+L/PyBlUMfIn1Oj\n2piC0LSDuoXmrcQVOtbs6pxEMDqE9gpRywbZmDAiDc9GaIIZO6K9BTnsH6JiSBnVPiT/V+ex\n65ukDKIJISmjIkiiQGgkq0BoJKtAaCSrQGgkq0BoJKtAaCSrQGgkq0BoJKtAaCSrQGgkq0Bo\nJKtAaCSrQGgkq0BoJKtAaCSrQGgkq0BoJKtAaCSrQGgkq+gW2hT1o/l5f0d3f/35iu8LbPnh\nx7Fsxj5vHuryc/D+7RaH+683M4GsjJxRVAt9M1V9bX6GM8SYX0a7CP0onv49tg1VrjwwE7oZ\n8pvRc6HXRs4o+Vf4JWdz7n7en/vR7H+8xEXog9k33t335rhtqLUHPoRu/3xMh/w1Ds1pHJIm\n/wq/5NCskKtupfz6pH9+4i5CG9Otmh+TB/2FfvZL298MhM44ZshU6EvV/Gd+7O/fK1OcuseO\nRbM+7J9wLk15fj7/1D1+NN3KsnHXlN2zX3/XU42a1xbn4dfvux2gXZW/z7jz+dBxQehnV/Eo\nu75p6fn9ExdGfr3//rWXfdPkZ9FgQ+ix0H3Lcep/2wva9b+t0fv2RtU9cd9vQXaPd0++7J8v\naB7uV/n1nzm9SEcz2nirRq/9uNuP2vTZL+2mD1Ura+jukePK84f3Oxp5/P671577iqebrRKj\nV+i62R48dD/1yO5bd+evFbI7v5vZP5qPu2zvF7f6VrS/fd38ez/e/9nJVV/6EQ9mkLgRqDz2\nm5uX9qlN73vpXRvd/WtvHp7/KurJQ2/2M/3Nvi3v6GvP79/v58jT91/XRVv33/B/iuBoFvrc\nfKTtTz1M292GR59CvzrsfsV76W9eupv74fF7PUwqlF3L/GHH5dCuIttXVd2Dj/a/+X6o990O\n8Fzl1vX8oZ79fnPDxMlzamb5+e+3/jHy5P03f2XRbrTRLHS7Eq36FWn3SZfF62O9X077p9Cv\nR582Ld78fNa5bTauQ8fR53oqWnkmbc5CGz8MN31oZR76/ejS85dHnpbSbAFUt9E/ZsHRK/S8\nh76aZ5uwf/3aSehuXXgysxniW7vSDiZ0Pb3tI3R9ajcVvk5qSwmEHilQdZMFzZq7PF/uzkI3\n67tLXZYj0OjGRMpFRz+0G938JfTSbzYK3XQgxxI9tOxcu/2E3Rbc84O9vTYKmz+mQveN53Xc\nQ1crQt+aXnzUcVTP2YNuzV0N3eowVJf9YqdbDzev34Vefn77szzy6P0vDCo2OdTgmGE/4UiK\nfj/4tb5Ne+jLyixHXc+EbjYLi1HH0Yh1brbXrvuW1b22wT5lGt09txMOx34uon3x6KHLyizH\nx+3l5/dN/cLI4/ffvOE/zHKIT9XtJ+w3hZ4f7KNbRR+fncj1Q9humvfQ3RzP49b1XOiL+XDj\nNd4wRdy1q6Ohuu71NVvc6NXtABw9NGLX4/f7cXvx+QNkNvLw/ts5ymfB4qNY6MI8mp/+9kuK\nY7eKPrTH3c1aitNoT2Ex7Cms50I/zOccx+3QrC33f/2dc2PVaHbifbefa2hvXct+j/boodPK\nnsKP20vP7x+cjfz5/p97CnPwWbPQ8XIx8zkOhCYQOkL2WexElhkIHTyvbhlJEQgdPEU/m40k\nCYRGsgqERrIKhEayCoRGsgqERrIKhEayCoRGsgqERrIKhEayCoRGsgqERrIKhEayCoRGsgqE\nRrIKhEayCoRGssp/xvgEfV4WF9cAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print model to console\n",
    "model\n",
    "\n",
    "# Plot model\n",
    "plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage of glmnet\n",
    "\n",
    "What's the advantage of glmnet over regular glm models?\n",
    "* glmnet models place constraints on your coefficients, which helps prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a custom trainControl\n",
    "\n",
    "The wine quality dataset was a regression problem, but now you are looking at a classification problem. This is a simulated dataset based on the \"don't overfit\" competition on Kaggle a number of years ago.\n",
    "\n",
    "Classification problems are a little more complicated than regression problems because you have to provide a custom summaryFunction to the `train()` function to use the AUC metric to rank your models. Start by making a custom trainControl, as you did in the previous chapter. Be sure to set `classProbs = TRUE`, otherwise the twoClassSummary for summaryFunction will break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom trainControl: myControl\n",
    "myControl <- trainControl(\n",
    "  method = \"cv\", number = 10,\n",
    "  summaryFunction = twoClassSummary,\n",
    "  classProbs = TRUE, # IMPORTANT!\n",
    "  verboseIter = TRUE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit glmnet with custom trainControl\n",
    "\n",
    "Now that you have a custom trainControl object, fit a glmnet model to the \"don't overfit\" dataset. Recall from the video that glmnet is an extension of the generalized linear regression model (or glm) that places constraints on the magnitude of the coefficients to prevent overfitting. This is more commonly known as \"penalized\" regression modeling and is a very useful technique on datasets with many predictors and few values.\n",
    "\n",
    "glmnet is capable of fitting two different kinds of penalized models, controlled by the alpha parameter:\n",
    "\n",
    "* Ridge regression (or alpha = 0)\n",
    "* Lasso regression (or alpha = 1)\n",
    "\n",
    "You'll now fit a glmnet model to the \"don't overfit\" dataset using the defaults provided by the caret package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsed with column specification:\n",
      "cols(\n",
      "  .default = col_double(),\n",
      "  y = col_character()\n",
      ")\n",
      "See spec(...) for full column specifications.\n"
     ]
    }
   ],
   "source": [
    "overfit <- read_csv(\"overfit.csv\")\n",
    "overfit$y <- factor(overfit$y, levels = c(\"class1\", \"class2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes 'tbl_df', 'tbl' and 'data.frame':\t250 obs. of  201 variables:\n",
      " $ y   : Factor w/ 2 levels \"class1\",\"class2\": 2 2 2 1 2 2 2 2 2 2 ...\n",
      " $ X1  : num  0.915 0.937 0.286 0.83 0.642 ...\n",
      " $ X2  : num  0.3342 0.1884 0.2697 0.5307 0.0215 ...\n",
      " $ X3  : num  0.137 0.177 0.52 0.811 0.115 ...\n",
      " $ X4  : num  0.2449 0.0876 0.3911 0.1826 0.1336 ...\n",
      " $ X5  : num  0.8483 0.0627 0.8198 0.5394 0.499 ...\n",
      " $ X6  : num  0.7359 0.7518 0.3326 0.0575 0.6744 ...\n",
      " $ X7  : num  0.0539 0.9551 0.0256 0.9208 0.3667 ...\n",
      " $ X8  : num  0.165 0.728 0.206 0.586 0.914 ...\n",
      " $ X9  : num  0.99 0.438 0.7 0.889 0.834 ...\n",
      " $ X10 : num  0.246 0.023 0.284 0.813 0.719 ...\n",
      " $ X11 : num  0.0604 0.933 0.3489 0.4118 0.9611 ...\n",
      " $ X12 : num  0.409 0.316 0.495 0.649 0.65 ...\n",
      " $ X13 : num  0.274 0.944 0.446 0.542 0.162 ...\n",
      " $ X14 : num  0.313 0.689 0.532 0.76 0.65 ...\n",
      " $ X15 : num  0.199 0.566 0.168 0.944 0.504 ...\n",
      " $ X16 : num  0.856 0.217 0.317 0.541 0.43 ...\n",
      " $ X17 : num  0.5989 0.9689 0.3905 0.8528 0.0423 ...\n",
      " $ X18 : num  0.364 0.433 0.729 0.263 0.769 ...\n",
      " $ X19 : num  0.2999 0.0742 0.9283 0.4011 0.6654 ...\n",
      " $ X20 : num  0.512 0.498 0.137 0.451 0.313 ...\n",
      " $ X21 : num  0.731 0.561 0.498 0.841 0.464 ...\n",
      " $ X22 : num  0.8867 0.7896 0.5464 0.0632 0.5762 ...\n",
      " $ X23 : num  0.843 0.769 0.919 0.356 0.938 ...\n",
      " $ X24 : num  0.227 0.293 0.126 0.71 0.848 ...\n",
      " $ X25 : num  0.2465 0.5303 0.214 0.0258 0.342 ...\n",
      " $ X26 : num  0.0308 0.9101 0.1426 0.4396 0.4499 ...\n",
      " $ X27 : num  0.244 0.886 0.366 0.25 0.771 ...\n",
      " $ X28 : num  0.4403 0.4453 0.0809 0.1529 0.8205 ...\n",
      " $ X29 : num  0.6143 0.8762 0.0962 0.1754 0.4546 ...\n",
      " $ X30 : num  0.607 0.0316 0.9398 0.7431 0.1437 ...\n",
      " $ X31 : num  0.7765 0.0238 0.4318 0.3665 0.3954 ...\n",
      " $ X32 : num  0.21 0.712 0.435 0.976 0.7 ...\n",
      " $ X33 : num  0.444 0.328 0.208 0.192 0.372 ...\n",
      " $ X34 : num  0.24 0.891 0.148 0.424 0.729 ...\n",
      " $ X35 : num  0.636 0.684 0.463 0.593 0.199 ...\n",
      " $ X36 : num  0.572 0.462 0.135 0.582 0.115 ...\n",
      " $ X37 : num  0.42 0.826 0.112 0.204 0.492 ...\n",
      " $ X38 : num  0.95 0.113 0.858 0.27 0.272 ...\n",
      " $ X39 : num  0.359 0.497 0.251 0.775 0.139 ...\n",
      " $ X40 : num  0.497 0.994 0.685 0.717 0.51 ...\n",
      " $ X41 : num  0.528 0.646 0.834 0.346 0.622 ...\n",
      " $ X42 : num  0.5364 0.0222 0.9715 0.6008 0.4174 ...\n",
      " $ X43 : num  0.0035 0.4706 0.4474 0.4186 0.8656 ...\n",
      " $ X44 : num  0.0311 0.3712 0.2891 0.1428 0.6456 ...\n",
      " $ X45 : num  0.79 0.608 0.986 0.651 0.389 ...\n",
      " $ X46 : num  0.908 0.2284 0.0361 0.7552 0.1496 ...\n",
      " $ X47 : num  0.574 0.347 0.543 0.27 0.703 ...\n",
      " $ X48 : num  0.0466 0.8138 0.0697 0.4573 0.9325 ...\n",
      " $ X49 : num  0.569 0.218 0.102 0.698 0.193 ...\n",
      " $ X50 : num  0.707 0.935 0.324 0.43 0.264 ...\n",
      " $ X51 : num  0.3566 0.208 0.1006 0.1381 0.0237 ...\n",
      " $ X52 : num  0.618 0.352 0.645 0.211 0.968 ...\n",
      " $ X53 : num  0.894 0.481 0.16 0.906 0.748 ...\n",
      " $ X54 : num  0.092 0.0437 0.0398 0.6777 0.3246 ...\n",
      " $ X55 : num  0.4067 0.9027 0.1804 0.0599 0.3799 ...\n",
      " $ X56 : num  0.8223 0.4676 0.8833 0.2398 0.0861 ...\n",
      " $ X57 : num  0.9217 0.0254 0.7113 0.081 0.8368 ...\n",
      " $ X58 : num  0.701 0.209 0.309 0.751 0.839 ...\n",
      " $ X59 : num  0.57086 0.31707 0.00623 0.32235 0.76597 ...\n",
      " $ X60 : num  0.224 0.7177 0.2335 0.1109 0.0151 ...\n",
      " $ X61 : num  0.6154 0.815 0.343 0.3883 0.0146 ...\n",
      " $ X62 : num  0.394 0.948 0.753 0.866 0.375 ...\n",
      " $ X63 : num  0.889 0.517 0.349 0.856 0.139 ...\n",
      " $ X64 : num  0.61 0.801 0.525 0.672 0.29 ...\n",
      " $ X65 : num  0.4771 0.1992 0.4017 0.5752 0.0637 ...\n",
      " $ X66 : num  0.558 0.582 0.353 0.92 0.143 ...\n",
      " $ X67 : num  0.691 0.36 0.517 0.498 0.127 ...\n",
      " $ X68 : num  0.7902 0.2943 0.3263 0.0217 0.4727 ...\n",
      " $ X69 : num  0.531 0.645 0.14 0.317 0.871 ...\n",
      " $ X70 : num  0.612 0.225 0.445 0.442 0.819 ...\n",
      " $ X71 : num  0.993 0.66 0.991 0.844 0.932 ...\n",
      " $ X72 : num  0.6689 0.7696 0.0429 0.0932 0.3703 ...\n",
      " $ X73 : num  0.178229 0.589873 0.310012 0.90483 0.000934 ...\n",
      " $ X74 : num  0.6517 0.0996 0.6174 0.3786 0.0233 ...\n",
      " $ X75 : num  0.1314 0.208 0.6572 0.0518 0.8911 ...\n",
      " $ X76 : num  0.602 0.515 0.952 0.69 0.691 ...\n",
      " $ X77 : num  0.8004 0.3705 0.4389 0.0894 0.1672 ...\n",
      " $ X78 : num  0.0338 0.1946 0.6498 0.2197 0.8582 ...\n",
      " $ X79 : num  0.6149 0.0757 0.5525 0.4636 0.9301 ...\n",
      " $ X80 : num  0.55 0.907 0.782 0.605 0.914 ...\n",
      " $ X81 : num  0.878 0.455 0.425 0.939 0.386 ...\n",
      " $ X82 : num  0.877 0.559 0.689 0.674 0.189 ...\n",
      " $ X83 : num  0.459 0.969 0.796 0.111 0.431 ...\n",
      " $ X84 : num  0.0227 0.5166 0.1737 0.8481 0.483 ...\n",
      " $ X85 : num  0.821 0.0939 0.1739 0.0846 0.9957 ...\n",
      " $ X86 : num  0.945 0.669 0.352 0.273 0.706 ...\n",
      " $ X87 : num  0.4 0.363 0.932 0.215 0.185 ...\n",
      " $ X88 : num  0.506 0.402 0.141 0.69 0.105 ...\n",
      " $ X89 : num  0.3842 0.3811 0.6787 0.7559 0.0621 ...\n",
      " $ X90 : num  0.598 0.344 0.387 0.827 0.716 ...\n",
      " $ X91 : num  0.318 0.958 0.269 0.602 0.566 ...\n",
      " $ X92 : num  0.6358 0.1826 0.611 0.0939 0.5087 ...\n",
      " $ X93 : num  0.169 0.1698 0.0551 0.6548 0.8838 ...\n",
      " $ X94 : num  0.436 0.507 0.705 0.661 0.25 ...\n",
      " $ X95 : num  0.7456 0.8874 0.0527 0.1275 0.4316 ...\n",
      " $ X96 : num  0.912 0.744 0.247 0.741 0.891 ...\n",
      " $ X97 : num  0.52145 0.00255 0.7744 0.03756 0.59865 ...\n",
      " $ X98 : num  0.0918 0.7237 0.4736 0.0695 0.4187 ...\n",
      "  [list output truncated]\n",
      " - attr(*, \"spec\")=List of 2\n",
      "  ..$ cols   :List of 201\n",
      "  .. ..$ y   : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_character\" \"collector\"\n",
      "  .. ..$ X1  : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X2  : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X3  : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X4  : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X5  : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X6  : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X7  : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X8  : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X9  : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X10 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X11 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X12 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X13 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X14 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X15 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X16 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X17 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X18 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X19 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X20 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X21 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X22 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X23 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X24 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X25 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X26 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X27 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X28 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X29 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X30 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X31 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X32 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X33 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X34 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X35 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X36 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X37 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X38 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X39 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X40 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X41 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X42 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X43 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X44 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X45 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X46 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X47 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X48 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X49 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X50 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X51 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X52 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X53 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X54 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X55 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X56 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X57 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X58 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X59 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X60 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X61 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X62 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X63 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X64 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X65 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X66 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X67 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X68 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X69 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X70 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X71 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X72 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X73 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X74 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X75 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X76 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X77 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X78 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X79 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X80 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X81 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X82 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X83 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X84 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X85 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X86 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X87 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X88 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X89 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X90 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X91 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X92 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X93 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X94 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X95 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X96 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X97 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. ..$ X98 : list()\n",
      "  .. .. ..- attr(*, \"class\")= chr  \"collector_double\" \"collector\"\n",
      "  .. .. [list output truncated]\n",
      "  ..$ default: list()\n",
      "  .. ..- attr(*, \"class\")= chr  \"collector_guess\" \"collector\"\n",
      "  ..- attr(*, \"class\")= chr \"col_spec\"\n"
     ]
    }
   ],
   "source": [
    "str(overfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in train.default(x, y, weights = w, ...):\n",
      "\"The metric \"Accuracy\" was not in the result set. ROC will be used instead.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01: alpha=0.10, lambda=0.01013 \n",
      "- Fold01: alpha=0.10, lambda=0.01013 \n",
      "+ Fold01: alpha=0.55, lambda=0.01013 \n",
      "- Fold01: alpha=0.55, lambda=0.01013 \n",
      "+ Fold01: alpha=1.00, lambda=0.01013 \n",
      "- Fold01: alpha=1.00, lambda=0.01013 \n",
      "+ Fold02: alpha=0.10, lambda=0.01013 \n",
      "- Fold02: alpha=0.10, lambda=0.01013 \n",
      "+ Fold02: alpha=0.55, lambda=0.01013 \n",
      "- Fold02: alpha=0.55, lambda=0.01013 \n",
      "+ Fold02: alpha=1.00, lambda=0.01013 \n",
      "- Fold02: alpha=1.00, lambda=0.01013 \n",
      "+ Fold03: alpha=0.10, lambda=0.01013 \n",
      "- Fold03: alpha=0.10, lambda=0.01013 \n",
      "+ Fold03: alpha=0.55, lambda=0.01013 \n",
      "- Fold03: alpha=0.55, lambda=0.01013 \n",
      "+ Fold03: alpha=1.00, lambda=0.01013 \n",
      "- Fold03: alpha=1.00, lambda=0.01013 \n",
      "+ Fold04: alpha=0.10, lambda=0.01013 \n",
      "- Fold04: alpha=0.10, lambda=0.01013 \n",
      "+ Fold04: alpha=0.55, lambda=0.01013 \n",
      "- Fold04: alpha=0.55, lambda=0.01013 \n",
      "+ Fold04: alpha=1.00, lambda=0.01013 \n",
      "- Fold04: alpha=1.00, lambda=0.01013 \n",
      "+ Fold05: alpha=0.10, lambda=0.01013 \n",
      "- Fold05: alpha=0.10, lambda=0.01013 \n",
      "+ Fold05: alpha=0.55, lambda=0.01013 \n",
      "- Fold05: alpha=0.55, lambda=0.01013 \n",
      "+ Fold05: alpha=1.00, lambda=0.01013 \n",
      "- Fold05: alpha=1.00, lambda=0.01013 \n",
      "+ Fold06: alpha=0.10, lambda=0.01013 \n",
      "- Fold06: alpha=0.10, lambda=0.01013 \n",
      "+ Fold06: alpha=0.55, lambda=0.01013 \n",
      "- Fold06: alpha=0.55, lambda=0.01013 \n",
      "+ Fold06: alpha=1.00, lambda=0.01013 \n",
      "- Fold06: alpha=1.00, lambda=0.01013 \n",
      "+ Fold07: alpha=0.10, lambda=0.01013 \n",
      "- Fold07: alpha=0.10, lambda=0.01013 \n",
      "+ Fold07: alpha=0.55, lambda=0.01013 \n",
      "- Fold07: alpha=0.55, lambda=0.01013 \n",
      "+ Fold07: alpha=1.00, lambda=0.01013 \n",
      "- Fold07: alpha=1.00, lambda=0.01013 \n",
      "+ Fold08: alpha=0.10, lambda=0.01013 \n",
      "- Fold08: alpha=0.10, lambda=0.01013 \n",
      "+ Fold08: alpha=0.55, lambda=0.01013 \n",
      "- Fold08: alpha=0.55, lambda=0.01013 \n",
      "+ Fold08: alpha=1.00, lambda=0.01013 \n",
      "- Fold08: alpha=1.00, lambda=0.01013 \n",
      "+ Fold09: alpha=0.10, lambda=0.01013 \n",
      "- Fold09: alpha=0.10, lambda=0.01013 \n",
      "+ Fold09: alpha=0.55, lambda=0.01013 \n",
      "- Fold09: alpha=0.55, lambda=0.01013 \n",
      "+ Fold09: alpha=1.00, lambda=0.01013 \n",
      "- Fold09: alpha=1.00, lambda=0.01013 \n",
      "+ Fold10: alpha=0.10, lambda=0.01013 \n",
      "- Fold10: alpha=0.10, lambda=0.01013 \n",
      "+ Fold10: alpha=0.55, lambda=0.01013 \n",
      "- Fold10: alpha=0.55, lambda=0.01013 \n",
      "+ Fold10: alpha=1.00, lambda=0.01013 \n",
      "- Fold10: alpha=1.00, lambda=0.01013 \n",
      "Aggregating results\n",
      "Selecting tuning parameters\n",
      "Fitting alpha = 0.55, lambda = 0.0101 on full training set\n"
     ]
    }
   ],
   "source": [
    "# Fit glmnet model: model\n",
    "model <- train(\n",
    "  y ~ . , overfit,\n",
    "  method = \"glmnet\",\n",
    "  trControl = myControl\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "glmnet \n",
       "\n",
       "250 samples\n",
       "200 predictors\n",
       "  2 classes: 'class1', 'class2' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 226, 226, 225, 225, 225, 224, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  alpha  lambda        ROC        Sens  Spec     \n",
       "  0.10   0.0001012745  0.4731884  0.05  0.9655797\n",
       "  0.10   0.0010127448  0.4774457  0.05  0.9699275\n",
       "  0.10   0.0101274483  0.4904891  0.00  0.9827899\n",
       "  0.55   0.0001012745  0.4586051  0.05  0.9568841\n",
       "  0.55   0.0010127448  0.4586051  0.00  0.9610507\n",
       "  0.55   0.0101274483  0.4998188  0.00  0.9871377\n",
       "  1.00   0.0001012745  0.4107790  0.00  0.9353261\n",
       "  1.00   0.0010127448  0.4108696  0.00  0.9525362\n",
       "  1.00   0.0101274483  0.4287138  0.00  0.9869565\n",
       "\n",
       "ROC was used to select the optimal model using the largest value.\n",
       "The final values used for the model were alpha = 0.55 and lambda = 0.01012745."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "1"
      ],
      "text/latex": [
       "1"
      ],
      "text/markdown": [
       "1"
      ],
      "text/plain": [
       "[1] 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print model to console\n",
    "model\n",
    "\n",
    "# Print maximum ROC statistic\n",
    "max(model[[\"results\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why a custom tuning grid?\n",
    "\n",
    "Why use a custom tuning grid for a glmnet model?\n",
    "* The default tuning grid is very small and there are many more potential glmnet models you want to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### glmnet with custom trainControl and tuning\n",
    "\n",
    "As you saw in the video, the glmnet model actually fits many models at once (one of the great things about the package). You can exploit this by passing a large number of `lambda` values, which control the amount of penalization in the model. `train()` is smart enough to only fit one model per alpha value and pass all of the lambda values at once for simultaneous fitting.\n",
    "\n",
    "My favorite tuning grid for glmnet models is:\n",
    "\n",
    "    expand.grid(alpha = 0:1,\n",
    "      lambda = seq(0.0001, 1, length = 100))\n",
    "\n",
    "This grid explores a large number of lambda values (100, in fact), from a very small one to a very large one. (You could increase the maximum lambda to 10, but in this exercise 1 is a good upper bound.)\n",
    "\n",
    "If you want to explore fewer models, you can use a shorter lambda sequence. For example, `lambda = seq(0.0001, 1, length = 10)` would fit 10 models per value of alpha.\n",
    "\n",
    "You also look at the two forms of penalized models with this tuneGrid: ridge regression and lasso regression. `alpha = 0` is pure ridge regression, and `alpha = 1` is pure lasso regression. You can fit a mixture of the two models (i.e. an elastic net) using an alpha between 0 and 1. For example, alpha = .05 would be 95% ridge regression and 5% lasso regression.\n",
    "\n",
    "In this problem you'll just explore the 2 extremes--pure ridge and pure lasso regression--for the purpose of illustrating their differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in train.default(x, y, weights = w, ...):\n",
      "\"The metric \"Accuracy\" was not in the result set. ROC will be used instead.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01: alpha=0, lambda=1 \n",
      "- Fold01: alpha=0, lambda=1 \n",
      "+ Fold01: alpha=1, lambda=1 \n",
      "- Fold01: alpha=1, lambda=1 \n",
      "+ Fold02: alpha=0, lambda=1 \n",
      "- Fold02: alpha=0, lambda=1 \n",
      "+ Fold02: alpha=1, lambda=1 \n",
      "- Fold02: alpha=1, lambda=1 \n",
      "+ Fold03: alpha=0, lambda=1 \n",
      "- Fold03: alpha=0, lambda=1 \n",
      "+ Fold03: alpha=1, lambda=1 \n",
      "- Fold03: alpha=1, lambda=1 \n",
      "+ Fold04: alpha=0, lambda=1 \n",
      "- Fold04: alpha=0, lambda=1 \n",
      "+ Fold04: alpha=1, lambda=1 \n",
      "- Fold04: alpha=1, lambda=1 \n",
      "+ Fold05: alpha=0, lambda=1 \n",
      "- Fold05: alpha=0, lambda=1 \n",
      "+ Fold05: alpha=1, lambda=1 \n",
      "- Fold05: alpha=1, lambda=1 \n",
      "+ Fold06: alpha=0, lambda=1 \n",
      "- Fold06: alpha=0, lambda=1 \n",
      "+ Fold06: alpha=1, lambda=1 \n",
      "- Fold06: alpha=1, lambda=1 \n",
      "+ Fold07: alpha=0, lambda=1 \n",
      "- Fold07: alpha=0, lambda=1 \n",
      "+ Fold07: alpha=1, lambda=1 \n",
      "- Fold07: alpha=1, lambda=1 \n",
      "+ Fold08: alpha=0, lambda=1 \n",
      "- Fold08: alpha=0, lambda=1 \n",
      "+ Fold08: alpha=1, lambda=1 \n",
      "- Fold08: alpha=1, lambda=1 \n",
      "+ Fold09: alpha=0, lambda=1 \n",
      "- Fold09: alpha=0, lambda=1 \n",
      "+ Fold09: alpha=1, lambda=1 \n",
      "- Fold09: alpha=1, lambda=1 \n",
      "+ Fold10: alpha=0, lambda=1 \n",
      "- Fold10: alpha=0, lambda=1 \n",
      "+ Fold10: alpha=1, lambda=1 \n",
      "- Fold10: alpha=1, lambda=1 \n",
      "Aggregating results\n",
      "Selecting tuning parameters\n",
      "Fitting alpha = 1, lambda = 1 on full training set\n"
     ]
    }
   ],
   "source": [
    "# Train glmnet with custom trainControl and tuning: model\n",
    "model <- train(\n",
    "  y ~ . , overfit,\n",
    "  tuneGrid = expand.grid(alpha = 0:1, lambda = seq(0.0001, 1, length = 20)),\n",
    "  method = \"glmnet\",\n",
    "  trControl = myControl\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "glmnet \n",
       "\n",
       "250 samples\n",
       "200 predictors\n",
       "  2 classes: 'class1', 'class2' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 225, 225, 224, 225, 225, 226, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  alpha  lambda      ROC        Sens  Spec     \n",
       "  0      0.00010000  0.4391304  0.00  0.9744565\n",
       "  0      0.05272632  0.4385870  0.00  0.9871377\n",
       "  0      0.10535263  0.4470109  0.00  1.0000000\n",
       "  0      0.15797895  0.4465580  0.00  1.0000000\n",
       "  0      0.21060526  0.4528986  0.00  1.0000000\n",
       "  0      0.26323158  0.4570652  0.00  1.0000000\n",
       "  0      0.31585789  0.4614130  0.00  1.0000000\n",
       "  0      0.36848421  0.4635870  0.00  1.0000000\n",
       "  0      0.42111053  0.4656703  0.00  1.0000000\n",
       "  0      0.47373684  0.4678442  0.00  1.0000000\n",
       "  0      0.52636316  0.4678442  0.00  1.0000000\n",
       "  0      0.57898947  0.4699275  0.00  1.0000000\n",
       "  0      0.63161579  0.4742754  0.00  1.0000000\n",
       "  0      0.68424211  0.4764493  0.00  1.0000000\n",
       "  0      0.73686842  0.4785326  0.00  1.0000000\n",
       "  0      0.78949474  0.4785326  0.00  1.0000000\n",
       "  0      0.84212105  0.4850543  0.00  1.0000000\n",
       "  0      0.89474737  0.4850543  0.00  1.0000000\n",
       "  0      0.94737368  0.4850543  0.00  1.0000000\n",
       "  0      1.00000000  0.4894022  0.00  1.0000000\n",
       "  1      0.00010000  0.3907609  0.05  0.9012681\n",
       "  1      0.05272632  0.5000000  0.00  1.0000000\n",
       "  1      0.10535263  0.5000000  0.00  1.0000000\n",
       "  1      0.15797895  0.5000000  0.00  1.0000000\n",
       "  1      0.21060526  0.5000000  0.00  1.0000000\n",
       "  1      0.26323158  0.5000000  0.00  1.0000000\n",
       "  1      0.31585789  0.5000000  0.00  1.0000000\n",
       "  1      0.36848421  0.5000000  0.00  1.0000000\n",
       "  1      0.42111053  0.5000000  0.00  1.0000000\n",
       "  1      0.47373684  0.5000000  0.00  1.0000000\n",
       "  1      0.52636316  0.5000000  0.00  1.0000000\n",
       "  1      0.57898947  0.5000000  0.00  1.0000000\n",
       "  1      0.63161579  0.5000000  0.00  1.0000000\n",
       "  1      0.68424211  0.5000000  0.00  1.0000000\n",
       "  1      0.73686842  0.5000000  0.00  1.0000000\n",
       "  1      0.78949474  0.5000000  0.00  1.0000000\n",
       "  1      0.84212105  0.5000000  0.00  1.0000000\n",
       "  1      0.89474737  0.5000000  0.00  1.0000000\n",
       "  1      0.94737368  0.5000000  0.00  1.0000000\n",
       "  1      1.00000000  0.5000000  0.00  1.0000000\n",
       "\n",
       "ROC was used to select the optimal model using the largest value.\n",
       "The final values used for the model were alpha = 1 and lambda = 1."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.5"
      ],
      "text/latex": [
       "0.5"
      ],
      "text/markdown": [
       "0.5"
      ],
      "text/plain": [
       "[1] 0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print model to console\n",
    "model\n",
    "\n",
    "# Print maximum ROC statistic\n",
    "max(model[[\"results\"]][[\"ROC\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAAHgCAMAAAC7G6qeAAAAOVBMVEUAAAAAgP9NTU1oaGh8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\nSUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage of random forests\n",
    "\n",
    "Here's the tuning plot for the custom tuned glmnet model you created in the last exercise. For the overfit dataset, which value of alpha is better?\n",
    "* alpha = 1 (lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median imputation vs. omitting rows\n",
    "\n",
    "What's the value of median imputation?\n",
    "* It lets you model data with missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply median imputation\n",
    "\n",
    "In this chapter, you'll be using a version of the Wisconsin Breast Cancer dataset. This dataset presents a classic binary classification problem: 50% of the samples are benign, 50% are malignant, and the challenge is to identify which are which.\n",
    "\n",
    "This dataset is interesting because many of the predictors contain missing values and most rows of the dataset have at least one missing value. This presents a modeling challenge, because most machine learning algorithms cannot handle missing values out of the box. For example, your first instinct might be to fit a logistic regression model to this data, but prior to doing this you need a strategy for handling the `NAs`.\n",
    "\n",
    "Fortunately, the `train()` function in caret contains an argument called preProcess, which allows you to specify that median imputation should be used to fill in the missing values. In previous chapters, you created models with the `train()` function using formulas such as `y ~ .`. An alternative way is to specify the `x` and `y` arguments to `train()`, where `x` is an object with samples in rows and features in columns and `y` is a numeric or factor vector containing the outcomes. Said differently, `x` is a matrix or data frame that contains the whole dataset you'd use for the data argument to the `lm()` call, for example, but excludes the response variable column; `y` is a vector that contains just the response variable column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "load(\"BreastCancer.RData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'bins'</li>\n",
       "\t<li>'Boston'</li>\n",
       "\t<li>'boston_scaled'</li>\n",
       "\t<li>'breast_cancer_x'</li>\n",
       "\t<li>'breast_cancer_y'</li>\n",
       "\t<li>'cor_matrix'</li>\n",
       "\t<li>'correct_classes'</li>\n",
       "\t<li>'crime'</li>\n",
       "\t<li>'diamonds'</li>\n",
       "\t<li>'error'</li>\n",
       "\t<li>'ind'</li>\n",
       "\t<li>'m_or_r'</li>\n",
       "\t<li>'model'</li>\n",
       "\t<li>'myControl'</li>\n",
       "\t<li>'n'</li>\n",
       "\t<li>'overfit'</li>\n",
       "\t<li>'p'</li>\n",
       "\t<li>'p_class'</li>\n",
       "\t<li>'rows'</li>\n",
       "\t<li>'Sonar'</li>\n",
       "\t<li>'sonar_test'</li>\n",
       "\t<li>'sonar_train'</li>\n",
       "\t<li>'split'</li>\n",
       "\t<li>'test'</li>\n",
       "\t<li>'train'</li>\n",
       "\t<li>'tuneGrid'</li>\n",
       "\t<li>'wine'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'bins'\n",
       "\\item 'Boston'\n",
       "\\item 'boston\\_scaled'\n",
       "\\item 'breast\\_cancer\\_x'\n",
       "\\item 'breast\\_cancer\\_y'\n",
       "\\item 'cor\\_matrix'\n",
       "\\item 'correct\\_classes'\n",
       "\\item 'crime'\n",
       "\\item 'diamonds'\n",
       "\\item 'error'\n",
       "\\item 'ind'\n",
       "\\item 'm\\_or\\_r'\n",
       "\\item 'model'\n",
       "\\item 'myControl'\n",
       "\\item 'n'\n",
       "\\item 'overfit'\n",
       "\\item 'p'\n",
       "\\item 'p\\_class'\n",
       "\\item 'rows'\n",
       "\\item 'Sonar'\n",
       "\\item 'sonar\\_test'\n",
       "\\item 'sonar\\_train'\n",
       "\\item 'split'\n",
       "\\item 'test'\n",
       "\\item 'train'\n",
       "\\item 'tuneGrid'\n",
       "\\item 'wine'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'bins'\n",
       "2. 'Boston'\n",
       "3. 'boston_scaled'\n",
       "4. 'breast_cancer_x'\n",
       "5. 'breast_cancer_y'\n",
       "6. 'cor_matrix'\n",
       "7. 'correct_classes'\n",
       "8. 'crime'\n",
       "9. 'diamonds'\n",
       "10. 'error'\n",
       "11. 'ind'\n",
       "12. 'm_or_r'\n",
       "13. 'model'\n",
       "14. 'myControl'\n",
       "15. 'n'\n",
       "16. 'overfit'\n",
       "17. 'p'\n",
       "18. 'p_class'\n",
       "19. 'rows'\n",
       "20. 'Sonar'\n",
       "21. 'sonar_test'\n",
       "22. 'sonar_train'\n",
       "23. 'split'\n",
       "24. 'test'\n",
       "25. 'train'\n",
       "26. 'tuneGrid'\n",
       "27. 'wine'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"bins\"            \"Boston\"          \"boston_scaled\"   \"breast_cancer_x\"\n",
       " [5] \"breast_cancer_y\" \"cor_matrix\"      \"correct_classes\" \"crime\"          \n",
       " [9] \"diamonds\"        \"error\"           \"ind\"             \"m_or_r\"         \n",
       "[13] \"model\"           \"myControl\"       \"n\"               \"overfit\"        \n",
       "[17] \"p\"               \"p_class\"         \"rows\"            \"Sonar\"          \n",
       "[21] \"sonar_test\"      \"sonar_train\"     \"split\"           \"test\"           \n",
       "[25] \"train\"           \"tuneGrid\"        \"wine\"           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Cl.thickness</th><th scope=col>Cell.size</th><th scope=col>Cell.shape</th><th scope=col>Marg.adhesion</th><th scope=col>Epith.c.size</th><th scope=col>Bare.nuclei</th><th scope=col>Bl.cromatin</th><th scope=col>Normal.nucleoli</th><th scope=col>Mitoses</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 5</td><td>NA</td><td> 1</td><td> 1</td><td>NA</td><td> 1</td><td> 3</td><td> 1</td><td>1 </td></tr>\n",
       "\t<tr><td>NA</td><td> 4</td><td> 4</td><td>NA</td><td> 7</td><td>10</td><td> 3</td><td>NA</td><td>1 </td></tr>\n",
       "\t<tr><td>NA</td><td>NA</td><td> 1</td><td> 1</td><td> 2</td><td>NA</td><td> 3</td><td> 1</td><td>1 </td></tr>\n",
       "\t<tr><td> 6</td><td> 8</td><td> 8</td><td>NA</td><td>NA</td><td> 4</td><td> 3</td><td> 7</td><td>1 </td></tr>\n",
       "\t<tr><td> 4</td><td> 1</td><td> 1</td><td> 3</td><td> 2</td><td> 1</td><td> 3</td><td>NA</td><td>1 </td></tr>\n",
       "\t<tr><td> 8</td><td>10</td><td>10</td><td> 8</td><td> 7</td><td>10</td><td>NA</td><td> 7</td><td>1 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllll}\n",
       " Cl.thickness & Cell.size & Cell.shape & Marg.adhesion & Epith.c.size & Bare.nuclei & Bl.cromatin & Normal.nucleoli & Mitoses\\\\\n",
       "\\hline\n",
       "\t  5 & NA &  1 &  1 & NA &  1 &  3 &  1 & 1 \\\\\n",
       "\t NA &  4 &  4 & NA &  7 & 10 &  3 & NA & 1 \\\\\n",
       "\t NA & NA &  1 &  1 &  2 & NA &  3 &  1 & 1 \\\\\n",
       "\t  6 &  8 &  8 & NA & NA &  4 &  3 &  7 & 1 \\\\\n",
       "\t  4 &  1 &  1 &  3 &  2 &  1 &  3 & NA & 1 \\\\\n",
       "\t  8 & 10 & 10 &  8 &  7 & 10 & NA &  7 & 1 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "Cl.thickness | Cell.size | Cell.shape | Marg.adhesion | Epith.c.size | Bare.nuclei | Bl.cromatin | Normal.nucleoli | Mitoses | \n",
       "|---|---|---|---|---|---|\n",
       "|  5 | NA |  1 |  1 | NA |  1 |  3 |  1 | 1  | \n",
       "| NA |  4 |  4 | NA |  7 | 10 |  3 | NA | 1  | \n",
       "| NA | NA |  1 |  1 |  2 | NA |  3 |  1 | 1  | \n",
       "|  6 |  8 |  8 | NA | NA |  4 |  3 |  7 | 1  | \n",
       "|  4 |  1 |  1 |  3 |  2 |  1 |  3 | NA | 1  | \n",
       "|  8 | 10 | 10 |  8 |  7 | 10 | NA |  7 | 1  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  Cl.thickness Cell.size Cell.shape Marg.adhesion Epith.c.size Bare.nuclei\n",
       "1  5           NA         1          1            NA            1         \n",
       "2 NA            4         4         NA             7           10         \n",
       "3 NA           NA         1          1             2           NA         \n",
       "4  6            8         8         NA            NA            4         \n",
       "5  4            1         1          3             2            1         \n",
       "6  8           10        10          8             7           10         \n",
       "  Bl.cromatin Normal.nucleoli Mitoses\n",
       "1  3           1              1      \n",
       "2  3          NA              1      \n",
       "3  3           1              1      \n",
       "4  3           7              1      \n",
       "5  3          NA              1      \n",
       "6 NA           7              1      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>benign</li>\n",
       "\t<li>benign</li>\n",
       "\t<li>benign</li>\n",
       "\t<li>benign</li>\n",
       "\t<li>benign</li>\n",
       "\t<li>malignant</li>\n",
       "</ol>\n",
       "\n",
       "<details>\n",
       "\t<summary style=display:list-item;cursor:pointer>\n",
       "\t\t<strong>Levels</strong>:\n",
       "\t</summary>\n",
       "\t<ol class=list-inline>\n",
       "\t\t<li>'benign'</li>\n",
       "\t\t<li>'malignant'</li>\n",
       "\t</ol>\n",
       "</details>"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item benign\n",
       "\\item benign\n",
       "\\item benign\n",
       "\\item benign\n",
       "\\item benign\n",
       "\\item malignant\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\emph{Levels}: \\begin{enumerate*}\n",
       "\\item 'benign'\n",
       "\\item 'malignant'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. benign\n",
       "2. benign\n",
       "3. benign\n",
       "4. benign\n",
       "5. benign\n",
       "6. malignant\n",
       "\n",
       "\n",
       "\n",
       "**Levels**: 1. 'benign'\n",
       "2. 'malignant'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] benign    benign    benign    benign    benign    malignant\n",
       "Levels: benign malignant"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(breast_cancer_x)\n",
    "head(breast_cancer_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in train.default(x = breast_cancer_x, y = breast_cancer_y, method = \"glm\", :\n",
      "\"The metric \"Accuracy\" was not in the result set. ROC will be used instead.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01: parameter=none \n",
      "- Fold01: parameter=none \n",
      "+ Fold02: parameter=none \n",
      "- Fold02: parameter=none \n",
      "+ Fold03: parameter=none \n",
      "- Fold03: parameter=none \n",
      "+ Fold04: parameter=none \n",
      "- Fold04: parameter=none \n",
      "+ Fold05: parameter=none \n",
      "- Fold05: parameter=none \n",
      "+ Fold06: parameter=none \n",
      "- Fold06: parameter=none \n",
      "+ Fold07: parameter=none \n",
      "- Fold07: parameter=none \n",
      "+ Fold08: parameter=none \n",
      "- Fold08: parameter=none \n",
      "+ Fold09: parameter=none \n",
      "- Fold09: parameter=none \n",
      "+ Fold10: parameter=none \n",
      "- Fold10: parameter=none \n",
      "Aggregating results\n",
      "Fitting final model on full training set\n"
     ]
    }
   ],
   "source": [
    "# Apply median imputation: model\n",
    "model <- train(\n",
    "  x = breast_cancer_x, y = breast_cancer_y,\n",
    "  method = \"glm\",\n",
    "  trControl = myControl,\n",
    "  preProcess = \"medianImpute\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generalized Linear Model \n",
       "\n",
       "699 samples\n",
       "  9 predictor\n",
       "  2 classes: 'benign', 'malignant' \n",
       "\n",
       "Pre-processing: median imputation (9) \n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 628, 629, 630, 629, 630, 629, ... \n",
       "Resampling results:\n",
       "\n",
       "  ROC        Sens       Spec\n",
       "  0.9920845  0.9694203  0.95\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print model to console\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing KNN imputation to median imputation\n",
    "\n",
    "Will KNN imputation always be better than median imputation?\n",
    "* No, you should try both options and keep the one that gives more accurate models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use KNN imputation\n",
    "\n",
    "In the previous exercise, you used median imputation to fill in missing values in the breast cancer dataset, but that is not the only possible method for dealing with missing data.\n",
    "\n",
    "An alternative to median imputation is k-nearest neighbors, or KNN, imputation. This is a more advanced form of imputation where missing values are replaced with values from other rows that are similar to the current row. While this is a lot more complicated to implement in practice than simple median imputation, it is very easy to explore in caret using the `preProcess` argument to `train()`. You can simply use `preProcess = \"knnImpute\"` to change the method of imputation used prior to model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in train.default(x = breast_cancer_x, y = breast_cancer_y, method = \"glm\", :\n",
      "\"The metric \"Accuracy\" was not in the result set. ROC will be used instead.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01: parameter=none \n",
      "- Fold01: parameter=none \n",
      "+ Fold02: parameter=none \n",
      "- Fold02: parameter=none \n",
      "+ Fold03: parameter=none \n",
      "- Fold03: parameter=none \n",
      "+ Fold04: parameter=none \n",
      "- Fold04: parameter=none \n",
      "+ Fold05: parameter=none \n",
      "- Fold05: parameter=none \n",
      "+ Fold06: parameter=none \n",
      "- Fold06: parameter=none \n",
      "+ Fold07: parameter=none \n",
      "- Fold07: parameter=none \n",
      "+ Fold08: parameter=none \n",
      "- Fold08: parameter=none \n",
      "+ Fold09: parameter=none \n",
      "- Fold09: parameter=none \n",
      "+ Fold10: parameter=none \n",
      "- Fold10: parameter=none \n",
      "Aggregating results\n",
      "Fitting final model on full training set\n"
     ]
    }
   ],
   "source": [
    "# Apply KNN imputation: model2\n",
    "model2 <- train(\n",
    "  x = breast_cancer_x, y = breast_cancer_y,\n",
    "  method = \"glm\",\n",
    "  trControl = myControl,\n",
    "  preProcess = \"knnImpute\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generalized Linear Model \n",
       "\n",
       "699 samples\n",
       "  9 predictor\n",
       "  2 classes: 'benign', 'malignant' \n",
       "\n",
       "Pre-processing: nearest neighbor imputation (9), centered (9), scaled (9) \n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 629, 629, 629, 630, 629, 629, ... \n",
       "Resampling results:\n",
       "\n",
       "  ROC        Sens       Spec \n",
       "  0.9926647  0.9694686  0.934\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print model to console\n",
    "model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare KNN and median imputation\n",
    "\n",
    "All of the preprocessing steps in the `train()` function happen in the training set of each cross-validation fold, so the error metrics reported include the effects of the preprocessing.\n",
    "\n",
    "This includes the imputation method used (e.g. `knnImpute` or `medianImpute`). This is useful because it allows you to compare different methods of imputation and choose the one that performs the best out-of-sample.\n",
    "\n",
    "median_model and knn_model are available in your workspace, as is resamples, which contains the resampled results of both models. Look at the results of the models by calling\n",
    "\n",
    "    dotplot(resamples, metric = \"ROC\")\n",
    "\n",
    "and choose the one that performs the best out-of-sample. Which method of imputation yields the highest out-of-sample ROC score for your glm model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order of operations\n",
    "\n",
    "Which comes first in caret's `preProcess()` function: median imputation or centering and scaling of variables?\n",
    "* Median imputation comes before centering and scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining preprocessing methods\n",
    "\n",
    "The preProcess argument to `train()` doesn't just limit you to imputing missing values. It also includes a wide variety of other preProcess techniques to make your life as a data scientist much easier. You can read a full list of them by typing ?preProcess and reading the help page for this function.\n",
    "\n",
    "One set of preprocessing functions that is particularly useful for fitting regression models is standardization: centering and scaling. You first center by subtracting the mean of each column from each value in that column, then you scale by dividing by the standard deviation.\n",
    "\n",
    "Standardization transforms your data such that for each column, the mean is 0 and the standard deviation is 1. This makes it easier for regression models to find a good solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in train.default(x = breast_cancer_x, y = breast_cancer_y, method = \"glm\", :\n",
      "\"The metric \"Accuracy\" was not in the result set. ROC will be used instead.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01: parameter=none \n",
      "- Fold01: parameter=none \n",
      "+ Fold02: parameter=none \n",
      "- Fold02: parameter=none \n",
      "+ Fold03: parameter=none \n",
      "- Fold03: parameter=none \n",
      "+ Fold04: parameter=none \n",
      "- Fold04: parameter=none \n",
      "+ Fold05: parameter=none \n",
      "- Fold05: parameter=none \n",
      "+ Fold06: parameter=none \n",
      "- Fold06: parameter=none \n",
      "+ Fold07: parameter=none \n",
      "- Fold07: parameter=none \n",
      "+ Fold08: parameter=none \n",
      "- Fold08: parameter=none \n",
      "+ Fold09: parameter=none \n",
      "- Fold09: parameter=none \n",
      "+ Fold10: parameter=none \n",
      "- Fold10: parameter=none \n",
      "Aggregating results\n",
      "Fitting final model on full training set\n"
     ]
    }
   ],
   "source": [
    "# Fit glm with median imputation: model1\n",
    "model1 <- train(\n",
    "  x = breast_cancer_x, y = breast_cancer_y,\n",
    "  method = \"glm\",\n",
    "  trControl = myControl,\n",
    "  preProcess = \"medianImpute\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generalized Linear Model \n",
       "\n",
       "699 samples\n",
       "  9 predictor\n",
       "  2 classes: 'benign', 'malignant' \n",
       "\n",
       "Pre-processing: median imputation (9) \n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 629, 628, 629, 629, 630, 629, ... \n",
       "Resampling results:\n",
       "\n",
       "  ROC        Sens       Spec \n",
       "  0.9917649  0.9695169  0.946\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print model1\n",
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in train.default(x = breast_cancer_x, y = breast_cancer_y, method = \"glm\", :\n",
      "\"The metric \"Accuracy\" was not in the result set. ROC will be used instead.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01: parameter=none \n",
      "- Fold01: parameter=none \n",
      "+ Fold02: parameter=none \n",
      "- Fold02: parameter=none \n",
      "+ Fold03: parameter=none \n",
      "- Fold03: parameter=none \n",
      "+ Fold04: parameter=none \n",
      "- Fold04: parameter=none \n",
      "+ Fold05: parameter=none \n",
      "- Fold05: parameter=none \n",
      "+ Fold06: parameter=none \n",
      "- Fold06: parameter=none \n",
      "+ Fold07: parameter=none \n",
      "- Fold07: parameter=none \n",
      "+ Fold08: parameter=none \n",
      "- Fold08: parameter=none \n",
      "+ Fold09: parameter=none \n",
      "- Fold09: parameter=none \n",
      "+ Fold10: parameter=none \n",
      "- Fold10: parameter=none \n",
      "Aggregating results\n",
      "Fitting final model on full training set\n"
     ]
    }
   ],
   "source": [
    "# Fit glm with median imputation and standardization: model2\n",
    "model2 <- train(\n",
    "  x = breast_cancer_x, y = breast_cancer_y,\n",
    "  method = \"glm\",\n",
    "  trControl = myControl,\n",
    "  preProcess = c(\"medianImpute\", \"center\", \"scale\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generalized Linear Model \n",
       "\n",
       "699 samples\n",
       "  9 predictor\n",
       "  2 classes: 'benign', 'malignant' \n",
       "\n",
       "Pre-processing: median imputation (9), centered (9), scaled (9) \n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 629, 629, 629, 629, 629, 629, ... \n",
       "Resampling results:\n",
       "\n",
       "  ROC        Sens       Spec \n",
       "  0.9918699  0.9671498  0.942\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print model2\n",
    "model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why remove near zero variance predictors?\n",
    "\n",
    "What's the best reason to remove near zero variance predictors from your data before building a model?\n",
    "* To reduce model-fitting time without reducing model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove near zero variance predictors\n",
    "\n",
    "As you saw in the video, for the next set of exercises, you'll be using the blood-brain dataset. This is a biochemical dataset in which the task is to predict the following value for a set of biochemical compounds:\n",
    "\n",
    "    log((concentration of compound in brain) /\n",
    "          (concentration of compound in blood))\n",
    "\n",
    "This gives a quantitative metric of the compound's ability to cross the blood-brain barrier, and is useful for understanding the biological properties of that barrier.\n",
    "\n",
    "One interesting aspect of this dataset is that it contains many variables and many of these variables have extemely low variances. This means that there is very little information in these variables because they mostly consist of a single value (e.g. zero).\n",
    "\n",
    "Fortunately, caret contains a utility function called `nearZeroVar()` for removing such variables to save time during modeling.\n",
    "\n",
    "`nearZeroVar()` takes in data `x`, then looks at the ratio of the most common value to the second most common value, `freqCut`, and the percentage of distinct values out of the number of total samples, `uniqueCut`. By default, caret uses `freqCut = 19` and `uniqueCut = 10`, which is fairly conservative. I like to be a little more aggressive and use `freqCut = 2` and `uniqueCut = 20` when calling `nearZeroVar()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "load(\"BloodBrain.rdata\") bloodbrain_x bloodbrain_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify near zero variance predictors: remove_cols\n",
    "remove_cols <- nearZeroVar(bloodbrain_x, names = TRUE, \n",
    "                           freqCut = 2, uniqueCut = 20)\n",
    "\n",
    "# Get all column names from bloodbrain_x: all_cols\n",
    "all_cols <- names(bloodbrain_x)\n",
    "\n",
    "# Remove from data: bloodbrain_x_small\n",
    "bloodbrain_x_small <- bloodbrain_x[ , setdiff(all_cols, remove_cols)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preProcess() and nearZeroVar()\n",
    "\n",
    "Can you use the `preProcess` argument in caret to remove near-zero variance predictors? Or do you have to do this by hand, prior to modeling, using the `nearZeroVar()` function?\n",
    "* Yes! Set the preProcess argument equal to \"nzv\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model on reduced blood-brain data\n",
    "\n",
    "Now that you've reduced your dataset, you can fit a glm model to it using the `train()` function. This model will run faster than using the full dataset and will yield very similar predictive accuracy.\n",
    "\n",
    "Furthermore, zero variance variables can cause problems with cross-validation (e.g. if one fold ends up with only a single unique value for that variable), so removing them prior to modeling means you are less likely to get errors during the fitting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\""
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generalized Linear Model \n",
       "\n",
       "208 samples\n",
       "112 predictors\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Bootstrapped (25 reps) \n",
       "Summary of sample sizes: 208, 208, 208, 208, 208, 208, ... \n",
       "Resampling results:\n",
       "\n",
       "  RMSE      Rsquared   MAE     \n",
       "  1.734699  0.1145815  1.131483\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit model on reduced data: model\n",
    "model <- train(x = bloodbrain_x_small, y = bloodbrain_y, method = \"glm\")\n",
    "\n",
    "# Print model to console\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal component analysis (PCA)\n",
    "### Using PCA as an alternative to nearZeroVar()\n",
    "\n",
    "An alternative to removing low-variance predictors is to run PCA on your dataset. This is sometimes preferable because it does not throw out all of your data: many different low variance predictors may end up combined into one high variance PCA variable, which might have a positive impact on your model's accuracy.\n",
    "\n",
    "This is an especially good trick for linear models: the pca option in the preProcess argument will center and scale your data, combine low variance variables, and ensure that all of your predictors are orthogonal. This creates an ideal dataset for linear regression modeling, and can often improve the accuracy of your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generalized Linear Model \n",
       "\n",
       "208 samples\n",
       "132 predictors\n",
       "\n",
       "Pre-processing: principal component signal extraction (132), centered\n",
       " (132), scaled (132) \n",
       "Resampling: Bootstrapped (25 reps) \n",
       "Summary of sample sizes: 208, 208, 208, 208, 208, 208, ... \n",
       "Resampling results:\n",
       "\n",
       "  RMSE       Rsquared   MAE   \n",
       "  0.6180152  0.4199532  0.4595\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit glm model using PCA: model\n",
    "model <- train(\n",
    "  x = bloodbrain_x, y = bloodbrain_y,\n",
    "  method = \"glm\", preProcess = \"pca\"\n",
    ")\n",
    "\n",
    "# Print model to console\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the PCA model's accuracy is slightly higher than the nearZeroVar() model from the previous exercise. PCA is generally a better method for handling low-information predictors than throwing them out entirely. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
